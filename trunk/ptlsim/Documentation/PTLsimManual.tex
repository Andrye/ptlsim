%% LyX 1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,english]{report}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\geometry{verbose,letterpaper,tmargin=0.75in,bmargin=0.75in,lmargin=0.75in,rmargin=0.75in,headheight=0in,headsep=0in,footskip=0in}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength\parskip{\medskipamount}
\setlength\parindent{0pt}
\usepackage{array}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\newcommand{\lyxline}[1]{
  {#1 \vspace{1ex} \hrule width \columnwidth \vspace{1ex}}
}
%% Bold symbol macro for standard LaTeX users
\providecommand{\boldsymbol}[1]{\mbox{\boldmath $#1$}}

%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
 \newenvironment{lyxlist}[1]
   {\begin{list}{}
     {\settowidth{\labelwidth}{#1}
      \setlength{\leftmargin}{\labelwidth}
      \addtolength{\leftmargin}{\labelsep}
      \renewcommand{\makelabel}[1]{##1\hfil}}}
   {\end{list}}
 \newenvironment{lyxcode}
   {\begin{list}{}{
     \setlength{\rightmargin}{\leftmargin}
     \setlength{\listparindent}{0pt}% needed for AMS classes
     \raggedright
     \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
     \normalfont\ttfamily}%
    \item[]}
   {\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[pdftitle={PTLsim User's Guide and Reference},colorlinks=true,linkcolor=blue]{hyperref}

\usepackage{babel}
\makeatother
\begin{document}
\noindent \begin{center}\textbf{\huge ~}\end{center}{\huge \par}

\vfill{}
\noindent \begin{center}\textsf{\textbf{\Huge PTLsim User's Guide
and Reference}}\end{center}{\Huge \par}

\noindent \begin{center}\emph{\huge The Anatomy of an x86-64 Out of
Order Microprocessor}\end{center}{\huge \par}
\bigskip{}

\noindent \begin{center}{\LARGE Matt T. Yourst}\\
\texttt{\large <yourst@yourst.com>}\end{center}{\large \par}

\noindent \begin{center}Revision 20051202\end{center}

\vfill{}
\noindent \begin{center}The latest version of PTLsim and this document
are always available at:\textsf{\textbf{\LARGE }}\\
\textsf{\textbf{\LARGE }}\\
\textsf{\textbf{\LARGE www.ptlsim.org}}\end{center}{\LARGE \par}

\bigskip{}
\noindent \begin{center}$\copyright$ 1999-2005 Matt T. Yourst \texttt{\small <yourst@yourst.com>}.\end{center}

\noindent \begin{center}The PTLsim software and manual are free software;\\
they are licensed under the GNU General Public License version 2.\end{center}

\tableofcontents{}


\part{\label{part:Introduction}PTLsim User's Guide}


\chapter{Introducing PTLsim}


\section{Introducing PTLsim}

\textbf{PTLsim} is a state of the art cycle accurate microprocessor
simulator and virtual machine for the x86 and x86-64 instruction sets.
This release of PTLsim models a modern speculative out of order x86-64
compatible processor core, cache hierarchy and supporting hardware.

PTLsim is very different from most cycle accurate simulators used
in research applications. It runs directly on the same platform it
is simulating (an x86-64 machine running Linux) and is able to switch
in and out of full out of order simulation mode and native x86-64
mode at any time completely transparent to the running user code.
This lets users quickly profile a small section of the user code without
the overhead of emulating the uninteresting parts. It is also capable
of switching between 32-bit and 64-bit threads on the fly.

Compared to competing simulators, PTLsim provides extremely high performance
even when running in full cycle accurate out of order simulation mode.
Through extensive tuning, cache profiling and the use of x86 specific
accelerated vector operations and instructions, PTLsim significantly
cuts simulation time compared to traditional research simulators.
Even with its optimized core, PTLsim still allows a significant amount
of flexibility for easy experimentation through the use of optimized
C++ template classes and libraries suited to synchronous logic design.
We have successfully run a wide array of programs under PTLsim, from
typical benchmarks to graphical applications and network servers.


\section{History}

PTLsim was designed and developed by Matt T. Yourst \texttt{\small <yourst@yourst.com>}
with its beginnings dating back to 2001. The main PTLsim code base,
including the out of order processor model, has been in active development
since 2003 and has been extensively used by our processor design research
group at the State University of New York at Binghamton. The out of
order simulator was based on an earlier model, also developed by Matt
Yourst, designed around the Alpha instruction set.

PTLsim is not related to other widely used simulators. It is our hope
that PTLsim will help microprocessor researchers move to a contemporary
and widely used instruction set (x86 and x86-64) with readily available
hardware implementations. This will provide a new option for researchers
stuck with simulation tools supporting only the Alpha or MIPS based
instruction sets, both of which have since been discontinued on real
commercially available hardware (making co-simulation impossible)
with an uncertain future in up to date compiler toolchains.

The PTLsim software and this manual are free software, licensed under
the GNU General Public License version 2.


\section{Documentation Roadmap}

This manual has been divided into several parts:

\begin{itemize}
\item Part \ref{part:Introduction} introduces PTLsim and describes its
structure and operation
\item Part \ref{part:x86andUops} describes the PTLsim internal micro-operation
instruction set and its relation to x86 and x86-64
\item Part \ref{part:OutOfOrderModel} details the design and implementation
of the PTLsim out of order core model
\item Part \ref{part:Appendices} is a reference manual for the PTLsim internal
uop instruction set, the performance monitoring events the simulator
supports and a variety of other technical information.
\end{itemize}

\section{Additional Resources}

The latest version of PTLsim and this document are always available
at the PTLsim web site:

\begin{quote}
\textsf{\textbf{\large http://www.ptlsim.org}}{\large \par}
\end{quote}

\chapter{Getting Started with PTLsim}


\section{Building PTLsim}

PTLsim is written in C++ with extensive use of x86 and x86-64 inline
assembly code for performance and virtualization purposes. In its
present release, it is designed for use on an x86-64 host system running
Linux 2.6.

\textbf{Notes:}

\begin{itemize}
\item \textbf{PTLsim is currently intended for x86-64 machines only.} Do
not attempt to build it on a normal 32-bit x86 machine - it will not
work. However, we will be modifying PTLsim in the near future to run
on regular 32-bit x86 systems (albeit with lower performance and the
lack of x86-64 support).
\item PTLsim is very sensitive to the \textbf{Linux kernel} version it is
running on. We have tested this version of PTLsim on 2.6.12 and \textbf{2.6.13},
but it may break on earlier versions due to changes in certain undocumented
system calls and structures we use for virtualization purposes. Section
\ref{sec:PTLsimInternals} gives more information on this. 
\item \textbf{gcc 3.4.x} should be used to compile the code, although it
should also work with gcc 3.3 or 4.0 (we have not tested this though)
\item We have not tried PTLsim on an \textbf{Intel based x86-64} (a.k.a.
EM64T) capable processor, but in theory it should work correctly.
You may need to adjust the Makefile options to specify this. Let us
know if you get this working!
\end{itemize}
To build PTLsim, unpack the sources (or obtain them via CVS checkout)
and just run \texttt{\small make} (on a multiprocessor machine, you
may wish to use \texttt{\small make -j2}). By default, the \texttt{\small Makefile}
specifies maximum optimization for an AMD x86-64 machine, so compilation
may be slow. For debugging purposes only, the \texttt{\small Makefile}
line specifying {}``\texttt{\small -O0 -g3}'' (no optimization,
all debugging info) may be used, but this will make PTLsim \emph{very}
slow.


\section{\label{sec:RunningPTLsim}Running PTLsim}

PTLsim invocation is very simple: after compiling the simulator and
making sure the \texttt{\small ptlsim} executable is in your path,
simply run:

\begin{quote}
\texttt{\small ptlsim~} \emph{full-path-to-executable} \emph{arguments...}
\end{quote}
PTLsim reads configuration options for running various user programs
by looking for a configuration file named \texttt{\small /home/}\emph{username}\texttt{\small /.ptlsim/}\emph{path/to/program/executablename}\texttt{\small .conf}.
To set options for each program, you'll need to create a directory
of the form \texttt{\small /home/}\emph{username}\texttt{\small /.ptlsim}
and make sub-directories under it corresponding to the full path to
the program. For example, to configure \texttt{\small /bin/ls} you'll
need to run \char`\"{}\texttt{\small mkdir /home/}\emph{username}\texttt{\small /.ptlsim/bin}''
and then edit \char`\"{}\texttt{\small /home/}\emph{username}\texttt{\small /.ptlsim/bin/ls.conf}\char`\"{}
with the appropriate options. For example, try putting the following
in \texttt{\small ls.conf} as described:

\begin{quote}
\texttt{\small -logfile ls.ptlsim -loglevel 9 -stats ls.stats -stopinsns
10000}{\small \par}
\end{quote}
Then run:

\begin{quote}
\texttt{\small ptlsim /bin/ls -la}{\small \par}
\end{quote}
PTLsim should display its system information banner, then the output
of simulating the directory listing. With the options above, PTLsim
will simulate \texttt{\small /bin/ls} starting at the first x86 instruction
in the dynamic linker's entry point, run until 10000 x86 instructions
have been committed, and will then switch back to native mode (i.e.
the user code will run directly on the real processor) until the program
exits. During this time, it will compile an extensive log of the state
of every micro-operation executed by the processor and will save it
to {}``\texttt{\small ls.ptlsim}'' in the current directory. It
will also create {}``\texttt{\small ls.stats}'', a binary file containing
snapshots of PTLsim's internal performance counters. The \texttt{\small ptlstats}
program can be used to print and analyze these statistics by running
{}``\texttt{\small ptlstats -dumpraw ls.stats}''.


\section{\label{sec:ConfigurationOptions}Configuration Options}

PTLsim supports a variety of options in the configuration file of
each program; you can run \char`\"{}ptlsim\char`\"{} without arguments
to get a full list of these options. The options described below are
used to control which code PTLsim executes and how it profiles it.
The default value for each option is shown in {[}brackets{]} at the
end of the description.

\begin{lyxlist}{WWWWWWWW}
\item [\textbf{\emph{Logging:}}]~
\item [\textbf{-quiet}]Do not print PTLsim system information banner {[}disabled{]}
\item [\textbf{-logfile~}\emph{file}]Log filename \emph{file} (use /dev/fd/1
for stdout, /dev/fd/2 for stderr) {[}(none){]}
\item [\textbf{-loglevel~}\emph{N}]Log level \emph{N}. Higher levels provide
a breakdown of every operation in every stage of the pipeline; {}``0''
disables all logging except startup messages and system call tracing
{[}0{]}
\item [\textbf{-startlog~}\emph{N}]Start logging after cycle \emph{N}
{[}infinity{]}
\item [\textbf{\emph{Statistics~Collection:}}]~
\item [\textbf{-stats~}\emph{file}]Statistics filename \emph{file} {[}(none){]}
\item [\textbf{-snapshot~}\emph{N}]Take statistical snapshot and reset
counters every \emph{N} cycles {[}infinite{]}
\item [\textbf{\emph{Simulation~Start~Point:}}]~
\item [\textbf{-startrip}~\emph{addr}]Start at RIP address \emph{startrip}
{[}(none){]}
\item [\textbf{-startrepeat}~\emph{N}]Start after passing start RIP at
least \emph{N} times {[}(none){]}
\item [\textbf{-excludeld}]Exclude dynamic linker execution (start at \texttt{\small main()}
instead) {[}disabled{]}
\item [\textbf{-trigger}]Trigger mode: wait for user process to use special
function \texttt{\small ptlcall\_switch\_to\_sim()} before entering
simulation mode {[}disabled{]}. This is described in Section \ref{sec:TriggerMode}.
\item [\textbf{\emph{Simulation~Stop~Point:}}]~
\item [\textbf{-stop~}\emph{N}]Stop after \emph{N} instructions {[}infinity{]}
\item [\textbf{-stop}rip~\emph{addr}]Stop before basic block RIP \emph{addr}
is translated for the first time {[}0{]}
\item [\textbf{-bbinsns~}\emph{N}]In final basic block, only translate
\emph{N} user instructions {[}infinity{]}
\item [\textbf{-stopinsns~}\emph{N}]Stop after committing \emph{N} user
instructions {[}infinity{]}
\item [\textbf{-flushevery~}\emph{N}]Flush pipeline after every \emph{N}
user instructions commit {[}infinity{]}
\item [\textbf{\emph{Sequential~and~Native~Control:}}]~
\item [\textbf{-profonly}]Profile user code in native mode only using CPU
performance counters; don't simulate anything {[}disabled{]}
\item [\textbf{-exitend}]Kill the thread after full simulation completes
rather than going native {[}disabled{]}
\item [\textbf{\emph{Debugging:}}]~
\item [\textbf{-dumpcode~}\emph{file}]Save page of user code at final
RIP to \emph{file} {[}(none){]}
\item [\textbf{-pause-at-startup}]Pause for N seconds at startup to let
a debugger attach {[}disabled{]} (see Section \ref{sec:DebuggingHints})
\item [\textbf{-perfect-cache}]Perfect cache hit rate {[}disabled{]}
\item [\textbf{-ooo}]Use out of order core {[}always enabled{]}
\end{lyxlist}
\textbf{\emph{NOTE:}} If you use the \char`\"{}\texttt{\small -logfile}\char`\"{}
option, \textbf{do not run it forever by accident} since the log files
will go on for gigabytes. Use \texttt{\small -startlog} and \texttt{\small -stop}
to limit the logged range. 

\textbf{\emph{NOTE:}} To actually modify the microarchitecture of
the simulated processor, you will need to edit the source code and
recompile.


\chapter{\label{sec:PTLsimInternals}PTLsim Internals}


\section{Overview}

The following is an overview of the source files for PTLsim:

\begin{itemize}
\item \texttt{\textbf{\small ooocore.cpp}} is the out of order simulator
itself. The microarchitectural model implemented by this simulator
is the subject of Part \ref{part:OutOfOrderModel}.
\item \texttt{\textbf{\small ptlhwdef.cpp}} and \texttt{\textbf{\small ptlhwdef.h}}
define the basic uop encodings, flags and registers. The tables of
uops might be interesting to see how a modern x86 processor is designed
at the microcode level. The basic format is discussed in Section \ref{sec:UopIntro};
all uops are documented in Section \ref{sec:UopReference}.
\item \texttt{\textbf{\small ooohwdef.h}} defines the parameters of the
out of order processor model not intrinsic to the PTLsim uop instruction
set itself.
\item \texttt{\textbf{\small translate-x86.cpp}} is where the x86 and x86-64
to uop translation takes place. It is complicated and generally you
shouldn't need to deal with this code to use or modify the simulator.
\item \texttt{\textbf{\small dcache.cpp}} and \texttt{\textbf{\small dcache.h}}
and \texttt{\textbf{\small dcacheint.h}} contain the data cache model.
At present the full L1/L2/L3/mem hierarchy is modeled. Note that the
instruction cache is missing at this point, but will be added back
in very soon. For SPEC this does not matter very much. The cache hierarchy
is very flexible configuration wise; it is described further in Section
\ref{sec:CacheHierarchy}.
\item \texttt{\textbf{\small kernel.cpp}} and \texttt{\textbf{\small kernel.h}}
is where all the virtual machine \char`\"{}black magic\char`\"{} takes
place to let PTLsim transparently switch between simulation and native
mode and 32-bit/64-bit mode. In general you should not need to touch
this since it is very Linux kernel specific (this version works with
2.6.12 - 2.6.13) and works at a level below the standard C/C++ libraries.
\item \texttt{\textbf{\small branchpred.cpp}} and \texttt{\textbf{\small branchpred.h}}
is the branch predictor. Currently this is set up as a hybrid bimodal
and history based predictor with various customizable parameters.
\item \texttt{\textbf{\small logic.h}} is a library of C++ templates for
implementing synchronous logic structures like associative arrays,
queues, register files, etc. It has some very clever features like
\texttt{\small FullyAssociativeArray8bit}, which uses x86 SSE vector
instructions to associatively match and process \textasciitilde{}16
byte-sized tags every cycle. These classes are fully parameterized
and useful for all kinds of simulations.
\item \texttt{\textbf{\small globals.h}}, \texttt{\textbf{\small superstl.h}}
and \texttt{\textbf{\small superstl.cpp}} implement various standard
library functions and classes as an alternative to C++ STL. These
libraries also contain a number of features very useful for bit manipulation.
\item \texttt{\textbf{\small uopinterface.cpp}} contains code for mapping
uop opcodes and characteristics to their implementation in \texttt{\small uopimpl.S}.
\item \texttt{\textbf{\small uopimpl.S}} contains x86-64 assembly language
implementations of all uops and their variations. PTLsim implements
most ALU and floating point uops in assembler so as to leverage the
exact semantics and flags generated by real x86 instructions, since
most PTLsim uops are so similar to the equivalent x86 instructions.
\item \texttt{\textbf{\small ptlsim.cpp}} and \texttt{\textbf{\small ptlsim.h}}
are responsible for initializing PTLsim and starting the appropriate
simulation core code.
\item \texttt{\textbf{\small config.cpp}} and \texttt{\textbf{\small config.h}}
manage the PTLsim configuration options for each user program.
\item \texttt{\textbf{\small datastore.cpp}} and \texttt{\textbf{\small datastore.h}}
manage the PTLsim statistics data store file structure.
\item \texttt{\textbf{\small lowlevel-64bit.S}} contains 64-bit startup
and context switching code. PTLsim execution starts here.
\item \texttt{\textbf{\small lowlevel-32bit.S}} contains 32-bit startup
and context switching code.
\item \texttt{\textbf{\small injectcode.cpp}} is compiled into the 32-bit
and 64-bit code injected into the target process to map the \texttt{\small ptlsim}
binary and pass control to it.
\item \texttt{\textbf{\small loader.h}} is used to pass information to the
injected boot code.
\item \texttt{\textbf{\small ptlstats.cpp}} is a utility for printing and
analyzing the statistics data store files in various human readable
ways.
\item \texttt{\textbf{\small cpuid.cpp}} is a utility program to show various
data returned by the x86 \texttt{\small cpuid} instruction. Run it
under PTLsim for a surprise.
\item \texttt{\textbf{\small genoffsets.cpp}} is a utility program used
during the build process to give the assembly language code the offsets
of various C++ structure fields.
\item \texttt{\textbf{\small ptlcalls.c}} and \texttt{\textbf{\small ptlcalls.h}}
are optionally compiled into user programs to let them switch into
and out of simulation mode on their own. The \texttt{\textbf{\small ptlcalls.o}}
file is typically linked with Fortran programs that can't use regular
C header files.
\end{itemize}

\section{Common Libraries and Logic Design APIs}

PTLsim includes a number of powerful C++ templates, macros and functions
not found anywhere else. This section attempts to provide an overview
of these structures so that users of PTLsim will use them instead
of trying to duplicate work we've already done.


\subsection{General Purpose Macros}

The file \texttt{\small globals.h} contains a wide range of very useful
definitions, functions and macros we have accumulated over the years,
including:

\begin{itemize}
\item Basic data types used throughout PTLsim (e.g. W64 for 64-bit words
and so on), \texttt{\small globals.h} defines a
\item Type safe C++ template based functions, including \texttt{\small min},
\texttt{\small max}, \texttt{\small abs}, \texttt{\small mux}, etc.
\item Iterator macros (\texttt{\small foreach}) 
\item Template based metaprogramming functions including \texttt{\small lengthof}
(finds the length of any static array) and \texttt{\small log2} (takes
the base-2 log of any constant at compile time)
\item Floor, ceiling and masking functions for integers and powers of two
(\texttt{\small floor}, \texttt{\small trunc}, \texttt{\small ceil},
\texttt{\small mask}, \texttt{\small floorptr}, \texttt{\small ceilptr},
\texttt{\small maskptr}, \texttt{\small signext}, etc)
\item Bit manipulation macros (\texttt{\small bit}, \texttt{\small bitmask},
\texttt{\small bits}, \texttt{\small lowbits}, \texttt{\small setbit},
\texttt{\small clearbit}, \texttt{\small assignbit}). Note that the
\texttt{\small bitvec} template (see below) should be used in place
of these macros wherever it is more convenient.
\item Comparison functions (\texttt{\small aligned}, \texttt{\small strequal},
\texttt{\small inrange}, \texttt{\small clipto})
\item Modulo arithmetic (\texttt{\small add\_index\_modulo}, \texttt{\small modulo\_span},
et al)
\item Definitions of basic x86 SSE vector functions (e.g. \texttt{\small x86\_cpu\_pcmpeqb}
et al)
\item Definitions of basic x86 assembly language functions (e.g. \texttt{\small x86\_bsf64}
et al)
\item A full suite of bit scanning functions (\texttt{\small lsbindex},
\texttt{\small msbindex}, \texttt{\small popcount} et al)
\item Miscellaneous functions (\texttt{\small arraycopy}, \texttt{\small setzero},
etc)
\end{itemize}

\subsection{Super Standard Template Library (SuperSTL)}

The Super Standard Template Library (SuperSTL) is an internal C++
library we use internally in lieu of the normal C++ STL for various
technical and preferential reasons. While the full documentation is
in the comments of \texttt{\small superstl.h} and \texttt{\small superstl.cpp},
the following is a brief list of its features:

\begin{itemize}
\item I/O stream classes familiar from Standard C++, including \texttt{\small istream}
and \texttt{\small ostream}. Unique to SuperSTL is how the comma operator
({}``,'') can be used to separate a list of objects to send to or
from a stream, in addition to the usual C++ insertion operator ({}``<\,{}<'').
\item To read and write binary data, the \texttt{\small idstream} and \texttt{\small odstream}
classes should be used instead.
\item String buffer (\texttt{\small stringbuf}) class for composing strings
in memory the same way they would be written to or read from an \texttt{\small ostream}
or \texttt{\small istream}.
\item String formatting classes (\texttt{\small intstring}, \texttt{\small hexstring},
\texttt{\small padstring}, \texttt{\small bitstring}, \texttt{\small bytemaskstring},
\texttt{\small floatstring}) provide a wrapper around objects to exercise
greater control of how they are printed.
\item Array (\texttt{\small array}) template class represents a fixed size
array of objects. It is essentially a simple but very fast wrapper
for a C-style array.
\item Bit vector (\texttt{\small bitvec}) is a heavily optimized and rewritten
version of the Standard C++ \texttt{\small bitset} class. It supports
many additional operations well suited to logic design purposes and
emphasizes extremely fast branch free code.
\item Dynamic Array (\texttt{\small dynarray}) template class provides for
dynamically sized arrays, stacks and other such structures, similar
to the Standard C++ \texttt{\small valarray} class.
\item Linked list node (\texttt{\small listlink}) template class forms the
basis of double linked list structures in which a single pointer refers
to the head of the list.
\item Queue list node (\texttt{\small queuelink}) template class supports
more operations than \texttt{\small listlink} and can serve as both
a node in a list and a list head/tail header.
\item Index reference (\texttt{\small indexref}) is a smart pointer which
compresses a full pointer into an index into a specific structure
(made unique by the template parameters). This class behaves exactly
like a pointer when referenced, but takes up much less space and may
be faster. The \texttt{\small indexrefnull} class adds support for
storing null pointers, which \texttt{\small indexref} lacks.
\item \texttt{\small Hashtable} class is a general purpose chaining based
hash table with user configurable key hashing and management via add-on
template classes.
\item \texttt{\small ChunkHashtable} class is a simplified hash table designed
for small data items, for instance where we simply want to detect
the presence of a key rather than associate data with it. It tries
to pack many keys into cache line sized chunks and does parallel vectorized
matching on each chunk for added speed.
\item \texttt{\small CRC32} calculation class is useful for hashing
\item \texttt{\small CycleTimer} is useful for timing intervals with sub-nanosecond
precision using the CPU cycle counter (discussed in Section \ref{sec:Timing}).
\end{itemize}

\subsection{Logic Standard Template Library (LogicSTL)}

The Logic Standard Template Library (LogicSTL) is an internally developed
add-on to SuperSTL which supports a variety of structures useful for
modeling sequential logic. Some of its primitives may look familiar
to Verilog or VHDL programmers. While the full documentation is in
the comments of \texttt{\small logic.h}, the following is a brief
list of its features:

\begin{itemize}
\item \texttt{\small latch} template class works like any other assignable
variable, but the new value only becomes visible after the \texttt{\small clock()}
method is called (potentially from a global clock chain).
\item \texttt{\small Queue} template class implements a general purpose
fixed size queue. The queue supports various operations from both
the head and the tail, and is ideal for modeling queues in microprocessors.
\item Iterators for \texttt{\small Queue} objects such as \texttt{\small foreach\_forward},
\texttt{\small foreach\_forward\_from}, \texttt{\small foreach\_forward\_after},
\texttt{\small foreach\_backward}, \texttt{\small foreach\_backward\_from},
\texttt{\small foreach\_backward\_before}.
\item \texttt{\small HistoryBuffer} maintains a shift register of values,
which when combined with a hash function is useful for implementing
predictor histories and the like.
\item \texttt{\small FullyAssociativeTags} template class is a general purpose
array of associative tags in which each tag must be unique. This class
uses highly efficient matching logic and supports pseudo-LRU eviction,
associative invalidation and direct indexing. It forms the basis for
most associative structures in PTLsim.
\item \texttt{\small FullyAssociativeArray} pairs a \texttt{\small FullyAssociativeTags}
object with actual data values to form the basis of a cache.
\item \texttt{\small AssociativeArray} divides a \texttt{\small FullyAssociativeArray}
into sets. In effect, this class can provide a complete cache implementation
for a processor.
\item \texttt{\small LockableFullyAssociativeTags}, \texttt{\small LockableFullyAssociativeArray}
and \texttt{\small LockableAssociativeArray} provide the same services
as the classes above, but support locking lines into the cache.
\item \texttt{\small CommitRollbackCache} leverages the \texttt{\small LockableFullyAssociativeArray}
class to provide a cache structure with the ability to roll back all
changes made to memory (not just within this object, but everywhere)
after a checkpoint is made.
\item \texttt{\small FullyAssociativeTags8bit} and \texttt{\small FullyAssociativeTags16bit}
work just like \texttt{\small FullyAssociativeTags}, except that these
classes are dramatically faster when using small 8-bit and 16-bit
tags. This is possible through the clever use of x86 SSE vector instructions
to associatively match and process 16 8-bit tags or 8 16-bit tags
every cycle. In addition, these classes support features like removing
an entry from the middle of the array while compacting entries around
it in constant time. These classes should be used in place of \texttt{\small FullyAssociativeTags}
whenever the tags are small enough (i.e. almost all tags except for
memory addresses).
\end{itemize}

\subsection{Miscellaneous Code}

The out of order simulator, ooocore.cpp, contains several reusable
classes, including:

\begin{itemize}
\item \texttt{\small IssueQueue} template class can be used to implement
all kinds of broadcast based issue queues
\item \texttt{\small StateList} and \texttt{\small ListOfStateLists} is
useful for collecting various lists that objects can be on into one
structure.
\end{itemize}

\section{\label{sec:Injection}Low Level Startup and Injection}

\emph{Note:} This section deals with the internal operation of the
PTLsim virtual machine manager, independent of the out of order simulation
engine. If you are only interested in modifying the simulator itself,
you can skip this section.

PTLsim is a very unusual Linux program. It does its own internal memory
management and threading without help from the standard libraries,
injects itself into other processes to take control of them, and switches
between 32-bit and 64-bit mode within a single process image. For
these reasons, it is very closely tied to the Linux kernel and uses
a number of undocumented system calls and features only available
in late 2.6 series kernels. 

PTLsim always starts and runs as a 64-bit process even when running
32-bit threads; it context switches between modes as needed. The statically
linked \texttt{\small ptlsim} executable begins executing at \texttt{\small ptlsim\_preinit\_entry}
in \texttt{\small lowlevel-64bit.S}. This code calls \texttt{\small ptlsim\_preinit()}
in \texttt{\small kernel.cpp} to set up our custom memory manager
and threading environment before any standard C/C++ functions are
used. After doing so, the normal \texttt{\small main()} function is
invoked.

The \texttt{\small ptlsim} binary can run in two modes. If executed
from the command line as a normal program, it starts up in \emph{inject}
mode. Specifically, \texttt{\small main()} in \texttt{\small ptlsim.cpp}
checks if the \texttt{\small inside\_ptlsim} variable has been set
by \texttt{\small ptlsim\_preinit\_entry}, and if not, PTLsim enters
inject mode. In this mode, \texttt{\small ptlsim\_inject()} in \texttt{\small kernel.cpp}
is called to effectively inject the \texttt{\small ptlsim} binary
into another process and pass control to it before even the dynamic
linker gets to load the program. In \texttt{\small ptlsim\_inject()},
the PTLsim process is forked and the child is placed under the parent's
control using \texttt{\small ptrace()}. The child process then uses
\texttt{\small exec()} to start the user program to simulate (this
can be either a 32-bit or 64-bit program). 

However, the user program starts in the stopped state, allowing \texttt{\small ptlsim\_inject()}
to use \texttt{\small ptrace()} and related functions to inject either
32-bit or 64-bit boot loader code directly into the user program address
space, overwriting the entry point of the dynamic linker. This code,
derived from \texttt{\small injectcode.cpp} (specifically compiled
as \texttt{\small injectcode-32bit.o} and \texttt{\small injectcode-64bit.o})
is completely position independent. Its sole function is to map the
rest of \texttt{\small ptlsim} into the user process address space
at virtual address \texttt{\small 0x70000000} and set up a special
\texttt{\small LoaderInfo} structure to allow the master PTLsim process
and the user process to communicate. The boot code also restores the
old code at the dynamic linker entry point after relocating itself.
Finally, \texttt{\small ptlsim\_inject()} adjusts the user process
registers to start executing the boot code instead of the normal program
entry point, and resumes the user process.

At this point, the PTLsim image injected into the user process exists
in a bizarre environment: if the user program is 32 bit, the boot
code will need to switch to 64-bit mode before calling the 64-bit
PTLsim entrypoint. Fortunately x86-64 and the Linux kernel make this
process easy, despite never being used by normal programs: a regular
far jump switches the current code segment descriptor to \texttt{\small 0x33},
effectively switching the instruction set to x86-64. For the most
part, the kernel cannot tell the difference between a 32-bit and 64-bit
process: as long as the code uses 64-bit system calls (i.e. \texttt{\small syscall}
instruction instead of \texttt{\small int 0x80} as with 32-bit system
calls), Linux assumes the process is 64-bit. There are some subtle
issues related to signal handling and memory allocation when performing
this trick, but PTLsim implements workarounds to these issues.

After entering 64-bit mode if needed, the boot code passes control
to PTLsim at \texttt{\small ptlsim\_preinit\_entry}. The \texttt{\small ptlsim\_preinit()}
function checks for the special \texttt{\small LoaderInfo} structure
on the stack and in the ELF header of PTLsim as modified by the boot
code; if these structures are found, PTLsim knows it is running inside
the user program address space. After setting up memory management
and threading, it captures any state the user process was initialized
with. This state is used to fill in fields in the global \texttt{\small ctx}
structure of class \texttt{\small CoreContext}: various floating point
related fields and the user program entry point and original stack
pointer are saved away at this point. If PTLsim is running inside
a 32-bit process, the 32-bit arguments, environment and kernel auxiliary
vector array (auxv) need to be converted to their 64-bit format for
PTLsim to be able to parse them from normal C/C++ code. Finally, control
is returned to \texttt{\small main()} to allow the simulator to start
up normally.


\section{Simulator Startup}

In \texttt{\small ptlsim.cpp}, the \texttt{\small main()} function
calls \texttt{\small init\_config()} to read in the user program specific
configuration as described in Sections \ref{sec:RunningPTLsim} and
\ref{sec:ConfigurationOptions}, then starts up the various other
simulator subsystems. If one of the \texttt{\small -excludeld} or
\texttt{\small -startrip} options were given, a breakpoint is inserted
at the RIP address where the user process should switch from native
mode to simulation mode (this may be at the dynamic linker entry point
by default).

Finally, \texttt{\small switch\_to\_native\_restore\_context()} is
called to restore the state that existed before PTLsim was injected
into the process and return to the dynamic linker entry point. This
may involve switching from 64-bit back to 32-bit mode to start executing
the user process natively as discussed in Section \ref{sec:Injection}.

After native execution reaches the inserted breakpoint thunk code,
the code performs a 32-to-64-bit long jump back into PTLsim, which
promptly restores the code underneath the inserted breakpoint thunk.
At this point, the \texttt{\small switch\_to\_sim()} function in \texttt{\small ptlsim.cpp}
is invoked to actually begin the simulation. This is done by calling
\texttt{\small out\_of\_order\_core\_toplevel\_loop()} in ooocore.cpp.

At some point during simulation, the user program or the configuration
file may request a switch back to native mode for the remainder of
the program. In this case, the \texttt{\small show\_stats\_and\_switch\_to\_native()}
function gets called to save the statistics data store, map the PTLsim
internal state back to the x86 compatible external state and return
to the 32-bit or 64-bit user code, effectively removing PTLsim from
the loop.

While the real PTLsim user process is running, the original PTLsim
injector process simply waits in the background for the real user
program with PTLsim inside it to terminate, then returns its exit
code.


\section{\label{sec:AddressSpaceSimulation}Address Space Simulation}

PTLsim maintains the \texttt{\small AddressSpace} class as global
variable \texttt{\small asp} (see \texttt{\small kernel.cpp}) to track
the attributes of each page within the virtual address space. To do
this, PTLsim uses Shadow Page Access Tables (SPATs), which are essentially
large two-level bitmaps. Since pages are 4096 bytes in size, each
64 kilobyte chunk of the bitmap can track 2 GB of virtual address
space. In each SPAT, each top level array entry points to a chunk
mapping 2 GB, such that with 131072 top level pointers, the full 48
bit virtual address space can typically be mapped with under a megabyte
of SPAT chunks, assuming the address space is sparse.

In the AddressSpace structure, there are separate SPAT tables for
readable pages (\texttt{\small readmap} field), writable pages (\texttt{\small writemap}
field) and executable pages (\texttt{\small execmap} field). Two additional
SPATs, \texttt{\small dtlbmap} and \texttt{\small itlbmap}, are used
to track which pages are currently mapped by the simulated translation
lookaside buffers (TLBs); this is discussed further in Section \ref{sec:TranslationLookasideBuffers}.

When running in native mode, PTLsim cannot track changes to the process
memory map made by native calls to \texttt{\small mmap()}, \texttt{\small munmap()},
etc. Therefore, at every switch from native to simulation mode, the
\texttt{\small resync\_with\_process\_maps()} function is called.
This function parses the \texttt{\small /proc/self/maps} metafile
maintained by the kernel to build a list of all regions mapped by
the current process. Using this list, the SPATs are rebuilt to reflect
the current memory map. This is absolutely critical for correct operation,
since during simulation, speculative loads and stores will only read
and write memory if the appropriate SPAT indicates the address is
accessible to user code. If the SPATs become out of sync with the
real memory map, PTLsim itself may crash rather than simply marking
the offending load or store as invalid. The \texttt{\small resync\_with\_process\_maps()}
function (or more specifically, the \texttt{\small mqueryall()} helper
function) is fairly kernel version specific since the format of \texttt{\small /proc/self/maps}
has changed between Linux 2.6.x kernels. New kernels may require updating
this function.

PTLsim does not use the normal C library implementations of \texttt{\small malloc()},
\texttt{\small free()}, \texttt{\small mmap()}, \texttt{\small new},
\texttt{\small delete} and so on. Instead, PTLsim code should always
use the \texttt{\small ptl\_alloc\_private\_pages()} family of functions
defined in \texttt{\small kernel.cpp} to ensure that PTLsim memory
remains completely invisible to user code (except, of course, within
PTLsim generated microcode sequences). This is done by clearing the
bits in the read, write and execute SPATs corresponding to the allocated
pages. The \texttt{\small new}, \texttt{\small delete}, \texttt{\small malloc()}
and \texttt{\small free()} functions can still be used since PTLsim
overrides these. Note that memory allocated in this way will \emph{not}
be accessible to user code.


\section{\label{sec:DebuggingHints}Debugging Hints}

When adding or modifying PTLsim, bugs will invariably crop up. Fortunately,
PTLsim provides a trivial way to find the location of bugs which silently
corrupt program execution. Since PTLsim can transparently switch between
simulation and native mode, isolating the divergence point between
the simulated behavior and what a real reference machine would do
can be done through binary search. The \texttt{\small -stopinsns}
configuration option can be set to stop simulation before the problem
occurs, then incremented until the first x86 instruction to break
the program is determined.

The out of order simulator (\texttt{\small ooocore.cpp}) includes
extensive debugging and integrity checking assertions. These may be
turned off by default for improved performance, but they can be easily
re-enabled by defining the \texttt{\small ENABLE\_CHECKS} symbol at
the top of \texttt{\small ooocore.cpp}. Additional check functions
are in the code but commented out; these may be used as well.

You can also debug PTLsim with \texttt{\small gdb}, although the process
is non-standard due to PTLsim's co-simulation architecture:

\begin{itemize}
\item Start PTLsim on the target program like normal. Notice the \texttt{\small Thread}
\texttt{\emph{\small N}} \texttt{\small is running in XX-bit mode}
message printed at startup: this is the PID you will be debugging,
not the {}``\texttt{\small ptlsim}'' process that may also be running.
\item Start GDB and type {}``\texttt{\small attach 12345}'' if \emph{12345}
was the PID listed above
\item Type {}``\texttt{\small symbol-file ptlsim}'' to load the PTLsim
internal symbols (otherwise gdb only knows about the benchmark code
itself). You should specify the full path to the PTLsim executable
here.
\item You're now debugging PTLsim. If you run the {}``\texttt{\small bt}''
command to get a backtrace, it should show the PTLsim functions starting
at address 0x70000000.
\end{itemize}
If the backtrace does not display enough information, go to the \texttt{\small Makefile}
and enable the \char`\"{}no optimization\char`\"{} options (the \char`\"{}-O0\char`\"{}
line instead of \char`\"{}-O99\char`\"{}) since that will make more
debugging information available to you.

The {}``\texttt{\small -pause-at-startup} \emph{seconds}'' configuration
option may be useful here, to give you time to attach with a debugger
before starting the simulation.


\section{\label{sec:Timing}Timing Issues}

PTLsim uses the \texttt{\small CycleTimer} class extensively to gather
data about its own performance using the CPU's timestamp counter.
At startup in \texttt{\small superstl.cpp}, the CPU's maximum frequency
is queried from the appropriate Linux kernel sysfs node (if available)
or from \texttt{\small /proc/cpuinfo} if not. Processors which dynamically
scale their frequency and voltage in response to load (like all Athlon
64 and K8 based AMD processors) require special handling. It is assumed
that the processor will be running at its maximum frequency (as reported
by sysfs) or a fixed frequency (as reported by \texttt{\small /proc/cpuinfo})
throughout the majority of the simulation time; otherwise the timing
results will be bogus.


\chapter{\label{sec:StatisticsInfrastructure}Statistics Collection and Control}


\section{Using PTLstats to Analyze Statistics}

PTLsim maintains a huge number of statistical counters and data points
during the simulation process, and can optionally save this data to
a statistics data store by using the {}``\texttt{\small -stats} \emph{filename}''
configuration option introduced in Section \ref{sec:ConfigurationOptions}.
The data store is a binary file format (defined in \texttt{\small datastore.cpp})
used to efficiently capture large quantities of statistical information
for later analysis. This file format supports storing multiple regular
snapshots of all counters by specifying the {}``\texttt{\small -snapshot}
\emph{N}'' option to save a snapshot of the simulator state every
\emph{N} cycles in addition to the final state.

The \emph{PTLstats} program is used to analyze the statistics data
store files produced by PTLsim. The syntax of this command is {}``\texttt{\small ptlstats
-}\emph{options} \emph{filename}''.

The following major actions) are supported:

\begin{lyxlist}{WWWWWWWW}
\item [(no~options)]Print the full PTLsim statistics tree in the textual
format shown below.
\item [\textbf{-subtree}\textbf{\emph{~}}\emph{root}]Print a subtree of
the statistics tree. For instance, to print just the load unit statistics
in the final snapshot, use {}``\texttt{\small ptlstats} \texttt{\textbf{\small -subtree
final/dcache/load}} \texttt{\emph{\small filename.stats}}''.
\item [\textbf{-collect}\textbf{\emph{~}}\emph{root}]Collect the same
subtree from muliple statistics files and print all the subtrees.
For instance, to print the L1 cache hit rate for two benchmarks, use
{}``\texttt{\small ptlstats} \texttt{\textbf{\small -collect /final/dcache/load/hit/L1}}
\texttt{\small bench1.stats bench2.stats ...}''. This option is useful
for taking a cross section from all benchmarks of a specific statistic
(or subtree of statistics).
\item [\textbf{-histogram}\textbf{\emph{~}}\emph{root}]Generate a histogram
graph, in SVG (Scalable Vector Graphics) format, of the specified
statistics node. The node must be an array of integers previously
marked as a histogram (see the source code examples for details).
Various other options are provided to control the graph size, title,
data percentile cutoff and log scaling. 
\end{lyxlist}
Additional options and their descriptions can be found by running
\texttt{\small ptlstats} without any arguments.

The following is an example of the type of output you can expect from
PTLstats:

\begin{lyxcode}
{\small dcache~\{}{\small \par}

~{\small ~store~\{}{\small \par}

~{\small ~~~issue~(total~134243383)~\{}{\small \par}

~{\small ~~~~~{[}~~22\%~{]}~replay~(total~29278598)~\{}{\small \par}

~{\small ~~~~~~~{[}~~~0\%~{]}~wait-sfraddr~=~0;}{\small \par}

~{\small ~~~~~~~{[}~~33\%~{]}~wait-storedata-sfraddr~=~9755097;}{\small \par}

~{\small ~~~~~~~{[}~~33\%~{]}~wait-storedata-sfraddr-sfrdata~=~9755097;}{\small \par}

~{\small ~~~~~~~{[}~~~6\%~{]}~wait-storedata-sfrdata~=~1891253;}{\small \par}

~{\small ~~~~~~~{[}~~~4\%~{]}~wait-sfrdata~=~1069751;}{\small \par}

~{\small ~~~~~~~{[}~~23\%~{]}~wait-sfraddr-sfrdata~=~6807400;}{\small \par}

~{\small ~~~~~\}}{\small \par}

~{\small ~~~~~{[}~~~0\%~{]}~exception~=~196094;}{\small \par}

~{\small ~~~~~{[}~~~0\%~{]}~ordering~=~55369;}{\small \par}

~{\small ~~~~~{[}~~78\%~{]}~complete~=~104592504;}{\small \par}

~{\small ~~~~~{[}~~~0\%~{]}~unaligned~=~120818;}{\small \par}

~{\small ~~~\}}{\small \par}

~{\small ~~~...}{\small \par}
\end{lyxcode}
Notice how PTLstats will automatically sum up all entries in certain
branches of the tree to provide the user with a breakdown by percentages
of the total for that subtree in addition to the raw values.


\section{Statistics support in code}

Internally, PTLsim represents data store nodes in a tree format via
the \texttt{\small DataStoreNode} class defined in \texttt{\small datastore.h}.
Each node can contain other data store nodes as well as a value (64-bit
integer, floating point, character string) or an array of values.
The following example illustrates the proper way to save statistical
counters (typically declared as global variables of type \texttt{\small W64},
a 64-bit integer) into the data store. In this example, the \texttt{\emph{\small root}}
\texttt{\small DataStoreNode} is assumed to be passed in as the parent
of all other nodes:

\begin{lyxcode}
{\small DataStoreNode\&~issue~=~root(\char`\"{}issue\char`\"{});~\{}{\small \par}

~{\small ~DataStoreNode\&~unit~=~issue(\char`\"{}unit\char`\"{});~\{}{\small \par}

~{\small ~~~unit.summable~=~1;}{\small \par}

~{\small ~~~unit.add(\char`\"{}integer\char`\"{},~issued\_integer\_uops);}{\small \par}

~{\small ~~~unit.add(\char`\"{}fp\char`\"{},~issued\_fp\_uops);}{\small \par}

~{\small ~~~unit.add(\char`\"{}load\char`\"{},~issued\_load\_uops);}{\small \par}

~{\small ~~~unit.add(\char`\"{}store\char`\"{},~issued\_store\_uops);}{\small \par}

~{\small ~\}}{\small \par}

~{\small ~DataStoreNode\&~histogram~=~issue(\char`\"{}times\char`\"{});~\{}{\small \par}

~{\small ~~~cluster.summable~=~1;}{\small \par}

~{\small ~~~foreach~(i,~MAX\_CLUSTERS)~\{}{\small \par}

~{\small ~~~~~stringbuf~sb;~sb~<\,{}<~i;}{\small \par}

~{\small ~~~~~cluster.addfloat(sb,~timer\_histogram{[}i{]});}{\small \par}

~{\small ~~~\}}{\small \par}

~{\small ~\}}{\small \par}

{\small \}}{\small \par}
\end{lyxcode}
In the example above, a statistics tree is created in which all subnodes
under the {}``\texttt{\small unit}'' node are integers together
assumed to total 100\% of whatever quantity is being measured. Setting
\texttt{\small summable = 1} tells PTLstats to print percentages next
to the raw values in this subtree for easier viewing. The {}``\texttt{\small times}''
subnode contains floating point values; the \texttt{\small stringbuf}
utility class is used to convert each slot index of this example histogram
into a proper node name. This is only an example - PTLsim already
contains code to save all statistics it generates.

PTLsim will by default call \texttt{\small ooo\_capture\_stats()}
every time a snapshot or final capture is taken, and will pass this
function the top level DataStoreNode for that snapshot. Therefore,
if adding your own statistics and counters, you should start by saving
these counters using code inside \texttt{\small ooo\_capture\_stats()}
like that shown above. 

We suggest using the data store mechanism to store \emph{all} statistics
generated by your additions to PTLsim, since this system has built-in
support for snapshots, checkpointing and structured easy to parse
data (unlike simply writing values to a text file). It is further
suggested that only raw values be saved, rather than doing computations
in the simulator itself - leave the analysis to PTLstats after gathering
the raw data. In particular, try to avoid using floating point within
the simulator if at all possible, since some floating point calculations
may reconfigure the SSE rounding and control flags in ways that break
the assumptions used to execute actual user code.


\section{\label{sec:TriggerMode}PTLsim Calls From User Code}

PTLsim optionally allows user code to control the simulator mode through
the \texttt{\small ptlcall\_xxx()} family of functions found in \texttt{\small ptlcalls.h}
when trigger mode is enabled (\texttt{\small -trigger} configuration
option). This file should be included by any PTLsim-aware user programs;
these programs must be recompiled to take advantage of these features.
Amongst the functions provided by \texttt{\small ptlcalls.h} are:

\begin{itemize}
\item \texttt{\small ptlcall\_switch\_to\_sim()} is only available while
the program is executing in native mode. It forces PTLsim to regain
control and begin simulating instructions as soon as this call returns.
\item \texttt{\small ptlcall\_switch\_to\_native()} stops simulation and
returns to native execution, effectively removing PTLsim from the
loop.
\item \texttt{\small ptlcall\_marker()} simply places a user-specified marker
number in the PTLsim log file
\item \texttt{\small ptlcall\_capture\_stats()} adds a new statistics data
store snapshot at the time it is called. 
\item \texttt{\small ptlcall\_nop()} does nothing but test the call mechanism.
\end{itemize}
These calls work by forcing execution to code on a {}``gateway page''
at a specific fixed address (\texttt{\small 0x1000} currently); PTLsim
will write the appropriate call gate code to this page depending on
whether the process is in native or simulated mode. In native mode,
the call gate page typically contains a 64-to-64-bit or 32-to-64-bit
far jump into PTLsim, while in simulated mode it contains a reserved
x86 opcode interpreted by the x86 decoder as a special kind of system
call.

Generally these calls are used to perform {}``intelligent benchmarking'':
the \texttt{\small ptlcall\_switch\_to\_sim()} call is made at the
top of the main loop of a benchmark after initialization, while the
\texttt{\small ptlcall\_switch\_to\_native()} call is inserted after
some number of iterations to stop simulation after a representative
subset of the code has completed. This intelligent approach is far
better than the blind {}``sample for N million cycles after S million
startup cycles'' approach used by most researchers.

Fortran programs will have to actually link in the \texttt{\small ptlcalls.o}
object file, since they cannot include C header files. The function
names that should be used in the Fortran code remain the same as those
from the \texttt{\small ptlcalls.h} header file.


\section{Notes on Benchmarking Methodology}

The x86 instruction set requires some different benchmarking techniques
than classical RISC ISAs. In particular, \textbf{IPC (Instructions
per Cycle) a NOT a good measure of performance for an x86 processor.}
Because one x86 instruction may be broken up into numerous uops, it
is never appropriate to compare IPC figures for committed x86 instructions
per clock with IPC values from a RISC machine. Furthermore, different
x86 implementations use varying numbers of uops per x86 instruction
as a matter of encoding, so even comparing the uop based IPC between
x86 implementations or RISC-like machines is inaccurate.

Users are strongly advised to use relative performance measures instead.
Comparing the total simulated cycle count required to complete a given
benchmark between different simulator configurations is much more
appropriate than IPC with the x86 instruction set. An example would
be \char`\"{}the baseline took 100M cycles, while our improved system
took 50M cycles, for a 2x improvement.


\section{Simulation Warmup Periods}

In some simulators, it is possible to quickly skip through a specific
number of instructions before starting to gather statistics, to avoid
including initialization code in the statistics. In PTLsim, this is
neither necessary nor desirable. Because PTLsim directly executes
your program on the host CPU until it switches to cycle accurate simulation
mode, there is no way to count instructions in this manner. 

Many researchers have gotten in the habit of blindly skipping a large
number of instructions in benchmarks to avoid profiling initialization
code. However, this is not a very intelligent policy: different benchmarks
have different startup times until the top of the main loop is reached,
and it is generally evident from the benchmark source code where that
point should be. Therefore, PTLsim supports \textbf{trigger points:}
by inserting a special function call (\texttt{\small ptlcall\_switch\_to\_sim})
within the benchmark source code and recompiling, the \texttt{\small -trigger}
PTLsim option can be used to run the code on the host CPU until the
trigger point is reached. If the source code is unavailable, the \texttt{\small -startrip}
\texttt{\emph{\small 0xADDRESS}} option will start full simulation
only at a specified address (e.g. function entry point). 

If you want to warm up the cache and branch predictors prior to starting
statistics collection, combine the \texttt{\small -trigger} option
with the \texttt{\small -snapshot} \texttt{\emph{\small N}} option,
to start full simulation at the top of the benchmark's main loop (where
the trigger call is), but only start gathering statistics \emph{N}
cycles later, after the processor is warmed up. Remember, since the
trigger point is placed \emph{after} all initialization code in the
benchmark, in general it is only necessary to use 10-20 million cycles
of warmup time before taking the first statistics snapshot. In this
time, the caches and branch predictor will almost always be completely
overwritten many times. This approach significantly speeds up the
simulation without any loss of accuracy compared to the \char`\"{}fast
simulation\char`\"{} mode provided by other simulators. 

In PTLstats, use the \texttt{\small -delta} option to make sure the
final statistics don't include the warmup period before the first
snapshot. To subtract the final snapshot from snapshot 0 (the first
snapshot after the warmup period), use a command similar to the following:

\begin{lyxcode}
{\small ptlstats~-deltastart~0~ptlsim.stats~>~ptlsim.stats.txt}{\small \par}
\end{lyxcode}

\section{Performance and Statistical Counters}

The full list of PTLsim performance and statistical counters is given
in Section \ref{sec:PerformanceCounters}.


\chapter{\label{part:x86andUops}x86 Instructions and Micro-Ops (uops)}


\section{\label{sec:UopIntro}Micro-Ops (uops) and TransOps}

PTLsim presents to user code a full implementation of the x86 and
x86-64 instruction set (both 32-bit and 64-bit modes), including most
user level instructions supported by the Intel Pentium 4 and AMD K8
microprocessors (i.e. all standard instructions, SSE/SSE2, x86-64
and most of x87 FP). At the present stage of development, the vast
majority of all userspace instructions are implemented. 

The x86 instruction set is based on the two-operand CISC concept of
load-and-compute and load-compute-store. However, modern x86 processors
(including PTLsim) do not directly execute complex x86 instructions.
Instead, these processors translate each x86 instruction into a series
of micro-operations (\emph{uops}) very similar to classical load-store
RISC instructions. Uops can be executed very efficiently on an out
of order core, unlike x86 instructions. In PTLsim, uops have three
source registers and one destination register. They may generate a
64-bit result and various x86 status flags, or may be loads, stores
or branches.

The x86 instruction decoding process initially generates translated
uops (\emph{transops}), which have a slightly different structure
than the true uops used in the processor core. Specifically, sources
and destinations are represented as un-renamed architectural registers
(or special temporary register numbers), and a variety of additional
information is attached to each uop only needed during the renaming
and retirement process. TransOps (represented by the \texttt{\small TransOp}
structure) consist of the following:

\begin{itemize}
\item \texttt{\small som}: Start of Macro-Op. Since x86 instructions may
consist of multiple transops, the first transop in the sequence has
its \texttt{\small som} bit set to indicate this.
\item \texttt{\small eom}: End of Macro-Op. This bit is set for the last
transop in a given x86 instruction (which may also be the first uop
for single-uop instructions)
\item \texttt{\small bytes}: Number of bytes in the corresponding x86 instruction
(1-15). This is only valid for a SOM uop.
\item \texttt{\small opcode}: the uop (not x86) opcode
\item \texttt{\small size}: the effective operation size (0-3, for 1/2/4/8
bytes)
\item \texttt{\small cond:} the x86 condition code for branches, selects,
sets, etc. For loads and stores, this field is reused to specify unaligned
access information as described later.
\item \texttt{\small setflags}: subset of the x86 flags set by this uop
(see Section \ref{sub:FlagsManagement})
\item \texttt{\small internal}: set for certain microcode operations. For
instance, loads and stores marked internal access on-chip registers
or buffers invisible to x86 code (e.g. machine state registers, segmentation
caches, floating point constant tables, etc).
\item \texttt{\small rd}, \texttt{\small ra}, \texttt{\small rb}, \texttt{\small rc}:
the architectural source and destination registers (see Section \ref{sub:RegisterRenaming})
\item \texttt{\small extshift}: shift amount (0-3 bits) used for shifted
adds (x86 memory addressing and LEA). The \texttt{\small rc} operand
is shifted left by this amount.
\item \texttt{\small cachelevel}: used for prefetching and non-temporal
loads and stores
\item \texttt{\small rbimm} and \texttt{\small rcimm}: signed 64-bit immediates
for the rb and rc operands. These are selected by specifying the special
constant \texttt{\small REG\_imm} in the \texttt{\small rb} and \texttt{\small rc}
fields, respectively.
\item \texttt{\small riptaken}: for branches only, the 64-bit target RIP
of the branch if it were taken.
\item \texttt{\small ripseq}: for branches only, the 64-bit sequential RIP
of the branch if it were not taken.
\end{itemize}
There may be other fields used for debugging or not relevant to the
out of order version of the simulator; these should be ignored.


\section{Descriptions of uops}

Section \ref{sec:UopReference} describes the semantics and encoding
of all uops supported by the PTLsim processor model. The following
is an overview of these uops.


\section{Simple Fast Path Instructions}

Simple integer and floating point operations are fairly straightforward
to decode into loads, stores and ALU operations; a typical load-op-store
ALU operation will consist of a load to fetch one operand, the ALU
operation itself, and a store to write the result. The instruction
set also implements a number of important but complex instructions
with bizarre semantics; typically the translator will synthesize and
inject into the uop stream up to 8 uops for more complex instructions. 


\section{x86-64}

The 64-bit x86-64 instruction set is a fairly straightforward extension
of the 32-bit IA-32 (x86) instruction set. The x86-64 ISA was introduced
by AMD in 2000 with its K8 microarchitecture; the same instructions
were subsequently plagiarized by Intel under a different name several
years later. In addition to extending all integer registers and ALU
datapaths to 64 bits, x86-64 also provides a total of 16 integer general
purpose registers and 16 SSE (vector floating and fixed point) registers.
It also introduced several 64-bit address space simplifications, including
RIP-relative addressing and corresponding new addressing modes, and
eliminated a number of legacy features from 64-bit mode, including
segmentation, BCD arithmetic, some byte register manipulation, etc.
Limited forms of segmentation are still present to allow thread local
storage and mark code segments as 64-bit. In general, the encoding
of x86-64 and x86 are very similar, with 64-bit mode adding a one
byte REX prefix to specify additional bits for source and destination
register indexes and effective address size. As a result, both variants
can be decoded by similar decoding logic into a common set of uops.


\section{\label{sec:OperationSizes}Operation Sizes}

Most x86-64 instructions can operate on 8, 16, 32 or 64 bits of a
given register. For 8-bit and 16-bit operations, only the low 8 or
16 bits of the destination register are actually updated; 32-bit and
64-bit operations are zero extended as with RISC architectures. As
a result, a dependency on the old destination register may be introduced
so merging can be performed. Fortunately, since x86 features destructive
overwrites of the destination register (i.e. the \texttt{\small rd}
and \texttt{\small ra} operands are the same), the \texttt{\small ra}
operand is generally already a dependency. Thus, the PT2x uop encoding
reserves 2 bits to specify the operation size; the low bits of the
new result are automatically merged with the old destination value
(in \texttt{\small ra}) as part of the ALU logic. This applies to
the \texttt{\small mov} uop as well, allowing operations like {}``\texttt{\small mov
al,bl}'' in one uop. Loads do not support this mode, so loads into
8-bit and 16-bit registers must be followed by a separate \texttt{\small mov}
uop to truncate and merge the loaded value into the old destination
properly.

The x86 ISA defines some bizarre byte operations as a carryover from
the ancient 8086 architecture; for instance, it is possible to address
the second byte of many integer registers as a separate register (i.e.
as \texttt{\small ah}, \texttt{\small bh}, \texttt{\small ch}, \texttt{\small dh}).
The \texttt{\small mask} uop is used for handling this rare but important
set of operations.


\section{\label{sub:FlagsManagement}Flags Management and Register Renaming}

Many x86 arithmetic instructions modify some or all of the processor's
numerous status and condition flag bits, but only 5 are relevant to
normal execution: Zero, Parity, Sign, Overflow, Carry. In accordance
with the well-known {}``ZAPS rule'', any instruction that updates
any of the Z/P/S flags updates all three flags, so in reality only
three flag entities need to be tracked: ZPS, O, F ({}``ZAPS'' also
includes an Auxiliary flag not accessible by most modern user instructions;
it is irrelevant to the discussion below).

The x86 flag update semantics can hamper out of order execution, so
we use a simple and well known solution. The 5 flag bits are attached
to each result and physical register (along with the invalid and waiting
bits described in Section \ref{sec:PhysicalRegisters}); these bits
are then consumed along with the actual result value by any consumers
that also need to access the flags. It should be noted that not all
uops generate all the flags as well as a 64-bit result, and some uops
only generate flags and no result data. 

The register renaming mechanism is aware of these semantics, and tracks
the latest x86 instruction in program order to update each set of
flags (ZAPS, C, O); this allows branches and other flag consumers
to directly access the result with the most recent program-ordered
flag updates yet still allows full out of order scheduling. To do
this, x86 processors maintain three separate rename table entries
for the ZAPS, CF, OF flags in addition to the register rename table
entry, any or all of which may be updated when uops are renamed. The
\texttt{\small TransOp} structure for each uop has a 3-bit \texttt{\small setflags}
field filled out during decoding in accordance with x86 semantics;
the \texttt{\small SETFLAG\_ZF}, \texttt{\small SETFLAG\_CF}, \texttt{\small SETFLAG\_OF}
bits in this field are used to determine which of the ZPS, O, F flag
subsets to rename.

As mentioned above, any consumer of the flags needs to consult at
most three distinct sources: the last ZAPS producer, the Carry producer
and the Overflow producer. This conveniently fits into PTLsim's three-operand
uop semantics. Various special uops access the flags associated with
an operand rather than the 64-bit operand data itself. Branches always
take two flag sources, since in x86 this is enough to evaluate any
possible condition code combination (the \texttt{\small cond\_code\_to\_flag\_regs}
array provides this mapping). Various ALU instructions consume only
the flags part of a source physical register; these include \texttt{\small addc}
(add with carry), \texttt{\small rcl/rcr} (rotate carry), \texttt{\small sel.}\texttt{\emph{\small cc}}
(select for conditional moves) and so on. Finally, the \texttt{\small collcc}
uop takes three operands (the latest producer of the ZAPS, CF and
OF flags) and merges the flag components of each operand into a single
flag set as its result. These uops are all documented in Section \ref{sec:UopReference}.


\section{\label{sub:UnalignedLoadsAndStores}Unaligned Loads and Stores}

Compared to RISC architectures, the x86 architecture is infamous for
its relatively widespread use of unaligned memory operations; any
implementation must efficiently handle this scenario. Fortunately,
analysis shows that unaligned accesses are rarely in the performance
intensive parts of a modern program, so we can aggressively eliminate
them on contact through rescheduling. PTLsim does this by initially
causing all unaligned loads and stores to raise an \texttt{\small UnalignedAccess}
internal exception, forcing a rollback of the current trace. To locate
the RIP of the offending x86 instruction, the sequential version of
the trace is then executed until the unaligned access is encountered
again. At this point, a special {}``unaligned'' bit is set for the
problem load or store in its translated basic block representation.
The sequential basic block is then retranslated such that when the
x86 instruction with the problem RIP is encountered, the offending
load or store is split into two aligned loads or stores.

PTLsim includes special uops to handle loads and stores split into
two in this manner. The \texttt{\small ld.lo} uop rounds down its
effective address $\left\lfloor A\right\rfloor $ to the nearest 64-bit
boundary and performs the load. The \texttt{\small ld.hi} uop rounds
up to $\left\lceil A+8\right\rceil $, performs another load, then
takes as its third rc operand the first (\texttt{\small ld.lo}) load's
result. The two loads are concatenated into a 128-bit word and the
final unaligned data is extracted. Stores are handled in a similar
manner, with \texttt{\small st.lo} and \texttt{\small st.hi} rounding
down and up to store parts of the unaligned value in adjacent 64-bit
blocks. Just as with normal loads and stores, these unaligned load
or store pairs access separate store buffers for each half as if they
were independent.


\section{Repeated String Operations}

The x86 architecture allows for repeated string operations, including
block moves, stores, compares and scans. The iteration count of these
repeated operations depends on a combination of the \texttt{\small rcx}
register and the flags set by the repeated operation (e.g. compare).
To translate these instructions, PTL treats the \texttt{\small rep
xxx} instruction as a single basic block; any basic block in progress
before the repeat instruction is capped and the repeat is translated
as a separate basic block. This conveniently lets us unroll and optimize
the repeated loop just like any other basic block. To handle the unusual
case where the repeat count is zero, a check instruction is inserted
at the top of the loop to protect against this case; PTL simply bypasses
the offending block if the check fails.


\section{\label{sec:ShiftRotateProblems}Problem Instructions}

The shift and rotate instructions have some of the most bizarre semantics
in the entire x86 instruction set: they may or may not modify a subset
of the flags depending on the rotation count operand, which we may
not even know until the instruction issues. For fixed shifts and rotates,
these semantics can be preserved by the uops generated, however variable
rotations are more complex. The \texttt{\small collcc} uop is put
to use here to collect all flags; the collected result is then fed
into the shift or rotate uop as its \texttt{\small rc} operand; the
uop then replicates the precise x86 behavior (including rotates using
the carry flag) according to its input operands.


\section{SSE Support}

PTLsim provides full support for SSE and SSE2 vector floating point
and fixed point, in both scalar and vector mode. As is done in the
AMD K8, each SSE operation on a 128-bit vector is split into two 64-bit
halves; each half (possibly consisting of a 64-bit load and one or
more FPU operations) is scheduled independently. Because SSE instructions
do not set flags like x86 integer instructions, architectural state
management can be restricted to the 16 128-bit SSE registers (represented
as 32 paired 64-bit registers) and a single \texttt{\small mxcsr}
architectural register containing sticky exception bits, which has
no effect on out of order execution). The processor's floating point
units can operate in either 64-bit IEEE double precision mode or on
two parallel 32-bit single precision values.


\section{\label{sub:x87-Floating-Point}x87 Floating Point}

The legacy x87 floating point architecture is the bane of all x86
processor vendors' existence, largely because its stack based nature
makes out of order processing so difficult. While there are certainly
ways of translating stack based instruction sets into flat addressing
for scheduling purposes, we do not do this. Fortunately, following
the Pentium III and AMD Athlon's introduction, x87 is rapidly headed
for planned obsolescence; most major applications released within
the last three years now use SSE instructions for their floating point
needs either exclusively or in all performance critical parts. To
this end, even Intel has relegated x86 support on the Pentium 4 to
a separate low performance in-order legacy unit, and AMD has severely
restricted its use in 64-bit mode. For this reason, PTLsim translates
legacy x87 instructions into a serialized, program ordered and emulated
form; the hardware does not contain any x87-style 80-bit floating
point registers (all floating point hardware is 32-bit and 64-bit
IEEE compliant). We have noticed little to no performance problem
from this approach when examining typical binaries, which rarely if
ever still use x87 instructions in compute-intensive code.


\section{Assists}

Some operations are too complex to inline directly into the uop stream.
To perform these instructions, a special uop (\texttt{\small brp}:
branch private) is executed to branch to an \emph{assist} function
implemented in microcode. In PTLsim, some assist functions are implemented
as regular C/C++ or assembly language code when they interact with
the rest of the virtual machine. Examples of instructions requiring
assists include system calls, interrupts, some forms of integer division,
handling of rare floating point conditions, CPUID, MSR reads/writes,
etc. These are listed in the \texttt{\small ASSIST\_xxx} enum found
in \texttt{\small translate-x86.cpp} and \texttt{\small ptlhwdef.h}.

When the processor issues an assist (\texttt{\small brp} uop), the
frontend pipeline is stalled and execution waits until the \texttt{\small brp}
commits, at which point an assist function within PTLsim is called.
In a real processor there are more efficient ways of doing this without
flushing the pipeline, however in PTLsim assists are sufficiently
rare that the performance impact is negligible and this approach significantly
reduces complexity. The exact mechanism used is described in Section
\ref{sec:PipelineFlushesAndBarriers}.


\part{\label{part:OutOfOrderModel}Out of Order Processor Model}


\chapter{\label{sec:OutOfOrderFeatures}Introduction}

PTLsim completely models a modern out of order x86-64 compatible processor
and cache hierarchy with cycle accurate simulation. The basic microarchitecture
of this model is most similar to the Intel Pentium 4 processor, but
incorporates some ideas from AMD K8, IBM Power4/Power5 and Alpha EV8.
The following is a summary of the characteristics of this processor
model:

\begin{itemize}
\item The simulator directly fetches pre-decoded micro-operations (Section
\ref{sec:FetchStage}) but can simulate cache accesses as if x86 instructions
were being decoded on fetch
\item Branch prediction is configurable; PTLsim currently includes various
models including a hybrid g-share based predictor, bimodal predictors,
saturating counters, etc.
\item Register renaming takes into account x86 quirks such as flags renaming
(Section \ref{sub:FlagsManagement})
\item Front end pipeline has configurable number of cycles to simulate x86
decoding or other tasks; this is used for adjusting the branch mispredict
penalty
\item Unified physical and architectural register file maps both in-flight
uops as well as committed architectural register values. Two rename
tables (speculative and committed register rename tables) are used
to track which physical registers are currently mapped to architectural
registers.
\item Unified physical register file for both integer and floating point
values.
\item Operands are read from the physical register file immediately before
issue. Unlike in some microprocessors, PTLsim does not do speculative
scheduling: the schedule and register read loop is assumed to take
one cycle.
\item Issue queues based on a collapsing design use broadcast based matching
to wake up instructions.
\item Clustered microarchitecture is highly configurable, allowing multi-cycle
latencies between clusters and multiple issue queues within the same
logical cluster.
\item Functional units, mapping of functional units to clusters, issue ports
and issue queues and uop latencies are all configurable.
\item Speculation recovery from branch mispredictions and load/store aliasing
uses the forward walk method to recover the rename tables, then annuls
all uops after and optionally including the mis-speculated uop.
\item Replay of loads and stores after store to load forwarding and store
to store merging dependencies are discovered.
\item Stores may issue even before data to store is known; the store uop
is replayed when all operands arrive.
\item Load and store queues use partial chunk address matching and store
merging for high performance and easy circuit implementation.
\item Prediction of load/store aliasing to avoid mis-speculation recovery
overhead.
\item Prediction and splitting of unaligned loads and stores to avoid mis-speculation
overhead
\item Commit unit supports stalling until all uops in an x86 instruction
are complete, to make x86 instruction commitment atomic
\end{itemize}
The PTLsim model is fully configurable in terms of the sizes of key
structures, pipeline widths, latency and bandwidth and numerous other
features.


\chapter{Fetch Stage}


\section{\label{sec:FetchStage}Instruction Fetching and the Basic Block Cache}

As described in Section \ref{sec:UopIntro}, x86 instructions are
decoded into transops prior to actual execution by the out of order
core. Some processors do this translation as x86 instructions are
fetched from an L1 instruction cache, while others use a trace cache
to store pre-decoded uops. PTLsim takes a middle ground to allow maximum
simulation flexibility. Specifically, the Fetch stage accesses the
L1 instruction cache and stalls on cache misses as if it were fetching
several variable length x86 instructions per cycle. However, actually
decoding x86 instructions into uops over and over again during simulation
would be extraordinarily slow. 

Therefore, for \emph{simulation purposes only}, PTLsim maintains a
\emph{basic block cache} containing the program ordered translated
uop (\emph{transop}) sequence for previously decoded basic blocks
in the program. Each basic block (\texttt{\small BasicBlock} structure)
consists of up to 64 transops and is terminated by either a control
flow operation (conditional, unconditional, indirect branch) or a
barrier operation (e.g. system call, synchronizing instruction, etc).
During the fetch process (implemented in the \texttt{\small fetch()}
function), PTLsim looks up the current RIP to fetch from and uses
the basic block cache to map that RIP to a BasicBlock structure. The
transop stream is then read from that decoded basic block in lieu
of decoding the x86 instructions again. As execution runs off the
end of each decoded basic block, the fetch unit checks if the next
RIP exists in the basic block cache. If so, the next block is streamed
into the fetch queue. Otherwise, the x86-to-transop translator is
called via \texttt{\small translate\_basic\_block()} to translate
an entire basic block at the current RIP before resuming execution.
Since the basic block cache is for simulation purposes only, this
adds no additional cycles to the simulated program.

An additional optimization, called \emph{synthesis}, is also used:
each uop in the basic block is mapped to the address of a native PTLsim
function implementing the semantics of that uop. This saves us from
having to use a large jump table later on, and can map uops to pre-compiled
templates that avoid nearly all further decoding of the uop during
execution.


\section{Fetch Queue}

Each transop fetched into the pipeline is immediately assigned a monotonically
increasing \emph{uuid} (universally unique identifier) to uniquely
track it for debugging and statistical purposes. The fetch unit attaches
additional information to each transop (such as the uop's uuid and
the RIP of the corresponding x86 instruction) to form a \texttt{\small FetchBufferEntry}
structure. This fetch buffer is then placed into the fetch queue (\texttt{\small fetchq})
assuming it isn't full (if it is, the fetch stage stalls). As the
fetch unit encounters transops with their EOM (end of macro-op) bit
set, the fetch RIP is advanced to the next x86 instruction according
to the instruction length stored in the SOM transop.

Branch uops trigger the branch prediction mechanism used to select
the next fetch RIP. Based on various information encoded in the branch
transop and the next RIP \emph{after} the x86 instruction containing
the branch, the \texttt{\small branchpred.predict()} function is used
to redirect fetching. If the branch is predicted not taken, the sense
of the branch's condition code is inverted and the transop's \texttt{\small riptaken}
and \texttt{\small ripseq} fields are swapped; this ensures all branches
are considered correct only if taken. Indirect branches (jumps) have
their \texttt{\small riptaken} field overwritten by the predicted
target address.


\chapter{Frontend and Key Structures}


\section{Resource Allocation}

During the Allocate stage, PTLsim dequeues uops from the fetch queue,
ensures all resources needed by those uops are free, and assigns resources
to each uop as needed. These resources include Reorder Buffer (ROB)
slots, physical registers and load store queue (LSQ) entries. In the
event that the fetch queue is empty or any of the ROB, physical register
file, load queue or store queue is full, the allocation stage stalls
until some resources become available.


\section{Reorder Buffer Entries}

The Reorder Buffer (ROB) in the PTLsim out of order model works exactly
like a traditional ROB: as a queue, entries are allocated from the
tail and committed from the head. Each \texttt{\small ReorderBufferEntry}
structure is the central tracking structure for uops in the pipeline.
This structure contains a variety of fields including:

\begin{itemize}
\item The decoded uop (\texttt{\small uop} field). This is the fully decoded
\texttt{\small TransOp} augmented with fetch-related information like
the uop's UUID, RIP and branch predictor information as described
in the Fetch stage (Section \ref{sec:FetchStage}).
\item Current state of the ROB entry and uop (\texttt{\small current\_state\_list};
see below)
\item Pointers to the physical register (\texttt{\small physreg}), LSQ entry
(\texttt{\small lsq}) and other resources allocated to the uop
\item Pointers to the three physical register operands to the uop, as well
as a possible store dependency used in replay scheduling (described
later)
\item Various cycle counters and related fields for simulating progress
through the pipeline
\end{itemize}

\subsection{ROB States}

Each ROB entry and corresponding uop can be in one of a number of
states describing its progress through the simulator state machine.
ROBs are linked into linked lists according to their current state;
these lists are named \texttt{\small rob\_}\emph{statename}\texttt{\small \_list}.
The \texttt{\small current\_state\_list} field specifies the list
the ROB is currently on. ROBs can be moved between states using the
\texttt{\small ROB::changestate(}\texttt{\emph{\small statelist}}\texttt{\small )}
method. The specific states will be described below as they are encountered.

\textbf{\emph{NOTE:}} the terms {}``ROB entry'' (singular) and {}``uop''
are used interchangeably from now on unless otherwise stated, since
there is a 1:1 mapping between the two.


\section{\label{sec:PhysicalRegisters}Physical Registers}


\subsection{Physical Registers}

Physical registers are represented in PTLsim by the \texttt{\small PhysicalRegister}
structure. Physical registers store several components:

\begin{itemize}
\item Index of the physical register (\texttt{\small idx}) and the physical
register file id (\texttt{\small rfid}) to which it belongs
\item The actual 64-bit register data
\item x86 flags: Z, P, S, O, C. These are discussed below in Section \ref{sub:FlagsManagement}.
\item Waiting flag (\texttt{\small FLAG\_WAIT}) for results not yet ready
\item Invalid flag (\texttt{\small FLAG\_INVAL}) for ready results which
encountered an exception. The exception code is written to the data
field in lieu of the real result
\item Current state of the physical register (\texttt{\small state})
\item ROB currently owning this physical register, or architectural register
mapping this physical register
\item Reference counter for the physical register. This is required for
reasons described in Section \ref{sub:PhysicalRegisterRecyclingComplications}.
\end{itemize}

\subsection{Physical Register File}

PTLsim uses a flexible physical register file model in which multiple
physical register files with different sizes and properties can optionally
be defined. Each physical register file in the \texttt{\small physregfiles{[}{]}}
array can be made accessible from one or more clusters. For instance,
uops which execute on floating point clusters can be forced to always
allocate a register in the floating point register file, or each cluster
can have a dedicated register file.

Various heuristics can also be used for selecting the register file
into which a result is placed. The default heuristic simply finds
the first acceptable physical register file with a free register.
Acceptable physical register files are those register files in which
the uop being allocated is allowed to write its result; this is configurable
based on clustering as described below. Other allocation policies,
such as alternation between available register files and dependency
based register allocation, are all possible by modifying the \texttt{\small rename()}
function where physical registers are allocated..

In each physical register file, physical register number 0 is defined
as the \emph{null register:} it always contains the value zero and
is used as an operand anywhere the zero value (or no value at all)
is required.

Physical register files are configured in \texttt{\small ooohwdef.h}.
The \texttt{\small PhysicalRegisterFile{[}{]}} array is defined to
declare each register file by name, register file ID (RFID, from 0
to the number of register files) and size. The \texttt{\small MAX\_PHYS\_REG\_FILE\_SIZE}
parameter must be greater than the largest physical register in the
processor.


\subsection{Physical Register States}

Each physical register can be in one of several states at any given
time. For each physical register file, PTLsim maintains linked lists
(the \texttt{\small PhysicalRegisterFile.states{[}}\emph{statename}\texttt{\small {]}}
lists) to track which registers are in each state. The \texttt{\small state}
field in each physical register specifies its state, and implies that
the physical register is on the list \texttt{\small physregfiles{[}physreg.}\texttt{\textbf{\small rfid}}\texttt{\small {]}.states{[}physreg.}\texttt{\textbf{\small state}}\texttt{\small {]}}.
The valid states are:

\begin{itemize}
\item \textbf{\emph{free:}} the register is not allocated to any uop.
\item \textbf{\emph{waiting:}} the register has been allocated to a uop
but that uop is waiting to issue.
\item \textbf{\emph{bypass:}} the uop associated with the register has issued
and produced a value (or encountered an exception), but that value
is only on the bypass network - it has not actually been written back
yet. For simulation purposes only, uops immediately write their results
into the physical register as soon as they issue, even though technically
the result is still only on the bypass network. This helps simplify
the simulator considerably without compromising accuracy.
\item \textbf{\emph{written:}} the uop associated with the register has
passed through the writeback stage and the value of the physical register
is now up to date; all future consumers will read the uop's result
from this physical register.
\item \textbf{\emph{arch:}} the physical register is currently mapped to
one of the architectural registers; it has no associated uop currently
in the pipeline
\item \textbf{\emph{pendingfree:}} this is a special state described in
Section \ref{sub:PhysicalRegisterRecyclingComplications}.
\end{itemize}
One physical register is allocated to each uop and moved into the
\emph{waiting} state, regardless of which type of uop it is. For integer,
floating point and load uops, the physical register holds the actual
numerical value generated by the corresponding uop. Branch uops place
the target RIP of the branch in a physical register. Store uops place
the merged data to store in the register. Technically branches and
stores do not need physical registers, but to keep the processor design
simple, they are allocated registers anyway.


\section{\label{sec:LoadStoreQueueEntry}Load Store Queue Entries}

Load Store Queue (LSQ) Entries (the \texttt{\small LoadStoreQueueEntry}
structure in PTLsim) are used to track additional information about
loads and stores in the pipeline that cannot be represented by a physical
register. Specifically, LSQ entries track:

\begin{itemize}
\item \textbf{Physical address} of the corresponding load or store
\item \textbf{Data} field (64 bits) stores the loaded value (for loads)
or the value to store (for stores)
\item \textbf{Address valid} bit flag indicates if the load or store knows
its effective physical address yet. If set, the physical address field
is valid.
\item \textbf{Data valid} bit flag indicates if the data field is valid.
For loads, this is set when the data has arrived from the cache. For
stores, this is set when the data to store becomes ready and is merged.
\item \textbf{Invalid} bit flag is set if an exception occurs in the corresponding
load or store.
\end{itemize}
The \texttt{\small LoadStoreQueueEntry} structure is technically a
superset of a structure known as an \emph{SFR} (Store Forwarding Register),
which completely represents any load or store and can be passed between
PTLsim subsystems easily. One LSQ entry is allocated to each load
or store during the Allocate stage.

In real processors, the load queue (LDQ) and store queue (STQ) are
physically separate for circuit complexity reasons. However, in PTLsim
a unified LSQ is used to make searching operations easier. One additional
bit flag (\texttt{\small store} bit) specifies whether an LSQ entry
is a load or store.


\subsection{\label{sub:RegisterRenaming}Register Renaming}

The basic register renaming process in the PTLsim x86 model is very
similar to classical register renaming, with the exception of the
flags complications described in Section \ref{sub:FlagsManagement}.
Two versions of the register rename table (RRT) are maintained: a
\emph{speculative RRT} which is updated as uops are renamed, and a
\emph{commit RRT}, which is only updated when uops successfully commit.
Since the simulator implements a unified physical and architectural
register file, the commit process does not actually involve any data
movement between physical and architectural registers: only the commit
RRT needs to be updated. The commit RRT is used only for exception
and branch mispredict recovery, since it holds the last known good
mapping of architectural to physical registers.

Each rename table contains 80 entries as shown in Table \ref{table:ArchitecturalRegisters}.
This table maps architectural registers and pseudo-registers to the
most up to date physical registers for the following:

\begin{itemize}
\item 16 x86-64 integer registers
\item 16 128-bit SSE registers (represented as separate 64-bit high and
low halves)
\item ZAPS, CF, OF flag sets described in Section \ref{sub:FlagsManagement}.
These rename table entries point to the physical register (with attached
flags) of the most recent uop in program order to update any or all
of the ZAPS, CF, OF flag sets, respectively.
\item Various integer and x87 status registers
\item Temporary pseudo-registers \texttt{\small temp0}-\texttt{\small temp7}
not visible to x86 code but required to hold temporaries (e.g. generated
addresses or value to swap in \texttt{\small xchg} instructions).
\item Special fixed values, e.g. \texttt{\small zero}, \texttt{\small imm}
(value is in immediate field), \texttt{\small mem} (destination of
stores)
\end{itemize}
%
\begin{table}

\caption{\label{table:ArchitecturalRegisters}Architectural registers and
pseudo-registers used for renaming.}

\noindent \begin{center}\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline 
\multicolumn{9}{|c|}{\textsf{\small Architectural Registers and Pseudo-Registers}}\tabularnewline
\hline
\hline 
\textsf{\small 0}&
\texttt{\small rax}&
\texttt{\small rcx}&
\texttt{\small rdx}&
\texttt{\small rbx}&
\texttt{\small rsp}&
\texttt{\small rbp}&
\texttt{\small rsi}&
\texttt{\small rdi}\tabularnewline
\hline 
\textsf{\small 8}&
\texttt{\small r8}&
\texttt{\small r9}&
\texttt{\small r10}&
\texttt{\small r11}&
\texttt{\small r12}&
\texttt{\small r13}&
\texttt{\small r14}&
\texttt{\small r15}\tabularnewline
\hline 
\textsf{\small 16}&
\texttt{\small xmml0}&
\texttt{\small xmmh0}&
\texttt{\small xmml1}&
\texttt{\small xmmh1}&
\texttt{\small xmml2}&
\texttt{\small xmmh2}&
\texttt{\small xmml3}&
\texttt{\small xmmh3}\tabularnewline
\hline 
\textsf{\small 24}&
\texttt{\small xmml4}&
\texttt{\small xmmh4}&
\texttt{\small xmml5}&
\texttt{\small xmmh5}&
\texttt{\small xmml6}&
\texttt{\small xmmh6}&
\texttt{\small xmml7}&
\texttt{\small xmmh7}\tabularnewline
\hline 
\textsf{\small 32}&
\texttt{\small xmml8}&
\texttt{\small xmmh8}&
\texttt{\small xmml9}&
\texttt{\small xmmh9}&
\texttt{\small xmml10}&
\texttt{\small xmmh10}&
\texttt{\small xmml11}&
\texttt{\small xmmh11}\tabularnewline
\hline 
\textsf{\small 40}&
\texttt{\small xmml12}&
\texttt{\small xmmh12}&
\texttt{\small xmml13}&
\texttt{\small xmmh13}&
\texttt{\small xmml14}&
\texttt{\small xmmh14}&
\texttt{\small xmml15}&
\texttt{\small xmmh15}\tabularnewline
\hline 
\textsf{\small 48}&
\texttt{\small fptos}&
\texttt{\small fpsw}&
\texttt{\small fpcw}&
\texttt{\small fptags}&
\texttt{\small fp4}&
\texttt{\small fp5}&
\texttt{\small fp6}&
\texttt{\small fp7}\tabularnewline
\hline 
\textsf{\small 56}&
\texttt{\small rip}&
\texttt{\small flags}&
\texttt{\small sr3}&
\texttt{\small mxcsr}&
\texttt{\small sr0}&
\texttt{\small sr1}&
\texttt{\small sr2}&
\texttt{\small zero}\tabularnewline
\hline
\hline 
\textsf{\small 64}&
\texttt{\small temp0}&
\texttt{\small temp1}&
\texttt{\small temp2}&
\texttt{\small temp3}&
\texttt{\small temp4}&
\texttt{\small temp5}&
\texttt{\small temp6}&
\texttt{\small temp7}\tabularnewline
\hline 
\textsf{\small 72}&
\texttt{\small zf}&
\texttt{\small cf}&
\texttt{\small of}&
\texttt{\small imm}&
\texttt{\small mem}&
\texttt{\small temp8}&
\texttt{\small temp9}&
\texttt{\small temp10}\tabularnewline
\hline
\end{tabular}\end{center}
\end{table}


Once the uop's three architectural register sources are mapped to
physical registers, these physical registers are placed in the \texttt{\small operands{[}}0,1,2\texttt{\small {]}}
fields. The fourth operand field, \texttt{\small operands{[}}3\texttt{\small {]}},
is used to hold a store buffer dependency for loads and stores; this
will be discussed later. The speculative RRT entries for both the
destination physical register and any modified flags are then overwritten.
Finally, the ROB is moved into the \textbf{\emph{frontend}} state.


\subsection{External State}

Since the rest of the simulator outside of the out of order core does
not know about the RRTs and expects architectural registers to be
in a standardized format, the system-wide \texttt{\small ctx} structure
of class \texttt{\small CoreState} is used to house the architectural
register file. The \texttt{\small REG\_flags} and \texttt{\small REG\_rip}
entries of this structure are directly updated by the out of order
core as instructions commit.


\section{Frontend Stages}

To simulate various processor frontend pipeline depths, ROBs are placed
in the \emph{frontend} state for a user-selectable number of cycles.
In the \texttt{\small frontend()} function, the \texttt{\small cycles\_left}
field in each ROB is decremented until it becomes zero. At this point,
the uop is moved to the \textbf{\emph{ready\_to\_dispatch}} state.
This feature can be used to simulate various branch mispredict penalties
by setting the \texttt{\small FRONTEND\_STAGES} constant.


\chapter{\label{sec:ClusterDispatchScheduleIssue}Scheduling, Dispatch and
Issue}


\section{\label{sec:Clustering}Clustering and Issue Queue Configuration}

The PTLsim out of order model can simulate an arbitrarily complex
set of functional units grouped into \emph{clusters}. Clusters are
specified by the \texttt{\small Cluster} class and are defined by
the \texttt{\small clusters{[}{]}} array in \texttt{\small ooohwdef.h}.
Each Cluster element defines the name of the cluster, which functional
units belong to the cluster (\texttt{\small fu\_mask} field) and the
maximum number of uops that can be issued in that cluster each cycle
(\texttt{\small issue\_width} field)

The \texttt{\small intercluster\_latency\_map} matrix defines the
forwarding latency, in cycles, between a given cluster and every other
cluster. If \texttt{\small intercluster\_latency\_map{[}}\emph{A}\texttt{\small {]}{[}}\emph{B}\texttt{\small {]}}
is \emph{L} cycles, this means that functional units in cluster \emph{B}
must wait \emph{L} cycles after a uop \emph{U} in cluster A completes
before cluster B's functional units can issue a uop dependent on \emph{U}'s
result. If the latency is zero between clusters \emph{A} and \emph{B},
producer and consumer uops in \emph{A} and \emph{B} can always be
issued back to back in subsequent cycles. Hence, the diagonal of the
forwarding latency matrix is always all zeros.

This clustering mechanism can be used to implement several features
of modern microprocessors. First, traditional clustering is possible,
in which it takes multiple additional cycles to forward results between
different clusters (for instance, one or more integer clusters and
a floating point unit). Second, several issue queues and corresponding
issue width limits can be defined within a given virtual cluster,
for instance to sort loads, stores and ALU operations into separate
issue queues with different policies. This is done by specifying an
inter-cluster latency of zero cycles between the relevant pseudo-clusters
with separate issue queues. Both of these uses are required to accurately
model most modern processors.

There is also an equivalent \texttt{\small intercluster\_bandwidth\_map}
matrix to specify the maximum number of values that can be routed
between any two clusters each cycle.

The \texttt{\small IssueQueue} template class is used to declare issue
queues; each cluster has its own issue queue. The syntax \texttt{\small IssueQueue<}\emph{size}\texttt{\small >}
\texttt{\small issueq\_}\emph{name}\texttt{\small ;} is used to declare
an issue queue with a specific size. In the current implementation,
the size can be from 1 to 64 slots. The macros \texttt{\small foreach\_issueq()},
\texttt{\small sched\_get\_all\_issueq\_free\_slots()} and \texttt{\small issueq\_operation\_on\_cluster\_with\_result()}
macros must be modified if the cluster and issue queue configuration
is changed to reflect all available clusters; the modifications required
should be obvious from the example code. These macros with switch
statements are required instead of a simple array since the issue
queues can be of different template types and sizes.


\section{Cluster Selection}

The \texttt{\small ReorderBufferEntry::select\_cluster()} function
is responsible for routing a given uop into a specific cluster at
the time it is dispatched; uops do not switch between clusters after
this.

Various heuristics are employed to select which cluster a given uop
should be routed to. In the reference implementation provided in \texttt{\small ooocore.cpp},
a weighted score is generated for each possible cluster by scanning
through the uop's operands to determine which cluster they will be
forwarded from. If a given operand's corresponding producer uop \emph{S}
is currently either dispatched to cluster \emph{C} but waiting to
execute or is still on the bypass network of cluster \emph{C}, then
cluster \emph{C}'s score is incremented. 

The final cluster is selected as the cluster with the highest score
out of the set of clusters which the uop can actually issue on (e.g.
a floating point uop cannot issue on a cluster with only integer units).
The \texttt{\small ReorderBufferEntry::executable\_on\_cluster\_mask}
bitmap can be used to further restrict which clusters a uop can be
dispatched to, for instance because certain clusters can only write
to certain physical register files. This mechanism is designed to
route each uop to the cluster in which the majority of its operands
will become available at the earliest time; in practice it works quite
well and variants of this technique are often used in real processors.


\section{\label{sec:Scheduling}Issue Queue Structure and Operation}

PTLsim implements issue queues in the \texttt{\small IssueQueue} template
class using the collapsing priority queue design used in most modern
processors. 

As each uop is dispatched, it is placed at the end of the issue queue
for its cluster and several associative arrays are updated to reflect
which operands the uop is still waiting for. In the IssueQueue class,
the \texttt{\small insert()} method takes the ROB index of the uop
(its \emph{tag} in issue queue terminology), the tags (ROB indices)
of its operands, and a map of which of the operands are ready versus
waiting. The ROB index is inserted into an associative array, and
the ROB index tags of any waiting operands are inserted into corresponding
slots in parallel arrays, one array per operand (in the current implementation,
up to 4 operands are tracked). If an operand was ready at dispatch
time, the slot for that operand in the corresponding array is marked
as invalid since there is no need to wake it up later. Notice that
the new slot is always at the end of the issue queue array; this is
made possible by the collapsing mechanism described below.

The issue queue maintains two bitmaps to track the state of each slot
in the queue. The \texttt{\small valid} bitmap indicates which slots
are occupied by uops, while the \texttt{\small issued} bitmap indicates
which of those uops have been issued. Together, these two bitmaps
form the state machine described in Table \ref{table:IssueQueueStateMachine}.

%
\begin{table}

\caption{\label{table:IssueQueueStateMachine}Issue Queue State Machine}

\noindent \begin{center}\begin{tabular}{|c|c|l|}
\hline 
Valid&
Issued&
Meaning\tabularnewline
\hline
\hline 
\texttt{\small 0}&
\texttt{\small 0}&
Unused slot\tabularnewline
\hline 
\texttt{\small 0}&
\texttt{\small 1}&
(invalid)\tabularnewline
\hline 
\texttt{\small 1}&
\texttt{\small 0}&
Dispatched but waiting for operands\tabularnewline
\hline 
\texttt{\small 1}&
\texttt{\small 1}&
Issued to a functional unit but not yet completed\tabularnewline
\hline
\end{tabular}\end{center}
\end{table}


After \texttt{\small insert()} is called, the slot is placed in the
dispatched state. As each uop completes, its tag (ROB index) is broadcast
using the \texttt{\small broadcast()} method to one or more issue
queues accessible in that cycle. Because of clustering, some issue
queues will receive the broadcast later than others; this is discussed
below. Each slot in each of the four operand arrays is compared against
the broadcast value. If the operand tag in that slot is valid and
matches the broadcast tag, the slot (in one of the operand arrays
only, not the entire issue queue) is invalidated to indicate it is
ready and no longer waiting for further broadcasts.

Every cycle, the \texttt{\small clock()} method uses the \texttt{\small valid}
and \texttt{\small issued} bitmaps together with the valid bitmaps
of each of the operand arrays to compute which issue queue slots in
the dispatched state are no longer waiting on any of their operands.
This bitmap of ready slots is then latched into the \texttt{\small allready}
bitmap.

The \texttt{\small issue()} method simply finds the index of the first
set bit in the \texttt{\small allready} bitmap (this is the slot of
the oldest ready uop in program order), marks the corresponding slot
as issued, and returns the slot. The processor then selects a functional
unit for the uop in that slot and executes it via the \texttt{\small ReorderBufferEntry::issue()}
method. After the uop has completed execution (i.e. it cannot possibly
be replayed), the \texttt{\small release()} method is called to remove
the slot from the issue queue, freeing it up for incoming uops in
the dispatch stage. The collapsing design of the issue queue means
that the slot is not simply marked as invalid - all slots after it
are physically shifted left by one, leaving a free slot at the end
of the array. This design is relatively simple to implement in hardware
and makes determining the oldest ready to issue uop very trivial.

Because of the collapsing mechanism, it is critical to note that the
slot index returned by \texttt{\small issue()} will become invalid
after the next call to the \texttt{\small remove()} method; hence,
it should never be stored anywhere if a slot could be removed from
the issue queue in the meantime.

If a uop issues but determines that it cannot actually complete at
that time, it must be \emph{replayed}. The \texttt{\small replay()}
method clears the issued bit for the uop's issue queue slot, returning
it to the dispatched state. The replay mechanism can optionally add
additional dependencies such that the uop is only re-issued after
those dependencies are resolved. This is important for loads and stores,
which may need to add a dependency on a prior store queue entry after
finding a matching address in the load or store queues. In rare cases,
a replay may also be required when a uop is issued but no applicable
functional units are left for it to execute on. The \texttt{\small ReorderBufferEntry::replay()}
method is a wrapper around \texttt{\small IssueQueue::replay()} used
to collect the operands the uop is still waiting for.


\subsection{Implementation}

PTLsim uses a novel method of modeling the issue queue and other associative
structures with small tags. Specifically, the \texttt{\small FullyAssociativeArrayTags8bit}
template class declared in \texttt{\small logic.h} and used to build
the issue queue makes use of the host processor's 128-bit vector (SSE)
instructions to do massively parallel associative matching, masking
and bit scanning on up to 16 tags every clock cycle. This makes it
substantially faster than simulators using the naive approach of scanning
the issue queue entries linearly. Similar classes in \texttt{\small logic.h}
support O(1) associative searches of both 8-bit and 16-bit tags; tags
longer than this are generally more efficient if the generic \texttt{\small FullyAssociativeArrayTags}
using standard integer comparisons is used instead.

As a result of this high performance design, each issue queue is limited
to 64 entries and the tags to be matched must be between 0 and 255
to fit in 8 bits. The \texttt{\small FullyAssociativeArrayTags16bit}
class can be used instead if longer tags are required, at the cost
of reduced simulation performance. To enable this, \texttt{\small BIG\_ROB}
must be defined in \texttt{\small ooohwdef.h}.


\subsection{Other Designs}

It's important to remember that the issue queue design described above
is \emph{one} possible implemention out of the many designs currently
used in industry and research processors. For instance, in lieu of
the collapsing design (used by the Pentium 4 and Power4/5/970), the
AMD K8 uses a sequence number tag of the ROB and comparator logic
to select the earliest ready instruction. Similarly, the Pentium 4
uses a set of bit vectors (a \emph{dependency matrix}) instead of
tag broadcasts to wake up instructions. These other approaches may
be implemented by modifying the \texttt{\small IssueQueue} class as
appropriate.


\section{\label{sec:Issue}Issue}

The \texttt{\small issue()} top-level function issues one or more
instructions in each cluster from each issue queue every cycle. This
function consults the \texttt{\small clusters{[}}\emph{clusterid}\texttt{\small {]}.issue\_width}
field defined in \texttt{\small ooohwdef.h} to determine the maximum
number of uops to issue from each cluster. The \texttt{\small issueq\_operation\_on\_cluster\_with\_result(cluster,
iqslot, issue())} macro (Section \ref{sec:Clustering}) is used to
invoke the \texttt{\small issue()} method of the appropriate cluster
to select the earliest ready issue queue slot, as described in Section
\ref{sec:Scheduling}. 

The \texttt{\small ReorderBufferEntry::issue()} method of the corresponding
ROB entry is then called to actually execute the uop. This method
first makes sure a functional unit is available within the cluster
that's capable of executing the uop; if not, the uop is replayed and
re-issued again on the next cycle. At this point, the uop's three
operands (\texttt{\small ra}, \texttt{\small rb}, \texttt{\small rc})
are read from the physical register file. If any of the operands are
invalid, the entire uop is marked as invalid with an \texttt{\small EXCEPTION\_Propagate}
result and is not further executed. Otherwise, the uop is executed
by calling the synthesized execute function for the uop (see Section
\ref{sec:FetchStage}).

Loads and stores are handled specially by calling the \texttt{\small issueload()}
or \texttt{\small issuestore()} method. Since loads and stores can
encounter an mis-speculation (e.g. when a load is erroneously issued
before an earlier store to the same addresses), the \texttt{\small issueload()}
and \texttt{\small issuestore()} functions can return \texttt{\small ISSUE\_MISSPECULATED}
to force all uops in program order after the mis-speculated uop to
be annulled and sent through the pipeline again. Similarly, if \texttt{\small issueload()}
or \texttt{\small issuestore()} return \texttt{\small ISSUE\_NEEDS\_REPLAY},
issuing from that cluster is aborted since the uop has been replayed
in accordance with Section \ref{sec:Scheduling}. It is important
to note that loads which miss the cache are considered to complete
successfully and do \emph{not} require a replay; their physical register
is simply marked as waiting until the load arrives. In both the mis-speculation
and replay cases, no further uops from the cluster's issue queue are
dispatched until the next cycle.

Branches are handled similar to integer and floating point operations,
except that they may cause a mis-speculation in the event of a branch
misprediction; this is discussed below.

If the uop caused an exception, we force it directly to the commit
stage and not through writeback; this keeps dependencies waiting until
they can be properly annulled by the speculation recovery logic. The
commit stage will detect the exception and take appropriate action.
If the exceptional uop was speculatively executed beyond a branch,
it will never reach commit anyway since the bogus branch would have
to commit before the exception would even become visible.

\textbf{\emph{NOTE:}} In PTLsim, all issued uops put their result
in the uop's assigned physical register at the time of issue, even
though the data technically does not appear there until writeback
(i.e. the physical register enters the \emph{written} state). This
is done to simplify the simulator implementation; it is assumed that
any data {}``read'' from physical registers before writeback is
in fact being read from the bypass network instead.


\chapter{\label{sec:SpeculationAndRecovery}Speculation and Recovery}

PTLsim allows speculative execution of two classes of uops which may
require all uops in program order after and optionally including the
mis-speculated uop to be annulled before they are committed to the
architectural state.


\section{Misspeculation Cases}


\subsection{Branch Mispredictions}

Branch mispredictions form the bulk of all mis-speculated operations.
Whenever the actual RIP returned by a branch uop differs from the
\texttt{\small riptaken} field of the uop, the branch has been mispredicted.
This means all uops after (but \emph{not} including) the branch must
be annulled and removed from all processor structures. The fetch queue
(Section \ref{sec:FetchStage}) is then reset and fetching is redirected
to the correct branch target. However, all uops in program order before
the branch are still correct and may continue executing.

Note that we do \emph{not} just reissue the branch: this would be
pointless, as we already know the correct RIP since the branch uop
itself has already executed once. Instead, we let it writeback and
commit as if it were predicted correctly.


\subsection{Unaligned Loads and Stores and Aliased Stores}

Loads and stores may also require the annulment of all uops following
(and this time including) the load or store in program order. The
conditions under which this process can occur are described in Sections
\ref{sec:IssuingLoads} and \ref{sub:AliasCheck}.


\section{\label{sec:SpeculationRecovery}Recovery}

In PTLsim, the \texttt{\small ReorderBufferEntry::annul()} method
removes any and all ROBs that entered the pipeline after and optionally
including the misspeculated uop (depending on the \texttt{\small keep\_misspec\_uop}
argument). Because this method moves all affected ROBs to the free
state, they are instantly taken out of consideration for future pipeline
stages and will be dropped on the next cycle.

We must be extremely careful to annul all uops in an x86 macro-op;
otherwise half the x86 instruction could be executed twice once refetched.
Therefore, if the first uop to annul is not also the first uop in
the x86 macro-op, we may have to scan backwards in the ROB until we
find the first uop of the macro-op. In this way, we ensure that we
can annul the entire macro-op. All uops comprising the macro-op are
guaranteed to still be in the ROB since none of the uops can commit
until the entire macro-op can commit. Note that this does not apply
if the final uop in the macro-op is a branch and that branch uop itself
is being retained as occurs with mispredicted branches.

The first uop to annul is determined in the \texttt{\small annul()}
method by scanning backwards in time from the excepting uop until
a uop with its SOM (start of macro-op) bit is set, as described in
Section \ref{sec:UopIntro}. This SOM uop represents the boundary
between x86 instructions, and is where we start annulment. The end
of the range of uops to annul is at the tail of the reorder buffer.

We have to reconstruct the speculative RRT as it existed just before
the first uop to be annulled was renamed. This is done by calling
the \texttt{\small pseudocommit()} method of each annulled uop to
implement the {}``fast flush with pseudo-commit'' algorithm as follows.
First, we overwrite the speculative RRT with the committed RRT. We
then \emph{simulate} the commitment of all non-speculative ROBs up
to the first uop to be annulled by updating the speculative RRT as
if it were the commit RRT. This brings the speculative RRT to the
same state as if all in flight nonspeculative operations before the
first uop to be annulled had actually committed. Fetching is then
resumed at the correct RIP, where new uops are renamed using the recovered
speculative RRT.

Other methods of RRT reconstruction (like backwards walk with saved
checkpoint values) are difficult to impossible because of the requirement
that flag rename tables be restored even if some of the required physical
registers with attached flags have since been freed. Technically RRT
checkpointing could be used but due to the load/store replay mechanism
in use, this would require a checkpoint at every load and store as
well as branches. Hence, the forward walk method seems to offer the
best performance in practice and is quite simple. The Pentium 4 is
believed to use a similar method of recovering from some types of
mis-speculations.

After reconstructing the RRT, for each ROB to annul, we broadcast
the ROB index to the appropriate cluster's issue queue, allowing the
issue queue to purge the slot of the ROB being annulled. Finally,
for each annulled uop, we free any resources allocated to it (i.e.,
the ROB itself, the destination physical register, the load/store
queue entry (if any) and so on. Any updates to the branch predictor
and return address stack made during the speculative execution of
branches are also rolled back.

Finally, the fetch unit is restarted at the correct RIP and uops enter
the pipeline and are renamed according to the recovered rename tables
and allocated resource maps.


\chapter{Load Issue}


\section{\label{sec:IssuingLoads}Issuing Loads}

The \texttt{\small ReorderBufferEntry::issueload()} function is responsible
for issuing all load uops. The \texttt{\small issueload()} method
starts by computing the effective physical address of the value to
load. In the released version of PTLsim, physical and virtual addresses
are the same. If the load is one of the special unaligned fixup forms
(\texttt{\small ld.lo}, \texttt{\small ld.hi}) described in Section
\ref{sub:UnalignedLoadsAndStores}, the address is re-aligned according
to the type of instruction. At this point, the \texttt{\small check\_access\_and\_alignment()}
function is called to resolve any immediately obvious exceptions such
as page faults or alignment problems (for normal loads).

If a given load or store accesses an unaligned address but is not
one of the special \texttt{\small ld.lo}/\texttt{\small ld.hi}/\texttt{\small st.lo}/\texttt{\small st.hi}
uops described in Section \ref{sub:UnalignedLoadsAndStores}, the
processor responds by annulling all uops after and including the problem
load; it then refetches instructions starting at the RIP address of
the load itself. When the load instruction is refetched, it is transformed
into a pair of \texttt{\small ld.lo}/\texttt{\small ld.hi} or \texttt{\small st.lo}/\texttt{\small st.hi}
uops in accordance with Section \ref{sub:UnalignedLoadsAndStores}.
This refetch approach is required rather than a simple replay operation
since a replay would require allocating two entries in the issue queue
and potentially two ROBs, which is not possible with the PTLsim design
once uops have been renamed.

Technically, PTLsim splits loads into into low and high fixup uops
by simply retranslating the basic block starting at the problem load's
RIP. In real processors, the frontend pipeline generally has logic
for predicting which loads and stores will be unaligned and will dynamically
split them into aligned parts. Functionally these two approaches are
the same, since in effect PTLsim predicts which loads need to be split
by simply retaining the retranslated basic block with the split loads.

If a load from the effective address would cause a page fault at this
point, the load is aborted and execution returns to the \texttt{\small ReorderBufferEntry::issue()}
method, causing the result to be marked with an exception (\texttt{\small EXCEPTION\_PageFaultOnRead}).

One x86-specific complication arises at this point. If a load (or
store) uop is the high part (\texttt{\small ld.hi} or \texttt{\small st.hi})
of an unaligned load or store pair, but the actual user address did
not overlap any of the high 64 bits accessed by the \texttt{\small ld.hi}
or \texttt{\small st.hi} uop, the load should be completely ignored,
even if the high part overlapped onto an invalid page. This is because
it is perfectly legal to do an unaligned load or store at the very
end of a page such that the next 64 bit chunk is not mapped to a valid
page; the x86 architecture mandates that the load or store execute
correctly as far as the user program is concerned.


\section{Store Queue Check and Store Dependencies}

After doing these exception checks, the load/store queue (LSQ) is
scanned backwards in time from the current load's entry to the LSQ's
head. If a given LSQ entry corresponds to a store, the store's address
has been resolved and the memory range needed by the load overlaps
the memory range touched by the store, the load effectively has a
dependency on the earlier store that must be resolved before the load
can issue. The meaning of {}``overlapping memory range'' is defined
more specifically in Section \ref{sec:StoreMerging}.

In some cases, the addresses of one or more prior stores that a load
may depend on may not have been resolved by the time the load issues.
Some processors will stall the load uop until \emph{all} prior store
addresses are known, but this can decrease performance by incorrectly
preventing independent loads from starting as soon as their address
is available. For this reason, the PTLsim processor model aggressively
issues loads as soon as possible unless the load is predicted to frequently
alias another store currently in the pipeline. This load/store aliasing
prediction technique is described in Section \ref{sub:AliasCheck}.

In either of the cases above, in which an overlapping store is identified
by address but that store's data is not yet available for forwarding
to the load, or where a prior store's address has not been resolved
but is \emph{predicted} to overlap the load, the load effectively
has a data flow dependency on the earlier store. This dependency is
represented by setting the load's fourth \texttt{\small rs} operand
(\texttt{\small operands{[}RS{]}} in the \texttt{\small ReorderBufferEntry})
to the store the load is waiting on. After adding this dependency,
the \texttt{\small replay()} method is used to force the load back
to the dispatched state, where it waits until the prior store is resolved.
After the load is re-issued for a second time, the store queue is
scanned again to make sure no intervening stores arrived in the meantime.
If a different match is found this time, the load is replayed a third
time. In practice, loads are rarely replayed more than once.


\section{Data Extraction}

Once the prior store a load depends on (if any) is ready and all the
exception checks above have passed, it is time to actually obtain
the load's data. This process can be complicated since some bytes
in the region accessed by the load could come from the data cache
while other bytes may be forwarded from a prior store. If one or more
bytes need to be obtained from the data cache, the L1 cache is probed
(via the \texttt{\small probe\_cache\_and\_sfr()} function) to see
if the required line is present. If so, and the combination of the
forwarded store (if any) and the L1 line fills in all bytes required
by the load, the final data can be extracted.

To extract the data, the load unit creates a 64-bit temporary buffer
by overlaying the bytes touched by the prior store (if any) on top
of the bytes obtained from the cache. The correct word is then extracted
and sign extended (if required) from this buffer to form the result
of the load. Unaligned loads (described in Section \ref{sub:UnalignedLoadsAndStores})
are somewhat more complex in that both the low and high 64 bit chunks
from the \texttt{\small ld.lo} and \texttt{\small ld.hi} uops, respectively,
are placed into a 128-bit buffer from which the final result is extracted.

For simulation purposes only, the data to load is immediately accessed
and recorded by \texttt{\small issueload()} regardless of whether
or not there is a cache miss. This makes the loaded data significantly
easier to track. In a real processor, the data extraction process
obviously only happens after the missing line actually arrives, however
our implementation in no way affects performance.


\section{\label{sec:CacheMissHandling}Cache Miss Handling}

If no combination of the prior store's forwarded bytes and data present
in the L1 cache can fulfill a load, this is miss and lower cache levels
must be accessed. This process is described in Sections \ref{sec:InitiatingCacheMiss}
and \ref{sec:FillingCacheMiss}. As far as the core is concerned,
the load is completed at this point even if the data has not yet arrived.
The issue queue entry for the load can be released since the load
is now officially in progress and cannot be replayed. Once the loaded
data has arrived, the cache subsystem calls the \texttt{\small ReorderBuffer::loadwakeup()},
which marks both the physical register and LSQ entry of the load as
ready, and places the load's ROB into the \emph{completed} state.
This allows the processor to wake up dependents of the load on the
next cycle.


\chapter{Stores}


\section{\label{sec:StoreMerging}Store to Store Forwarding and Merging}

In the PTLsim out of order model, a given store may merge its data
with that of a previous store in program order. This ensures that
loads which may need to forward data from a store always reference
exactly one store queue entry, rather than having to merge data from
multiple smaller prior stores to cover the entire byte range being
loaded. In this model, physical memory is divided up into 8 byte (64
bit) chunks. As each store issues, it scans the store queue backwards
in program order to find the most recent prior store to the same 8
byte aligned physical address. If there is a match, the current store
depends on the matching prior store, and cannot complete and forward
its data to other consuming loads and stores until the prior store
in question also completes. This ensures that the current store's
data can be composited on top of the older store's data to form a
single up to date 8-byte chunk. As described in Section \ref{sec:LoadStoreQueueEntry},
each store queue entry contains a byte mask to indicate which of the
8 bytes in each chunk are currently modified by stores in flight versus
those bytes which must come from the data cache.

Technically there are more efficient approaches, such as allowing
stores to issue in any order so long as they do not overlap on the
basis of individual bytes. However, no modern processor allows such
arbitrary forwarding since the circuit complexity involved with scanning
the store queue for partial address matches would be prohibitive and
slow. Instead, most processors only support store to load forwarding
when a single larger prior store covers the entire byte range accessed
by a smaller or same sized load; all other combinations stall the
load until the overlapping prior stores commit to the data cache. 

The store inheritance scheme used by PTLsim (described first) is an
improvement to the more common {}``stall on size mismatch'' scheme
above, but may incur more store dependency replays (since stores now
depend on other stores when they target the same 8-byte chunk) compared
to a stall on size mismatch scheme. As a case study, the Pentium 4
processor (Prescott core) implements a combination of these approaches.


\section{\label{sec:SplitPhaseStores}Split Phase Stores}

The \texttt{\small ReorderBufferEntry::issuestore()} function is responsible
for issuing all store uops. Stores are unusual in that they can issue
even if their \texttt{\small rc} operand (the value to store) is not
ready at the same time as the \texttt{\small ra} and \texttt{\small rb}
operands forming the effective address. This property is useful since
it allows a store to establish an entry in the store queue as soon
as the effective address can be generated, even if the data to store
is not ready. By establishing addresses in the store queue as soon
as possible, we can avoid performance losses associated with the unnecessary
replay of loads that may depend on a store whose address is unavailable
at the time the load issues. In effect, this means that each store
uop may actually issue twice.

In the first phase issue, which occurs as soon as the \texttt{\small ra}
and \texttt{\small rb} operands become ready, the store uop computes
its effective physical address, checks that address for all exceptions
(such as alignment problems and page faults) and writes the address
into the corresponding \texttt{\small LoadStoreQueueEntry} structure
before setting its the \texttt{\small addrvalid} bit as described
in Section \ref{sec:LoadStoreQueueEntry}. If an exception is detected
at this point, the \texttt{\small invalid} bit in the store queue
entry is set and the destination physical register's \texttt{\small FLAG\_inv}
flag is set so any attempt to commit the store will fail.


\subsection{\label{sub:AliasCheck}Load Queue Search (Alias Check)}

The load queue is then searched to find any loads after the current
store in program order which have already issued but have done so
without forwarding data from the current store. These loads erroneously
issued before the current store (now known to overlap the load's address)
was able to forward the correct data to the offending load(s). This
situation is known as \emph{aliasing}, and is effectively a mis-speculation
requiring the annulment of all instructions after and including the
store. The annulment is performed in accordance with Section \ref{sec:MisspeculationAnnulment},
similar to how uops after a mispredicted branch would be annulled.
Unlike branches, technically we do not need to re-fetch instructions
starting at the mis-speculated store, but in the current implementation
of the simulator this is done for simplicity reasons.

Since the annulment process required to correct aliasing violations
is expensive, it is desirable to predict in advance which loads and
stores are likely to alias each other such that loads predicted to
alias are never issued when prior stores in the store queue still
have unknown addresses. This works because in most out of order processors,
statistically speaking, very few loads alias stores compared to normal
loads from the cache. When an aliasing mis-speculation occurs, an
entry is added to a small fully associative structure (typically $\le16$
entries) called the Load Store Alias Predictor (LSAP). This structure
is indexed by a portion of the address of the load instruction that
aliased. This allows the load unit to avoid issuing any load uop that
matches any address in the LSAP if any prior store addresses are still
unresolved; if this is the case, a dependency is created on the first
unresolved store such that the load is replayed (and the load and
store queues are again scanned) once that store resolves. Similar
methods of aliasing prediction are used by the Pentium 4 (Prescott
core only) and Alpha 21264.


\subsection{Store Queue Search (Merge Check)}

At this point the store queue is searched for prior stores to the
same 8-byte block as described above in Section \ref{sec:StoreMerging};
if the store depends on a prior store, the scheduler structures are
updated to add an additional dependency (in \texttt{\small operands{[}RS{]}})
on this prior store before the store is replayed in accordance with
Section \ref{sec:Scheduling} to wait for the prior store to complete.
If no prior store is found, or the prior store is ready, the current
store is marked as a second phase store by setting the \texttt{\small load\_store\_second\_phase}
flag in its ROB entry. Finally, the store is replayed in accordance
with Section \ref{sec:Scheduling}.

In the second phase of store uop scheduling, the store uop is only
re-issued when all four operands (\texttt{\small ra} + \texttt{\small rb}
address, \texttt{\small rc} data and \texttt{\small rs} source store
queue entry) are valid. The second phase repeats the scan of the load
and store queues described above to catch any loads and stores that
may have issued between the first and second phase issues; the store
is replayed a third time if necessary. Otherwise, the \texttt{\small rc}
operand data is merged with the data from the prior store (if any)
store queue entry, and the combined data and bytemask is written into
the current store's store queue entry. Finally, the entry's \texttt{\small dataready}
bit is set to make the entry available for forwarding to other waiting
loads and stores.

The first and second phases may be combined into a single issue without
replay if both the address and data operands of the store are all
ready at the same time and the prior store (if any) the current store
inherits from has already successfully issued.


\chapter{Forwarding, Wakeup and Writeback}


\section{Forwarding and the Clustered Bypass Network}

Immediately after each uop is issued and the \texttt{\small ReorderBufferEntry::issue()}
method actually generates its result, the \texttt{\small cycles\_left}
field of the ROB is set to the expected latency of the uop (e.g. between
1 and 5 cycles). The uop is then moved to the \emph{issued} state
and placed on the \texttt{\small rob\_issued\_list}. Every cycle,
the \texttt{\small complete()} method iterates through each ROB in
issued state and decrements its \texttt{\small cycles\_left} field.
If \texttt{\small cycles\_left} becomes zero, the corresponding uop
has completed execution. The ROB is moved to the \emph{completed}
state (on \texttt{\small rob\_completed\_list}) and its physical register
or store queue entry is moved to the \texttt{\small bypass} state
so newly dispatched uops do not try to wait for it.

The \texttt{\small transfer()} function is also called every cycle.
This function examines the list of ROBs in the \emph{completed} state
and is responsible for broadcasting the completed ROB's tag (ROB index)
to the issue queues. Because of clustering (Section \ref{sec:Clustering}),
some issue queues will receive the broadcast later than others. Specifically,
the ROB's \texttt{\small forward\_cycle} field determines which issue
queues and remote clusters are visible \texttt{\small forward\_cycle}
cycles after the uop completed. The \texttt{\small forward()} method,
called by \texttt{\small transfer()} for each uop in the \emph{completed}
state, indexes into a lookup table \texttt{\small forward\_at\_cycle\_lut{[}}\emph{cluster}\texttt{\small {]}{[}}\emph{forward\_cycle}\texttt{\small {]}}
to get a bitmap of which remote clusters are accessible \texttt{\small forward\_cycle}
cycles after he uop completed, relative to the original cluster.the
uop issued in. The \texttt{\small IssueQueue::broadcast()} method
(Section \ref{sec:Scheduling}) is then called for each applicable
cluster to wake up any operands of uops in that cluster waiting on
the newly completed uop.

The \texttt{\small MAX\_FORWARDING\_LATENCY} constant (in \texttt{\small ooohwdef.h})
specifies the maximum number of cycles between any two clusters. After
the ROB has progressed through \texttt{\small MAX\_FORWARDING\_LATENCY}
cycles in the \emph{completed} state, it is moved to the \texttt{\small ready-to-writeback}
state, effectively meaning the result has arrived at the physical
register file and is eligible for writeback in the next cycle.


\section{Writeback}

Every cycle, the \texttt{\small writeback()} function scans the list
of ROBs in the \emph{ready-to-writeback} state and selects at most
\texttt{\small WRITEBACK\_WIDTH} results to write to the physical
register file. The \texttt{\small forward()} method is first called
one final time to catch the corner case in which a dependent uop was
dispatched while producer uop was waiting in the \emph{ready-to-writeback}
state.

As mentioned in Section \ref{sec:Issue}, for simulation purposes
only, each uop puts its result directly into its assigned physical
register at the time of issue, even though the data technically does
not appear there until writeback. This is done to simplify the simulator
implementation; it is assumed that any data {}``read'' from physical
registers before writeback is in fact being read from the bypass network
instead. Therefore, no actual data movement occurs in the \texttt{\small writeback()}
function; its sole purpose is to place the uop's physical register
into the written state (via the \texttt{\small PhysicalRegister::writeback()}
method) and to move the ROB into its terminal state, \emph{ready-to-commit}.


\chapter{\label{sec:CommitStage}Commitment}


\section{Introduction}

The commit stage examines uops from the head of the ROB, blocks until
all uops comprising a given x86 instruction are ready to commit, commits
the results of those uops to the architectural state and finally frees
the resources associated with each uop.


\section{Atomicity of x86 instructions}

The x86 architecture specifies \emph{atomic execution} for all distinct
x86 instructions. This means that since each x86 instruction may be
comprised of multiple uops; none of these uops may commit until \emph{all}
uops in the instruction are ready to commit. In PTLsim, this is accomplished
by checking if the uop at the head of the ROB (next to commit) has
its SOM (start of macro-op) bit set. If so, the ROB is scanned forwards
from the SOM uop to the next uop in program order with its EOM (end
of macro-op) bit set. If all uops in this range are ready to commit
and exception-free, the SOM uop is allowed to commit, effectively
unlocking the ROB head pointer until the next uop with a SOM bit set
is encountered. However, any exception in any uop comprising the x86
instruction at the head of the ROB causes the pipeline to be flushed
and an exception to be taken. Similarly, external interrupts are only
acknowledged at the boundary between x86 instructions (i.e. after
the EOM uop of each instruction).


\section{Commitment}

As each uop commits, it may update several components of the architectural
state. 

Integer ALU and floating point uops obviously update their destination
architectural register (\emph{rd}). In PTLsim, this is done by simply
updating the committed register rename table (\texttt{\small commitrrt})
rather than actually copying register values. However, the old physical
register mapped to architectural register \emph{rd} will normally
become inaccessible after the Commit RRT mapping for \emph{rd} is
overwritten with the committing uop's physical register index. The
old physical register previously mapped to \emph{rd} can then be freed.
Technically physical registers allocated to intermediate uops (such
as those used to hold temporary values) can be immediately freed without
updating any Commit RRT entries, but for consistency we do not do
this.

In PTLsim, a physical register is freed by moving it to the \texttt{\small PHYSREG\_FREE}
state. Unfortunately for various reasons related to long pipelines
and the renaming of x86 flags, register reclamation is not so simple,
but this will be discussed below in Section \ref{sub:PhysicalRegisterRecyclingComplications}.

Some uops may also commit to a subset of the x86 flags, as specified
in the uop encoding. For these uops, in theory no rename tables need
updating, since the flags can be directly masked into the \texttt{\small REG\_flags}
architectural pseudo-register. Should the pipeline be flushed, the
rename table entries for the ZAPS, CF, OF flag sets will all be reset
to point to the \texttt{\small REG\_flags} pseudo-register anyway.
However, for the speculation recovery scheme described in Section
\ref{sec:SpeculationRecovery}, the \texttt{\small REG\_zf}, \texttt{\small REG\_cf},
and \texttt{\small REG\_of} commit RRT entries are updated as well
to match the updates done to the speculative RRT.

Branches and jumps update the \texttt{\small REG\_rip} pseudo architectural
register, while all other uops simply increment \texttt{\small REG\_rip}
by the number of bytes in the x86 instruction being committed. The
number of bytes (1-15) is stored in a 4-bit field of the first uop
in each x86 instruction (i.e. the uop with its SOM bit set).

Stores commit to the architectural state by writing directly to the
data cache. Remember that a series of stores into a given 64-bit chunk
of memory are merged within the store queue to the store uop's corresponding
STQ entry as the store uop issues, so the commit unit always writes
64 bits to the cache at a time. The byte mask associated with the
STQ entry of the store uop is used to only update the modified bytes
in each chunk of memory in program order.


\section{\label{sub:PhysicalRegisterRecyclingComplications}Physical Register
Recycling Complications}


\subsection{Problem Scenarios}

In some processor designs, it is not always possible to immediately
free the physical register mapped to a given architectural register
when that old architectural register mapping is overwritten during
commit as described above. Out of order x86 processors must maintain
three separate rename table entries for the ZAPS, CF, OF flags in
addition to the register rename table entry, any or all of which may
be updated when uops rename and retire, depending on the uop's flag
renaming semantics (see Section \ref{sub:FlagsManagement}), For this
reason, even though a given physical register value may become inaccessible
and hence dead at commit time, the flags associated with that physical
register are frequently still referenced within the pipeline, so the
physical register itself must remain allocated.

Consider the following specific example, with uops listed in program
order:

\begin{itemize}
\item \texttt{\small sub rax = rax,rbx}\\
Assign RRT{[}\texttt{\small rax}{]} = phys reg r0\\
Assign RRT{[}\texttt{\small flags}{]} = \emph{r0} (since SUB all updates
flags)
\item \texttt{\small mov rax = rcx}\\
Assign RRT{[}\texttt{\small rax}{]} = phys reg r1\\
\emph{No flags renamed:} MOV never updates flags, so RRT{[}\texttt{\small flags}{]}
is still \emph{r0}.
\item \texttt{\small br.e target}\\
Depends on flags attached to \emph{r0}, even though actual architectural
register (\texttt{\small rax}) for \emph{r0} has already been overwritten
in the commit RRT by the MOV's commit. We cannot free \emph{r0} since
the BR uop might not have issued yet.
\end{itemize}
This situation only happens with instruction sets like x86 (and SPARC
or even PowerPC to some extent) which support writing flags (particularly
multiple independent flags) and data in a single instruction.


\subsection{Reference Counting}

For these reasons, we need to prevent U2's register from being freed
if it is still referenced by anything still in the pipeline; the normal
reorder buffer mechanism cannot always handle this situation in a
very long pipeline.

One solution (the one used by PTLsim) is to give each physical register
a reference counter. Physical registers can be referenced from three
structures: as operands to ROBs, from the speculative RRT, and from
the committed RRT. As each uop operand is renamed, the counter for
the corresponding physical register is incremented by calling the
\texttt{\small PhysicalRegister::addref()} method. As each uop commits,
the counter for each of its operands is decremented via the \texttt{\small PhysicalRegister::unref()}
method. Similarly, \texttt{\small unref()} and \texttt{\small addref()}
are used whenever an entry in the speculative RRT or commit RRT is
updated. During mis-speculation recovery (see Section \ref{sec:SpeculationRecovery}),
\texttt{\small unref()} is also used to unlock the operands of uops
slated for annulment. Finally, \texttt{\small unref()} and \texttt{\small addref()}
are used when loads and stores need to add a new dependency on a waiting
store queue entry (see Sections \ref{sec:IssuingLoads} and \ref{sec:SplitPhaseStores}).

As we update the committed RRT during the commit stage, the old register
R mapped to the destination architectural register A of the uop being
committed is examined. The register R is only moved to the \emph{free}
state iff its reference counter is zero. Otherwise, it is moved to
the \emph{pendingfree} state. The hardware examines the counters of
\emph{pendingfree} physical registers every cycle and moves physical
registers to the \emph{free} state only when their counters become
zero and they are in the \emph{pendingfree} state.


\subsection{Hardware Implementation}

The hardware implementation of this scheme is straightforward and
low complexity. The counters can have a very small number of bits
since it is very unlikely a given physical register would be referenced
by all 100+ uops in the ROB; 3 bits should be enough to handle the
typical maximum of < 8 uops sharing a given operand. Counter overflows
can simply stall renaming or flush the pipeline since they are so
rare.

The counter table can be updated in bulk each cycle by adding/subtracting
the appropriate sum or just adding zero if the corresponding register
wasn't used. Since there are several stages between renaming and commit,
the same counter is never both incremented and decremented in the
same cycle, so race conditions are not an issue. 

In real processors, the Pentium 4 uses a scheme similar to this one
but uses bit vectors instead. For smaller physical register files,
this may be a better solution. Each physical register has a bit vector
with one bit per ROB entry. If a given physical register P is still
used by ROB entry E in the pipeline, P's bit vector bit R is set.
Register P cannot be freed until all bits in its vector are zero.


\section{\label{sec:PipelineFlushesAndBarriers}Pipeline Flushes and Barriers}

In some cases, the entire pipeline must be empty after a given uop
commits. For instance, a \emph{barrier} uop (e.g. system call, serializing
uop, or major microcode assist) will stall the frontend when first
renamed, and when committed (at which point it is the only uop in
the pipeline), it will call \texttt{\small flush\_pipeline()} to restart
fetching at the appropriate RIP. Exceptions have a similar effect
when they reach the commit stage. After doing this, the current architectural
registers must be copied into the externally visible \texttt{\small ctx.commitarf{[}{]}}
array, since normally the architectural registers are scattered throughout
the physical register file. Fortunately, the commit stage also updates
\texttt{\small ctx.commitarf{[}{]}} in parallel with the commit RRT,
even though the \texttt{\small commitarf} array is never actually
read by the out of order core.

At this point, the \texttt{\small handle\_barrier()} or \texttt{\small handle\_exception()}
function is called to actually communicate with the world outside
the out of order core. In the case of \texttt{\small handle\_barrier()},
generally this involves executing native code inside PTLsim to execute
a system call on behalf of the simulated thread, or to service a very
complex x86 instruction (e.g. \texttt{\small cpuid}, floating point
save or restore, etc). For \texttt{\small handle\_exception()}, the
simulation is stopped and the user is notified that a genuine user
visible (non-speculative) exception reached the commit stage.

If execution can continue after handling the barrier or exception,
the \texttt{\small external\_to\_core\_state()} function is called
to completely reset the out of order core using the state stored in
\texttt{\small ctx.commitarf{[}{]}}. This involves allocating a fixed
physical register for each of the 64 architectural registers in \texttt{\small ctx.commitarf{[}{]}},
setting the speculative and committed rename tables to their proper
cold start values, and resetting all reference counts on physical
registers as appropriate. If the processor is configured with multiple
physical register files (Section \ref{sec:PhysicalRegisters}), the
initial physical register for each architectural register is allocated
in the first physical register file only (this is configurable by
modifying \texttt{\small external\_to\_core\_state()}). At this point,
the main simulation loop can resume as if the processor had just restarted
from scratch.


\chapter{\label{sec:CacheHierarchy}Cache Hierarchy}

The PTLsim cache hierarchy model is highly flexible and can be used
to model a wide variety of contemporary cache structures. The cache
subsystem (defined in \texttt{\small dcacheint.h} and implemented
by \texttt{\small dcache.cpp}) by default consists of four levels:

\begin{itemize}
\item \textbf{L1 data cache} is directly probed by all loads and stores
\item \textbf{L1 instruction cache} services all instruction fetches
\item \textbf{L2 cache} is shared between data and instructions, with data
paths to both L1 caches
\item \textbf{L3 cache} is also shared and is optionally present
\item \textbf{Main memory} is considered infinite in size but still has
configurable characteristics
\end{itemize}
These cache levels are listed in order from highest level (closer
to the core) to lowest level (far away). The cache hierarchy is assumed
to be \emph{inclusive}, i.e. any data in higher levels is assumed
to always be present in lower levels. Additionally, the cache levels
are generally \emph{write-through}, meaning that every store updates
all cache levels, rather than waiting for a dirty line to be evicted.
PTLsim supports a 48-bit virtual address space and 40-bit physical
addresses in accordance with the x86-64 minimum requirements.


\section{General Configurable Parameters}

All caches support configuration of:

\begin{itemize}
\item Line size in bytes. Any power of two size is acceptable, however the
line size of a lower cache level must be the same or larger than any
line size of a higher level cache. For example, it is illegal to have
128 byte L1 lines with 64 byte L2 lines.
\item Set count may be any power of two number. The total cache size in
bytes is of course (line size) $\times$ (set count)$\times$ (way
count)
\item Way count (associativity) may be any number from 1 (direct mapped)
up to the set count (fully associative). Note that simulation performance
(and clock speed in a real processor) will suffer if the associativity
is too great, particularly for L1 caches.
\item Latency in cycles from a load request to the arrival of the data.
\end{itemize}
In dcacheint.h, the two base classes \texttt{\small CacheLine} and
\texttt{\small CacheLineWithValidMask} are interchangeable, depending
on the model being used. The \texttt{\small CacheLine} class is a
standard cache line with no actual data (since the bytes in each line
are simply held in memory for simulation purposes). 

The \texttt{\small CacheLineWithValidMask} class adds a bitmask specifying
which bytes within the cache line contain valid data and which are
unknown. This is useful for implementing {}``no stall on store''
semantics, in which stores simply allocate a new way in the appropriate
set but only set the valid bits for those bytes actually modified
by the store. The rest of the cache line not touched by the store
can be brought in later without stalling the processor (unless a load
tries to access them); this is PTLsim's default model. Additionally,
this technique may be used to implement sectored cache lines, in which
the line fill bus is smaller than the cache line size. This means
that groups of bytes within the line may be filled over subsequent
cycles rather than all at once.

The \texttt{\small AssociativeArray} template class in \texttt{\small logic.h}
forms the basis of all caches in PTLsim. To construct a cache in which
specific lines can be locked into place, the \texttt{\small LockableAssociativeArray}
template class may be used instead. Finally, the \texttt{\small CommitRollbackCache}
template class is useful for creating versions of PTLsim with cache
level commit/rollback support for out of order commit, fault recovery
and advanced speculation techniques.

The various caches are defined in \texttt{\small dcacheint.h} by specializations
of these template classes. The classes are \texttt{\small L1Cache},
\texttt{\small L1ICache}, \texttt{\small L2Cache} and \texttt{\small L3Cache}.


\section{\label{sec:InitiatingCacheMiss}Initiating a Cache Miss}

As described in Section \ref{sec:IssuingLoads}, in the out of order
core model, the \texttt{\small issueload()} function determines if
some combination of a prior store's forwarded bytes (if any) and data
present in the L1 cache can fulfill a load. If not, this is a miss
and lower cache levels must be accessed. In this case, a \texttt{\small LoadStoreInfo}
structure (defined in \texttt{\small dcache.h}) is prepared with various
metadata about the load, including which ROB entry and physical register
to wake up when the load arrives, its size, alignment, sign extension
properties, prefetch properties and so on. The \texttt{\small issueload\_slowpath()}
function (defined in \texttt{\small dcache.cpp}) is then called with
this information, the physical address to load and any data inherited
from a prior store still in the pipeline. The \texttt{\small issueload\_slowpath()}
function moves the load request out of the core pipeline and into
the cache hierarchy. 

The \emph{Load Fill Request Queue} (LFRQ) is a structure used to hold
information about any outstanding loads that have missed any cache
level. The LFRQ allows a configurable number of loads to be outstanding
at any time and provides a central control point between cache lines
arriving from the L2 cache or lower levels and the movement of the
requested load data into the processor core to dependent instructions.
The \texttt{\small LoadFillReq} structure, prepared by \texttt{\small issueload\_slowpath()},
contains all the data needed to return a filled load to the core:
the physical address of the load, the data and bytemask already known
so far (e.g. forwarded from a prior store) and the \texttt{\small LoadStoreInfo}
metadata described above.

The \emph{Miss Buffer} (MB) tracks all outstanding cache lines, rather
than individual loads. Each MB slot uses a bitmap to track one or
more LFRQ entries that need to be awakened when the missing cache
line arrives. After adding the newly created \texttt{\small LoadFillReq}
entry to the LFRQ, the \texttt{\small MissBuffer::initiate\_miss()}
method uses the missing line's physical address to allocate a new
slot in the miss buffer array (or simply uses an existing slot if
a miss was already in progress on a given line). In any case, the
MB's wakeup bitmap is updated to reflect the new LFRQ entry referring
to that line. Each MB entry contains a \texttt{\small cycles} field,
indicating the number of cycles remaining for that miss buffer before
it can be moved up the cache hierarchy until it reaches the core.
Each entry also contains two bits (\texttt{\small icache} and \texttt{\small dcache})
indicating which L1 caches to which the line should eventually be
delivered; this is required because a single L2 line (and corresponding
miss buffer) may be referenced by both the L1 data and instruction
caches. 

In \texttt{\small initiate\_miss()}, the L2 and L3 caches are probed
to see if they contain the required line. If the L2 has the line,
the miss buffer is placed into the \texttt{\small STATE\_DELIVER\_TO\_L1}
state, indicating that the line is now in progress to the L1 cache.
Similarly, an L2 miss but L3 hit results in the \texttt{\small STATE\_DELIVER\_TO\_L2}
state, and a miss all the way to main memory results in \texttt{\small STATE\_DELIVER\_TO\_L3}.

In the very unlikely event that either the LFRQ slot or miss buffer
are full, an exception is returned to out of order core, which typically
replays the affected load until space in these structures becomes
available. For prefetch requests, only a miss buffer is allocated;
no LFRQ slot is needed.


\section{\label{sec:FillingCacheMiss}Filling a Cache Miss}

The \texttt{\small MissBuffer::clock()} method implements all synchronous
state transitions. For each active miss buffer, the \texttt{\small cycles}
counter is decremented, and if it becomes zero, the MB's current state
is examined. If a given miss buffer was in the \texttt{\small STATE\_DELIVER\_TO\_L3}
state (i.e. in progress from main memory) and the cycle counter just
became zero, a line in the L3 cache is validated with the incoming
data (this may involve evicting another line in the same set to make
room). The MB is then moved to the next state up the cache hierarchy
(i.e. \texttt{\small STATE\_DELIVER\_TO\_L2} in this example) and
its cycles field is updated with the latency of the cache level it
is now leaving (e.g. \texttt{\small L3\_LATENCY} in this example). 

This process continues with successive levels until the MB is in the
\texttt{\small STATE\_DELIVER\_TO\_L1} state and its cycles field
has been decremented to zero. If the MB's \texttt{\small dcache} bit
is set, the L1 corresponding line is validated and the \texttt{\small lfrq.wakeup()}
method is called to invoke a new state machine to wake up any loads
waiting on the recently filled line (as known from the MB's \texttt{\small lfrqmap}
bitmap). If the MB's \texttt{\small icache} bit was set, the line
is validated in the L1 instruction cache, and the \texttt{\small icache\_wakeup\_func()}
callback is used to notify the out of order core's fetch stage that
it may probe the cache for the missing line again. In any case, the
miss buffer is then returned to the unused state.

Each LFRQ slot can be in one of three states: \emph{free}, \emph{waiting}
and \emph{ready}. LFRQ slots remain in the \emph{waiting} state as
long as they are referenced by a miss buffer; once the \texttt{\small lfrq.wakeup()}
method is called, all slots affiliated with that miss buffer are moved
to the \emph{ready} state. The \texttt{\small LoadFillRequestQueue::clock()}
method finds up to \texttt{\small MAX\_WAKEUPS\_PER\_CYCLE} LFRQ slots
in the \emph{ready} state and wakes them up by calling the \texttt{\small load\_filled\_callback()}
callback with the saved \texttt{\small LoadStoreInfo} metadata. The
out of order core handles this callback as described in Section \ref{sec:CacheMissHandling}.

For simulation purposes only, the value to be loaded is immediately
recorded as soon as the load issues, independent of the cache hit
or miss status. In real hardware, the LFRQ entry data would be used
to extract the correct bytes from the newly arrived line and perform
sign extension and alignment. If the original load required bytes
from a mixture of its source store buffer and the data cache, the
SFR data and mask fields in the LFRQ entry would be used to perform
this merging operation. The data would then be written into the physical
register specified by the \texttt{\small LoadStoreInfo} metadata and
that register would be marked as ready before sending a signal to
the issue queues to wake up dependent operations.

In some cases, the out of order core may need to annul speculatively
executed loads. The cache subsystem is notified of this through the
\texttt{\small annul\_lfrq\_slot()} function called by the core. This
function clears the specified LFRQ slot in each miss buffer's lfrqmap
entry (since that slot should no longer be awakened now that it has
been annulled), and frees the LFRQ entry itself.


\section{\label{sec:TranslationLookasideBuffers}Translation Lookaside Buffers}

The cache subsystem includes separate translation lookaside buffers
(TLBs) for data (DTLB) and instructions (ITLB) to map virtual to physical
addresses. Note that virtual addresses are always used within the
simulated virtual address space; hence the TLBs are solely for accurately
measuring performance. To achieve fast simulation, the TLBs are not
actually associatively scanned on each access; instead, the \texttt{\small TranslationLookasideBuffer::check()}
method simply checks one of the simulator's Shadow Page Access Tables
(SPATs) as described in Section \ref{sec:AddressSpaceSimulation}.
For DTLB accesses, the \texttt{\small dtlbmap} SPAT is used, while
ITLB accesses use the \texttt{\small itlbmap} SPAT. If a bit in the
appropriate SPAT is set, that page is considered mapped within the
TLB. When entries are added to or evicted from the TLBs, the SPAT
bit for the old entry's virtual page address must be cleared and the
bit for the new entry's virtual page address must be set; this keeps
the SPATs up to date.

TLB miss penalties can be modeled in various ways. In most x86 processors,
a hardware state machine is used to walk the 3-level or 4-level page
table tree by issuing a chain of loads until the lowest level page
table containing the physical address and attributes is reached. This
can take from \textasciitilde{}10 cycles up to hundreds of cycles
if the page tables themselves are not already in the cache hierarchy.
To model this, the \texttt{\small probe\_cache\_and\_sfr()} function
queries the DTLB for every access, and if a miss is detected, it simulates
an L2 cache miss to add some latency to the load causing the TLB miss.
The TLB is then updated with the \texttt{\small TranslationLookasideBuffer::replace()}
method.

\textbf{\emph{NOTE:}} PTLsim does not fully support self modifying
code. Since no actual data is stored within the data and instruction
caches (only tags are maintained), self modifying code may appear
to work correctly. However, according to the x86 standard, any stores
to any instructions currently in the pipeline must flush the entire
pipeline; PTLsim does not currently do this.


\chapter{Branch Prediction}


\section{Introduction}

PTLsim provides a variety of branch predictors in \texttt{\small branchpred.cpp}.
The branch prediction subsystem is relatively independent of the core
simulator and can be treated as a black box, so long as it implements
the interfaces in \texttt{\small branchpred.h}.

The branch prediction subsystem always contains at least three distinct
predictors for the three main classes of branches:

\begin{itemize}
\item \emph{Conditional Branch Predictor} returns a boolean (taken or not
taken) for each conditional branch (\texttt{\small br.cc} uop)
\item \emph{Branch Target Buffer} (BTB) predicts indirect branch (\texttt{\small jmp}
uop) targets
\item \emph{Return Address Stack} (RAS) predicts return instructions (i.e.
specially marked indirect \texttt{\small jmp} uops) based on prior
calls
\item Unconditional branches (\texttt{\small bru}) are never predicted since
their destination is explicitly encoded.
\end{itemize}
All these predictors are accessed by the core through the \texttt{\small BranchPredictorInterface}
object. Based on the opcode and other uop information, the core determines
the type flags of each branch uop:

\begin{itemize}
\item \texttt{\small BRANCH\_HINT\_UNCOND} for unconditional branches. These
are never predicted since the destination is implied.
\item \texttt{\small BRANCH\_HINT\_COND} for conditional branches.
\item \texttt{\small BRANCH\_HINT\_INDIRECT} for indirect branches, including
returns.
\item \texttt{\small BRANCH\_HINT\_CALL} for calls (both direct and indirect).
This implies that the return address of the call should be a should
be pushed on the RAS.
\item \texttt{\small BRANCH\_HINT\_RET} for returns (indirect branches).
This implies that the return address should be taken from the top
RAS stack entry, not the BTB.
\end{itemize}
Multiple flags may be present for each uop (for instance, \texttt{\small BRANCH\_HINT\_RET}
and \texttt{\small BRANCH\_HINT\_INDIRECT} are both used for the \texttt{\small jmp}
uop terminating an x86 \texttt{\small ret} instruction).

To make a prediction at fetch time, the core calls the \texttt{\small BranchPredictorInterface::predict()}
method, passing it a \texttt{\small PredictorUpdate} structure. This
structure is carried along with each uop until it retires, and contains
all the information needed to eventually update the branch predictor
at the end of the pipeline. The contents will vary depending on the
predictor chosen, but in general this structure contains pointers
into internal predictor counter tables and various flags. The \texttt{\small predict()}
method fills in this structure.

As each uop commits, the \texttt{\small BranchPredictorInterface::update()}
method is passed the uop's saved \texttt{\small PredictorUpdate} structure
and the branch outcome (expected target RIP versus real target RIP)
so the branch predictor can be updated. In PTLsim, predictor updates
only occur at retirement to avoid corruption caused by speculative
instructions.


\section{Conditional Branch Predictor}

The PTLsim conditional branch predictor is the most flexible predictor,
since it can be easily replaced. The default predictor implemented
in \texttt{\small branchpred.cpp} is a selection based predictor.
In essence, two separate predictors are maintained. The \emph{history
predictor} hashes a shift register of previously predicted branches
into a table slot; this slot returns whether or not the branch with
that history is predicted as taken. PTLsim supports various combinations
of the history and branch address to provide \emph{gshare} based semantics.
The \emph{bimodal predictor} is simpler; it uses 2-bit saturating
counters to predict if a given branch is likely to be taken. Finally,
a \emph{selection predictor} specifies which of the two predictors
is more accurate and should be used for future predictions. This style
of predictor, sometimes called a \emph{McFarling predictor}, has been
described extensively in the literature and variations are used by
most modern processors.

Through the \texttt{\small CombinedPredictor} template class, the
user can specify the sizes of all the tables (history, bimodal, selector),
the history depth, the method in which the global history and branch
address are combined and so on. Alternatively, the conditional branch
predictor can be replaced with something entirely different if desired.


\section{Branch Target Buffer}

The Branch Target Buffer (BTB) is essentially a small cache that maps
indirect branch RIP addresses (i.e., \texttt{\small jmp} uops) into
predicted target RIP addresses. It is set associative, with a user
configurable number of sets and ways. In PTLsim, the BTB does not
take into account any indirect branch history information. The BTB
is a nearly universal structure in branch prediction; see the literature
for more information.


\section{Return Address Stack}

The Return Address Stack (RAS) predicts the target address of indirect
jumps marked with the \texttt{\small BRANCH\_HINT\_RET} flag. Whenever
the \texttt{\small BRANCH\_HINT\_RET} flag is passed to the predict()
method, the top RAS stack entry is returned as the predicted target,
overriding anything in the BTB.

Unlike the conditional branch predictor and BTB, the RAS updated speculatively
in the frontend pipeline, before the outcome of calls and returns
are known. This allows better performance when closely spaced calls
and returns must be predicted as they are fetched, before either the
call or corresponding return have actually executed. However, when
called with the \texttt{\small BRANCH\_HINT\_RET} flag, the \texttt{\small predict()}
method only returns the RIP at the top of the RAS, but does not push
or pop the RAS. This must be done after the corresponding \texttt{\small bru}
or \texttt{\small jmp} (for direct and~or indirect calls, respectively)
or \texttt{\small jmp} (for returns) uop is actually allocated in
the ROB. 

This approach is required since the RAS is speculatively updated:
if uops must be annulled (because of branch mispredictions or mis-speculations),
the annulment occurs by walking backwards in the ROB until the excepting
uop is encountered. However, if the RAS were updated during the fetch
stage, some uops may not be in the ROB yet and hence the rollback
logic cannot undo speculative changes made to the RAS by these uops.
This causes the RAS to get out of alignment and performance suffers.

To solve this problem, the RAS is only updated in the allocate stage
immediately after fetch. In the out of order core's \texttt{\small rename()}
function, the \texttt{\small BranchPredictorInterface::updateras()}
method is called to either push or pop an entry from the RAS (calls
push entries, returns pop entries). Unlike the conditional branch
predictor and BTB, this is the only place the RAS is updated, rather
than performing updates at commit time.

If uops must be annulled, the \texttt{\small ReorderBufferEntry::annul()}
method calls the \texttt{\small BranchPredictorInterface::annulras()}
method with the \texttt{\small PredictorUpdate} structure for each
uop it encounters in reverse program order. This method effectively
undoes whatever change was made to the RAS when the \texttt{\small updateras()}
method was called with the same \texttt{\small PredictorUpdate} information
during renaming and allocation. This is possible because \texttt{\small updateras()}
saves checkpoint information (namely, the old RAS top of stack and
the value at that stack slot) before updating the RAS; this allows
the RAS state to be rolled backwards in time as uops are annulled
in reverse program order. At the end of the annulment process when
fetching is restarted at the correct RIP, the RAS state should be
identical to the state that existed before the last uop to be annulled
was originally fetched.


\part{\label{part:Appendices}Appendices}


\chapter{\label{sec:UopReference}PTLsim uop Reference}

The following sections document the semantics and encoding of each
micro-operation (uop) supported by the PTLsim processor core. The
\texttt{\small opinfo{[}{]}} table in \texttt{\small ptlhwdef.cpp}
and constants in \texttt{\small ptlhwdef.h} give actual numerical
values for the opcodes and other fields described below.

\newpage
\texttt{\textbf{\large ~}}\textsf{}\\
\textsf{\Large Merging Rules}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small op}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ (ra} \textsf{\emph{op}} \textsf{rb)}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Merging Rules:}}

\textsf{The x86 compatible ALUs implement operations on 1, 2, 4 or
8 byte quantities. Unless otherwise indicated, all operations take
a 2-bit size shift field (}\texttt{\small sz}\textsf{) used to determine
the effective size in bytes of the operation as follows:}

\begin{itemize}
\item \textsf{\textbf{sz = 0:}} \textsf{Low byte of} \textsf{\emph{rd}}
\textsf{is set to the 8-bit result; high 7 bytes of} \textsf{\emph{rd}}
\textsf{are set to corresponding bytes of} \textsf{\emph{ra}}\textsf{.}
\item \textsf{\textbf{sz = 1:}} \textsf{Low two bytes of} \textsf{\emph{rd}}
\textsf{is set to the 16-bit result; high 6 bytes of} \textsf{\emph{rd}}
\textsf{are set to corresponding bytes of} \textsf{\emph{ra}}\textsf{.}
\item \textsf{\textbf{sz = 2:}} \textsf{Low four bytes of} \textsf{\emph{rd}}
\textsf{is set to the 32-bit result; high 4 bytes of} \textsf{\emph{rd}}
\textsf{are cleared to zero in accordance with x86-64 zero extension
semantics. The} \textsf{\emph{ra}} \textsf{operand is unused and should
be} \texttt{\small REG\_zero}\textsf{.}
\item \textsf{\textbf{sz = 3:}} \textsf{All 8 bytes of} \textsf{\emph{rd}}
\textsf{are set to the 64-bit result.} \textsf{\emph{ra}} \textsf{is
unused and should be} \texttt{\small REG\_zero}\textsf{.}
\end{itemize}
\textsf{Flags are calculated based on the} \textsf{\emph{sz}}\textsf{-byte
value produced by the ALU, not the final 64-bit result in} \textsf{\emph{rd}}\textsf{.}

\bigskip{}
\textsf{\Large Other Pseudo-Operators}{\Large \par}
\lyxline{\Large}

\medskip{}
\textsf{The descriptions in this reference use various pseudo-operators
to describe the semantics of each uop. These operators are described
below.}

\textsf{\textbf{EvalFlags(}}\textsf{\textbf{\emph{ra}}}\textsf{\textbf{)}}

\textsf{The} \textsf{\emph{EvalFlags}} \textsf{pseudo-operator evaluates
the ZAPS, CF, OF flags attached to the source operand} \textsf{\emph{ra}}
\textsf{in accordance with the type of condition code evaluation specified
by the uop. The operator returns 1 if the evaluation is true; otherwise
0 is returned.}

\textsf{\textbf{SignExt(}}\textsf{\textbf{\emph{ra}}}\textsf{\textbf{,
N)}}

\textsf{The} \textsf{\emph{SignExt}} \textsf{operator sign extends
the ra operand by the number of bits specified by N. Specifically,
bit} \textsf{\emph{ra}}\textsf{{[}N{]} is copied to all high order
bits from bit 63 down to bit} \textsf{\emph{N}}\textsf{. If N is not
specified, it is assumed to mean the number of bits in the effective
size of the uop's result (as described under Merging Rules).}

\textsf{\textbf{MergeWithSFR(mem, sfr)}}

\textsf{The} \textsf{\emph{MergeWithSFR}} \textsf{pseudo-operator
is described in the reference page for load uops.}

\textsf{\textbf{MergeAlign(mem, sfr)}}

\textsf{The} \textsf{\emph{MergeAlign}} \textsf{pseudo-operator is
described in the reference page for load uops.}

\newpage
\texttt{\textbf{\large mov and or xor andnot ornot nand nor eqv}}\textsf{}\\
\textsf{\Large Logical Operations}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llc}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small mov}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ rb}\tabularnewline
\texttt{\textbf{\small and}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ ra \& rb}\tabularnewline
\texttt{\textbf{\small or}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ ra | rb}\tabularnewline
\texttt{\textbf{\small xor}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ ra \textasciicircum{} rb}\tabularnewline
\texttt{\textbf{\small andnot}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ (\textasciitilde{}ra) \& rb}\tabularnewline
\texttt{\textbf{\small ornot}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ (\textasciitilde{}ra) | rb}\tabularnewline
\texttt{\textbf{\small nand}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ \textasciitilde{}(ra \& rb)}\tabularnewline
\texttt{\textbf{\small nor}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ \textasciitilde{}(ra | rb)}\tabularnewline
\texttt{\textbf{\small eqv}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ \textasciitilde{}(ra \textasciicircum{}
rb)}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{All operations merge the ALU result with} \textsf{\emph{ra}}
\textsf{and generate flags in accordance with the standard x86 merging
rules described previously.}
\end{itemize}
\newpage
\texttt{\textbf{\large add sub addadd addsub subadd subsub addm subm
addc subc}}\textsf{}\\
\textsf{\Large Add and Subtract}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small add}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ ra + rb}\tabularnewline
\texttt{\textbf{\small sub}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ ra - rb}\tabularnewline
\texttt{\textbf{\small adda}}&
\texttt{\small rd = ra,rb,rc{*}S}&
\textsf{rd = ra $\leftarrow$ ra + rb + (rc <\,{}<
S)}\tabularnewline
\texttt{\textbf{\small adds}}&
\texttt{\small rd = ra,rb,rc{*}S}&
\textsf{rd = ra $\leftarrow$ ra + rb + (rc <\,{}<
S)}\tabularnewline
\texttt{\textbf{\small suba}}&
\texttt{\small rd = ra,rb,rc{*}S}&
\textsf{rd = ra $\leftarrow$ ra + rb + (rc <\,{}<
S)}\tabularnewline
\texttt{\textbf{\small subs}}&
\texttt{\small rd = ra,rb,rc{*}S}&
\textsf{rd = ra $\leftarrow$ ra + rb + (rc <\,{}<
S)}\tabularnewline
\texttt{\textbf{\small addm}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra + rb) \& ((1 <\,{}<
rc) - 1)}\tabularnewline
\texttt{\textbf{\small subm}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra - rb) \& ((1 <\,{}<
rc) - 1)}\tabularnewline
\texttt{\textbf{\small addc}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra + rb) + rc.cf}\tabularnewline
\texttt{\textbf{\small subc}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra - rb) - rc.cf}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{All operations merge the ALU result with} \textsf{\emph{ra}}
\textsf{and generate flags in accordance with the standard x86 merging
rules described previously.}
\item \textsf{The} \texttt{\small adda}\textsf{,} \texttt{\small adds}\textsf{,}
\texttt{\small suba}\textsf{,} \texttt{\small subs} \textsf{uops are
useful for small shifts and x86 three-operand} \texttt{\small LEA}\textsf{-style
address generation.}
\item \textsf{The} \texttt{\small addc} \textsf{and} \texttt{\small subc}
\textsf{uops use only the carry flag field of their rc operand; the
value is unused.}
\end{itemize}
\newpage
\texttt{\textbf{\large sel}}\textsf{}\\
\textsf{\Large Conditional Select}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small sel.}}\texttt{\textbf{\emph{\small cc}}}&
\texttt{\small rd = (ra),rb,rc}&
\textsf{rd = (EvalFlags(ra)) ? rc : rb}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{\textbf{\emph{cc}}} \textsf{is any valid condition code flag
evaluation}
\item \textsf{The} \texttt{\small sel} \textsf{uop merges the selected operand
with} \textsf{\emph{rb}} \textsf{in accordance with the standard x86
merging rules described previously (except that} \texttt{\small sel}
\textsf{uses} \textsf{\emph{rb}} \textsf{as the merge target instead
of} \textsf{\emph{ra}}\textsf{)}
\item \textsf{The 64-bit result and all flags are treated as a single value
for selection purposes, i.e. the flags attached to the selected input
are passed to the output}
\item \textsf{If one of the (rb, rc) operands is not valid (has} \texttt{\small FLAG\_INV}
\textsf{set) but the selected operand is valid, the result is valid.
This is an exception to the invalid bit propagation rule only when
the selected input is valid. If the} \textsf{\emph{ra}} \textsf{operand
is invalid, the result is always invalid.}
\item \textsf{If any of the inputs are waiting (}\texttt{\small FLAG\_WAIT}
\textsf{is set), the uop does not issue, even if the selected input
was ready. This is a pipeline simplification.}
\end{itemize}
\newpage
\texttt{\textbf{\large set}}\textsf{}\\
\textsf{\Large Conditional Set}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small set.}}\texttt{\textbf{\emph{\small cc}}}&
\texttt{\small rd = (ra),rb}&
\textsf{rd = rb $\leftarrow$ EvalFlags(ra) ? 1 : 0}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{\textbf{\emph{cc}}} \textsf{is any valid condition code flag
evaluation}
\item \textsf{The value 0 or 1 is zero extended to the operation size and
merged with} \textsf{\emph{rb}} \textsf{in accordance with the standard
x86 merging rules described previously (except that} \texttt{\small set}
\textsf{uses} \textsf{\emph{rb}} \textsf{as the merge target instead
of} \textsf{\emph{ra}}\textsf{)}
\item \textsf{Flags attached to} \textsf{\emph{ra}} \textsf{(condition code)
are passed through to the output}
\end{itemize}
\newpage
\texttt{\textbf{\large set.sub set.and}}\textsf{}\\
\textsf{\Large Conditional Compare and Set }{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small set.sub.}}\texttt{\textbf{\emph{\small cc}}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = rc $\leftarrow$ EvalFlags(ra - rb) ? 1 : 0}\tabularnewline
\texttt{\textbf{\small set.and.}}\texttt{\textbf{\emph{\small cc}}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = rc $\leftarrow$ EvalFlags(ra \& rb) ? 1 : 0}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small set.sub} \textsf{and} \texttt{\small set.and}
\textsf{uops take the place of a} \texttt{\small sub} \textsf{or}
\texttt{\small and} \textsf{uop immediately consumed by a} \texttt{\small set}
\textsf{uop; this is intended to shorten the critical path if uop
merging is performed by the processor}
\item \textsf{\textbf{\emph{cc}}} \textsf{is any valid condition code flag
evaluation}
\item \textsf{The value 0 or 1 is zero extended to the operation size and
then merged with} \textsf{\emph{rc}} \textsf{in accordance with the
standard x86 merging rules described previously (except that} \texttt{\small set.sub}
\textsf{and} \texttt{\small set.and} \textsf{use} \textsf{\emph{rc}}
\textsf{as the merge target instead of} \textsf{\emph{ra}}\textsf{)}
\item \textsf{Flags generated as the result of the comparison are passed
through with the result}
\end{itemize}
\newpage
\texttt{\textbf{\large br}}\textsf{}\\
\textsf{\Large Conditional Branch}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small br}}\texttt{\textbf{\emph{\small .cc}}}&
\texttt{\small rip = (ra),riptaken,ripseq}&
\textsf{rip = EvalFlags(ra) ? riptaken : ripseq}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{\textbf{\emph{cc}}} \textsf{is any valid condition code flag
evaluation}
\item \textsf{The} \texttt{\small rip} \textsf{(user-visible instruction
pointer register) is reset to one of two immediates. If the flags
evaluation is true, the} \textsf{\emph{riptaken}} \textsf{immediate
is selected; otherwise the} \textsf{\emph{ripseq}} \textsf{immediate
is selected.}
\item \textsf{If the flag evaluation is false (i.e., ripseq is selected),
the} \texttt{\small BranchMispredict} \textsf{internal exception is
raised. The processor should annul all uops after the branch and restart
fetching at the RIP specified by the result (in this case,} \textsf{\emph{ripseq}}\textsf{).}
\item \textsf{Branches are always assumed to be taken. If the branch is
predicted as not taken (i.e. future uops come from the next sequential
RIP after the branch), it is the responsibility of the decoder or
frontend to swap the} \textsf{\emph{riptaken}} \textsf{and} \textsf{\emph{ripseq}}
\textsf{immediates and invert the condition of the branch.}
\item \textsf{The destination register should always be} \texttt{\small REG\_rip}\textsf{;
otherwise this uop is undefined.}
\item \textsf{If the target RIP falls within an unmapped page, not present
page or a marked as no-execute (NX), the} \texttt{\small PageFaultOnExec}
\textsf{exception is taken.}
\item \textsf{No flags are generated by this uop}
\end{itemize}
\newpage
\texttt{\textbf{\large br.sub br.and}}\textsf{}\\
\textsf{\Large Compare and Conditional Branch}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small br}}\texttt{\textbf{\emph{\small .cc}}}&
\texttt{\small rip = ra,rb,riptaken,ripseq}&
\textsf{rip = EvalFlags(ra - rb) ? riptaken : ripseq}\tabularnewline
\texttt{\textbf{\small br}}\texttt{\textbf{\emph{\small .cc}}}&
\texttt{\small rip = ra,rb,riptaken,ripseq}&
\textsf{rip = EvalFlags(ra \& rb) ? riptaken : ripseq}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small br.sub} \textsf{and} \texttt{\small br.and}
\textsf{uops take the place of a} \texttt{\small sub} \textsf{or}
\texttt{\small and} \textsf{uop immediately consumed by a} \texttt{\small br}
\textsf{uop; this is intended to shorten the critical path if uop
merging is performed by the processor}
\item \textsf{\textbf{\emph{cc}}} \textsf{is any valid condition code flag
evaluation}
\item \textsf{The} \texttt{\small rip} \textsf{(user-visible instruction
pointer register) is reset to one of two immediates. If the flags
evaluation is true, the} \textsf{\emph{riptaken}} \textsf{immediate
is selected; otherwise the} \textsf{\emph{ripseq}} \textsf{immediate
is selected}
\item \textsf{If the flag evaluation is false (i.e., ripseq is selected),
the} \texttt{\small BranchMispredict} \textsf{internal exception is
raised. The processor should annul all uops after the branch and restart
fetching at the RIP specified by the result (in this case,} \textsf{\emph{ripseq}}\textsf{)}
\item \textsf{Branches are always assumed to be taken. If the branch is
predicted as not taken (i.e. future uops come from the next sequential
RIP after the branch), it is the responsibility of the decoder or
frontend to swap the} \textsf{\emph{riptaken}} \textsf{and} \textsf{\emph{ripseq}}
\textsf{immediates and invert the condition of the branch}
\item \textsf{The destination register should always be} \texttt{\small REG\_rip}\textsf{;
otherwise this uop is undefined}
\item \textsf{If the target RIP falls within an unmapped page, not present
page or a marked as no-execute (NX), the} \texttt{\small PageFaultOnExec}
\textsf{exception is taken.}
\item \textsf{Flags generated as the result of the comparison are passed
through with the result}
\end{itemize}
\newpage
\texttt{\textbf{\large jmp}}\textsf{}\\
\textsf{\Large Indirect Jump}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small jmp}}&
\texttt{\small rip = ra,riptaken}&
\textsf{rip = ra}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small rip} \textsf{(user-visible instruction
pointer register) is reset to the target address specified by} \textsf{\emph{ra}}
\item \textsf{If the} \textsf{\emph{ra}} \textsf{operand does not match
the} \textsf{\emph{riptaken}} \textsf{immediate, the} \texttt{\small BranchMispredict}
\textsf{internal exception is raised. The processor should annul all
uops after the branch and restart fetching at the RIP specified by
the result (in this case,} \textsf{\emph{ra}}\textsf{)}
\item \textsf{Indirect jumps are always assumed to match the predicted target
in} \textsf{\emph{riptaken}}\textsf{. If some other target is predicted,
it is the responsibility of the decoder or frontend to set the} \textsf{\emph{riptaken}}
\textsf{immediate to that predicted target}
\item \textsf{The destination register should always be} \texttt{\small REG\_rip}\textsf{;
otherwise this uop is undefined}
\item \textsf{If the target RIP falls within an unmapped page, not present
page or a marked as no-execute (NX), the} \texttt{\small PageFaultOnExec}
\textsf{exception is taken.}
\item \textsf{No flags are generated by this uop}
\end{itemize}
\newpage
\texttt{\textbf{\large jmpp}}\textsf{}\\
\textsf{\Large Indirect Jump Within Microcode}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small jmpp}}&
\texttt{\small null = ra,riptaken}&
\textsf{internalrip = ra}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small jmpp} \textsf{uop redirects uop fetching
into microcode not accessible as x86 instructions. The target address
(inside PTLsim, not x86 space) is specified by} \textsf{\emph{ra}}
\item \textsf{If the} \textsf{\emph{ra}} \textsf{operand does not match
the} \textsf{\emph{riptaken}} \textsf{immediate, the} \texttt{\small BranchMispredict}
\textsf{internal exception is raised. The processor should annul all
uops after the branch and restart fetching at the RIP specified by
the result (in this case,} \textsf{\emph{ra}}\textsf{)}
\item \textsf{Indirect jumps are always assumed to match the predicted target
in} \textsf{\emph{riptaken}}\textsf{. If some other target is predicted,
it is the responsibility of the decoder or frontend to set the} \textsf{\emph{riptaken}}
\textsf{immediate to that predicted target}
\item \textsf{The destination register should always be} \texttt{\small REG\_rip}\textsf{;
otherwise this uop is undefined}
\item \textsf{The user visible rip register is not updated after this uop
issues; otherwise it would point into PTLsim space not accessible
to x86 code. Updating is resumed after a normal} \texttt{\small jmp}
\textsf{issues to return to user code. It is the responsibility of
the decoder to move the user address to return to into some temporary
register (traditionally} \texttt{\small REG\_sr2} \textsf{but this
is not required).}
\item \textsf{No flags are generated by this uop}
\end{itemize}
\newpage
\texttt{\textbf{\large bru}}\textsf{}\\
\textsf{\Large Unconditional Branch}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small bru}}&
\texttt{\small rip = riptaken}&
\textsf{rip = riptaken}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small rip} \textsf{(user-visible instruction
pointer register) is reset to the specified immediate. The processor
may redirect fetching from the new RIP}
\item \textsf{No exceptions are possible with unconditional branches}
\item \textsf{If the target RIP falls within an unmapped page, not present
page or a marked as no-execute (NX), the} \texttt{\small PageFaultOnExec}
\textsf{exception is taken.}
\item \textsf{No flags are generated by this uop}
\end{itemize}
\newpage
\texttt{\textbf{\large brp}}\textsf{}\\
\textsf{\Large Unconditional Branch Within Microcode}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small bru}}&
\texttt{\small null = riptaken}&
\textsf{internalrip = riptaken}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small brp} \textsf{uop redirects uop fetching
into microcode not accessible as x86 instructions. The target address
(inside PTLsim, not x86 space) is specified by the} \textsf{\emph{riptaken}}
\textsf{immediate}
\item \textsf{The} \texttt{\small rip} \textsf{(user-visible instruction
pointer register) is reset to the specified} \textsf{\emph{riptaken}}
\textsf{immediate. The processor may redirect fetching from the new
RIP}
\item \textsf{No exceptions are possible with unconditional branches}
\item \textsf{The user visible rip register is not updated after this uop
issues; otherwise it would point into PTLsim space not accessible
to x86 code. Updating is resumed after a normal} \texttt{\small jmp}
\textsf{uop issues to return to user code. It is the responsibility
of the decoder to move the user address to return to into some temporary
register (traditionally} \texttt{\small REG\_sr2} \textsf{but this
is not required).}
\item \textsf{No flags are generated by this uop}
\end{itemize}
\newpage
\texttt{\textbf{\large chk}}\textsf{}\\
\textsf{\Large Check Speculation}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small chk}}\texttt{\textbf{\emph{\small .cc}}}&
\texttt{\small rd = ra,recrip,extype}&
\textsf{rd = EvalCheck(ra) ? 0 : recrip}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small chk} \textsf{uop verifies} \textsf{\emph{certain}}
\textsf{properties about ra. If this verification check passes, no
action is taken. If the check fails,} \texttt{\small chk} \textsf{signals
an exception of the user specified type in the} \textsf{\emph{rc}}
\textsf{immediate. The result of the} \texttt{\small chk} \textsf{uop
in this case is the user specified RIP to recover at after the check
failure is handled in microcode. This recovery RIP is saved in the}
\texttt{\small recoveryrip} \textsf{internal register.}
\item \textsf{This mechanism is intended to allow simple inlined uop sequences
to branch into microcode if certain conditions fail, since normally
inlined uop sequences cannot contain embedded branches. One example
use is in the} \texttt{\small REP} \textsf{series of instructions
to ensure that the count is not zero on entry (a special corner case).}
\item \textsf{Unlike most conditional uops, the} \texttt{\small chk} \textsf{uop
directly checks the numerical value of} \textsf{\emph{ra}} \textsf{against
zero, and ignores any attached flags. Therefore, the} \textsf{\textbf{\emph{cc}}}
\textsf{condition code flag evaluation type is restricted to the subset
(e, ne, be, nbe, l, nl, le, nle).}
\item \textsf{No flags are generated by this uop}
\end{itemize}
\newpage
\texttt{\textbf{\large ld ld.lo ld.hi ldx ldx.lo ldx.hi}}\textsf{}\\
\textsf{\Large Load}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small ld}}&
\texttt{\small rd = {[}ra,rb{]},sfra}&
\textsf{rd = MergeWithSFR(mem{[}ra + rb{]}, sfra)}\tabularnewline
\texttt{\textbf{\small ld.lo}}&
\texttt{\small rd = {[}ra+rb{]},sfra}&
\textsf{rd = MergeWithSFR(mem{[}floor(ra + rb), 8{]}, sfra)}\tabularnewline
\texttt{\textbf{\small ld.hi}}&
\texttt{\small rd = {[}ra+rb{]},rc,sfra}&
\parbox[t]{0.50\columnwidth}{\textsf{rd = MergeAlign(}\\
\textsf{~~MergeWithSFR(mem{[}(floor(ra + rb), 8) + 8{]}, sfra),
rc)}}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{\emph{The PTLsim load unit model is described in substantial
detail in Section \ref{sec:IssuingLoads}; this section only gives
an overview of the load uop semantics.}}
\item \textsf{The} \texttt{\small ld} \textsf{family of uops loads values
from the virtual address specified by the sum} \textsf{\emph{ra}}
\textsf{+} \textsf{\emph{rb}}\textsf{. The} \texttt{\small ld} \textsf{form
zero extends the loaded value, while the} \texttt{\small ldx} \textsf{form
sign extends the loaded value to 64 bits.}
\item \textsf{All values are zero or sign extended to 64 bits; no subword
merging takes place as with ALU uops. The decoder is responsible for
following the load with an explicit} \texttt{\small mov} \textsf{uop
to merge 8-bit and 16-bit loads with their old destination register.}
\item \textsf{The} \textsf{\emph{sfra}} \textsf{operand specifies the store
forwarding register (a.k.a. store buffer) to merge with data from
the cache to form the final result. The inherited SFR may be determined
dynamically by querying a store queue or can be predicted statically.}
\item \textsf{If the load misses the cache, the} \texttt{\small FLAG\_WAIT}
\textsf{flag of the result is set.}
\item \textsf{Load uops do not generate any other condition code flags}
\end{itemize}
\textsf{\textbf{Unaligned Load Support:}}

\begin{itemize}
\item \textsf{The processor supports unaligned loads via a pair of} \texttt{\small ld.lo}
\textsf{and} \texttt{\small ld.hi} \textsf{uops; an overview can be
found in Section \ref{sub:UnalignedLoadsAndStores}. The alignment
type of the load is stored in the uop's cond field (0 =} \texttt{\small ld}\textsf{,
1 =} \texttt{\small ld.lo}\textsf{, 2 =} \texttt{\small ld.hi}\textsf{).}
\item \textsf{The} \texttt{\small ld.lo} \textsf{uop rounds down its effective
address $\left\lfloor ra+rb\right\rfloor $ to the nearest 64-bit
boundary and performs the load. The} \texttt{\small ld.hi} \textsf{uop
rounds $\left\lceil ra+rb+8\right\rceil $ up to the next 64-bit boundary,
performs a load at that address, then takes as its third rc operand
the first (}\texttt{\small ld.lo}\textsf{) load's result. The two
loads are concatenated into a 128-bit word and the final unaligned
data is extracted (and sign extended if the} \texttt{\small ldx} \textsf{form
was used). }
\item \textsf{Special corner case for when the actual user address (}\textsf{\emph{ra}}
\textsf{+} \textsf{\emph{rb}}\textsf{) did not actually require any
bytes in the 8-byte range loaded by the} \texttt{\small ld.hi} \textsf{uop
(i.e. the load was contained entirely within the low 64-bit aligned
chunk). Since it is perfectly legal to do an unaligned load to the
very end of the page such that the next 64 bit chunk is not mapped
to a valid page, the} \texttt{\small ld.hi} \textsf{uop does not actually
access memory; the entire result is extracted from the prior} \texttt{\small ld.lo}
\textsf{result in the} \textsf{\emph{rc}} \textsf{operand.}
\end{itemize}
\textsf{\textbf{Exceptions:}}

\begin{itemize}
\item \texttt{\small UnalignedAccess} \textsf{if the address (}\textsf{\emph{ra}}
\textsf{+} \textsf{\emph{rb}}\textsf{) is not aligned to an integral
multiple of the size in bytes of the load. Unaligned loads (}\texttt{\small ld.lo}
\textsf{and} \texttt{\small ld.hi}\textsf{) do not generate this exception.
Since x86 automatically corrects alignment problems, microcode must
handle this exception as described in Section \ref{sub:UnalignedLoadsAndStores}.}
\item \texttt{\small PageFaultOnRead} \textsf{if the virtual address (}\textsf{\emph{ra}}
\textsf{+} \textsf{\emph{rb}}\textsf{) falls on a page not accessible
to the caller in the current operating mode, or a page marked as not
present.}
\item \textsf{Various other exceptions and replay conditions may exist depending
on the specific processor core model.}
\end{itemize}
\newpage
\texttt{\textbf{\large st}}\textsf{}\\
\textsf{\Large Store}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small st}}&
\texttt{\small sfrd = {[}ra,rb{]},rc,sfra}&
\textsf{sfrd = MergeWithSFR((ra + rb), sfra, rc)}\tabularnewline
\texttt{\textbf{\small st.lo}}&
\texttt{\small sfrd = {[}ra+rb{]},rc,sfra}&
\textsf{sfrd = MergeWithSFR(floor(ra + rb, 8), sfra, rc)}\tabularnewline
\texttt{\textbf{\small st.hi}}&
\texttt{\small sfrd = {[}ra+rb{]},rc,sfra}&
\textsf{sfrd = MergeWithSFR(floor(ra + rb, 8) + 8, sfra, rc)}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{\emph{The PTLsim store unit model is described in substantial
detail in Section \ref{sec:StoreMerging}; this section only gives
an overview of the store uop semantics.}}
\item \textsf{The} \texttt{\small st} \textsf{family of uops prepares values
to be stored to the virtual address specified by the sum} \textsf{\emph{ra}}
\textsf{+} \textsf{\emph{rb}}\textsf{.}
\item \textsf{The} \textsf{\emph{sfra}} \textsf{operand specifies the store
forwarding register (a.k.a. store buffer) to merge the data to be
stored (the} \textsf{\emph{rc}} \textsf{operand) into. The inherited
SFR may be determined dynamically by querying a store queue or can
be predicted statically, as described in} \textsf{\emph{\ref{sec:StoreMerging}.}}
\item \textsf{Store uops only generate the SFR for tracking purposes; the
cache is only written when the SFR is committed.}
\item \textsf{The store uop may issue as soon as the} \textsf{\emph{ra}}
\textsf{and} \textsf{\emph{rb}} \textsf{operands are ready, even if
the} \textsf{\emph{rc}} \textsf{and} \textsf{\emph{sfra}} \textsf{operands
are not known. The store must be replayed once these operands become
known, in accordance with Section \ref{sec:SplitPhaseStores}.}
\item \textsf{Store uops do not generate any other condition code flags}
\end{itemize}
\textsf{\textbf{Unaligned Store Support:}}

\begin{itemize}
\item \textsf{The processor supports unaligned stores via a pair of} \texttt{\small st.lo}
\textsf{and} \texttt{\small st.hi} \textsf{uops; an overview can be
found in Section \ref{sub:UnalignedLoadsAndStores}. The alignment
type of the load is stored in the uop's cond field (0 =} \texttt{\small st}\textsf{,
1 =} \texttt{\small st.lo}\textsf{, 2 =} \texttt{\small st.hi}\textsf{).}
\item Stores are handled in a similar manner, with \texttt{\small st.lo}
and \texttt{\small st.hi} rounding down and up to store parts of the
unaligned value in adjacent 64-bit blocks. 
\item \textsf{The} \texttt{\small st.lo} \textsf{uop rounds down its effective
address $\left\lfloor ra+rb\right\rfloor $ to the nearest 64-bit
boundary and stores the appropriately aligned portion of the} \texttt{\small rc}
\textsf{operand that actually falls within that range of 8 bytes.
The} \texttt{\small ld.hi} \textsf{uop rounds $\left\lceil ra+rb+8\right\rceil $
up to the next 64-bit boundary and similarly stores the appropriately
aligned portion of the} \texttt{\small rc} \textsf{operand that actually
falls within that high range of 8 bytes.}
\item \textsf{Special corner case for when the actual user address (}\textsf{\emph{ra}}
\textsf{+} \textsf{\emph{rb}}\textsf{) did not actually touch any
bytes in the 8-byte range normally written by the} \texttt{\small st.hi}
\textsf{uop (i.e. the store was contained entirely within the low
64-bit aligned chunk). Since it is perfectly legal to do an unaligned
store to the very end of the page such that the next 64 bit chunk
is not mapped to a valid page, the} \texttt{\small st.hi} \textsf{uop
does not actually do anything in this case (the bytemask of the generated
SFR is set to zero and no exceptions are checked).}
\end{itemize}
\textsf{\textbf{Exceptions:}}

\begin{itemize}
\item \texttt{\small UnalignedAccess} \textsf{if the address (}\textsf{\emph{ra}}
\textsf{+} \textsf{\emph{rb}}\textsf{) is not aligned to an integral
multiple of the size in bytes of the store. Unaligned stores (}\texttt{\small st.lo}
\textsf{and} \texttt{\small st.hi}\textsf{) do not generate this exception.
Since x86 automatically corrects alignment problems, microcode must
handle this exception as described in Section \ref{sub:UnalignedLoadsAndStores}.}
\item \texttt{\small PageFaultOnWrite} \textsf{if the virtual address (}\textsf{\emph{ra}}
\textsf{+} \textsf{\emph{rb}}\textsf{) falls on a write protected
page, a page not accessible to the caller in the current operating
mode, or a page marked as not present.}
\item \texttt{\small LoadStoreAliasing} \textsf{if a prior load is found
to alias the store (see Section \ref{sub:AliasCheck}).}
\item \textsf{Various other exceptions and replay conditions may exist depending
on the specific processor core model.}
\end{itemize}
\newpage
\texttt{\textbf{\large ldp ldxp}}\textsf{}\\
\textsf{\Large Load from Internal Space}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small ldp}}&
\texttt{\small rd = {[}ra,rb{]}}&
\textsf{rd = MSR{[}ra+rb{]}}\tabularnewline
\texttt{\textbf{\small ldxp}}&
\texttt{\small rd = {[}ra+rb{]}}&
\textsf{rd = SignExt(MSR{[}ra+rb{]})}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small ldp} \textsf{and} \texttt{\small ldxp}
\textsf{uops load values from the internal PTLsim address space not
accessible to x86 code. Typically this address space is mapped to
internal machine state registers (MSRs) and microcode scratch space.
The internal address to access is specified by the sum} \textsf{\emph{ra}}
\textsf{+} \textsf{\emph{rb}}\textsf{. The} \texttt{\small ldp} \textsf{form
zero extends the loaded value, while the} \texttt{\small ldxp} \textsf{form
sign extends the loaded value to 64 bits.}
\item \textsf{Load uops do not generate any other condition code flags}
\item \textsf{Internal loads may not be unaligned, and never stall or generate
exceptions.}
\end{itemize}
\newpage
\texttt{\textbf{\large stp}}\textsf{}\\
\textsf{\Large Store to Internal Space}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small stp}}&
\texttt{\small null = {[}ra,rb{]},rc}&
\textsf{MSR{[}ra+rb{]} = rc}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small stp} \textsf{uop stores a value to the
internal PTLsim address space not accessible to x86 code. Typically
this address space is mapped to internal machine state registers (MSRs)
and microcode scratch space. The internal address to store is specified
by the sum} \textsf{\emph{ra}} \textsf{+} \textsf{\emph{rb}} \textsf{and
the value to store is specified by} \textsf{\emph{rc}}\textsf{.}
\item \textsf{Store uops do not generate any other condition code flags}
\item \textsf{Internal stores may not be unaligned, and never stall or generate
exceptions.}
\end{itemize}
\newpage
\texttt{\textbf{\large shl shr sar rotl rotr rotcl rotcr}}\textsf{}\\
\textsf{\Large Shifts and Rotates}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small shl}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra <\,{}< rb)}\tabularnewline
\texttt{\textbf{\small shr}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra >\,{}> rb)}\tabularnewline
\texttt{\textbf{\small sar}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ SignExt(ra >\,{}> rb)}\tabularnewline
\texttt{\textbf{\small rotl}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra} \textsf{\emph{rotateleft}} \textsf{rb)}\tabularnewline
\texttt{\textbf{\small rotr}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra} \textsf{\emph{rotateright}} \textsf{rb)}\tabularnewline
\texttt{\textbf{\small rotcl}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (\{rc.cf, ra\}} \textsf{\emph{rotateleft}}
\textsf{rb)}\tabularnewline
\texttt{\textbf{\small rotcr}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (\{rc.cf, ra\}} \textsf{\emph{rotateright}}
\textsf{rb)}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The shift and rotate instructions have some of the most bizarre
semantics in the entire x86 instruction set: they may or may not modify
flags depending on the rotation count operand, which we may not even
know until the instruction issues. This is introduced in Section \ref{sec:ShiftRotateProblems}.}
\item \textsf{The specific rules are as follows:}

\begin{itemize}
\item \textsf{If the count $rb=0$ is zero, no flags are modified}
\item \textsf{If the count $rb=1$, both OF and CF are modified, but ZAPS
is preserved}
\item \textsf{If the count $rb>1$, only the CF is modified. (Technically
the value in OF is undefined, but on K8 and P4, it retains the old
value, so we try to be compatible).}
\item \textsf{Shifts also alter the ZAPS flags while rotates do not.}
\end{itemize}
\item \textsf{For constant counts (immediate} \textsf{\emph{rb}} \textsf{values),
the semantics are easy to determine in advance.}
\item \textsf{For variable counts (}\textsf{\emph{rb}} \textsf{comes from
register), things are more complex. Since the shift needs to determine
its output flags at runtime based on both the shift count and the
input flags (CF, OF, ZAPS), we need to specify the latest versions
in program order of all the existing flags. However, this would require
three operands to the shift uop not even counting the value and count
operands. Therefore, we use a} \texttt{\small collcc} \textsf{(collect
condition code flags, see Section \ref{sub:FlagsManagement}) uop
to get all the most up to date flags into one result, using three
operands for ZAPS, CF, OF. This forms a zero word with all the correct
flags attached, which is then forwarded as the} \textsf{\emph{rc}}
\textsf{operand to the shift. This may add additional scheduling constraints
in the case that one of the operands to the shift itself sets the
flags, but this is fairly rare. Conveniently, this also lets us directly
implement the 65-bit} \texttt{\small rotcl}\textsf{/}\texttt{\small rotcr}
\textsf{uops in hardware with little additional complexity.}
\item \textsf{All operations merge the ALU result with} \textsf{\emph{ra}}
\textsf{and generate flags in accordance with the standard x86 merging
rules described previously.}
\item \textsf{The specific flags attached to the result depend on the input
conditions described above. The user should always assume these uops
always produce the latest version of each of the ZAPS, CF, OF flag
sets.}
\end{itemize}
\newpage
\texttt{\textbf{\large mask}}\textsf{}\\
\textsf{\Large Masking, Insertion and Extraction}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small mask}}\texttt{\textbf{\emph{\small .x|z}}}&
\texttt{\small rd = ra,rb,{[}ms,mc,ds{]}}&
\textsf{See semantics below}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small mask} \textsf{uop and its variants are
used for generalized bit field extraction, insertion, sign and zero
extension using the 18-bit control field in the immediate}
\item \textsf{These uops are used extensively within PTLsim microcode, but
are also useful if the processor supports dynamically merging a chain
of} \texttt{\small shr}\textsf{,} \texttt{\small and}\textsf{,} \texttt{\small or}
\textsf{uops.}
\item \textsf{The condition code flags (ZAPS, CF, OF) are the flags logically
generated by the final AND operation.}
\end{itemize}
\medskip{}
\textsf{\textbf{Control Field Format}}

\textsf{The 18-bit} \textsf{\emph{rc}} \textsf{immediate has the following
three 6-bit fields:}

\noindent \begin{center}\textsf{}\begin{tabular}{|c|c|c|}
\hline 
\textsf{\textbf{DS}}&
\textsf{\textbf{MC}}&
\textsf{\textbf{MS}}\tabularnewline
\hline 
\textsf{12}&
\textsf{6}&
\textsf{0}\tabularnewline
\hline
\end{tabular}\end{center}

\begin{itemize}
\item \textsf{The} \texttt{\small mask} \textsf{uop and its variants are
used for generalized bit field extraction, insertion, sign and zero
extension using the 18-bit control field in the immediate}
\end{itemize}
\medskip{}
\textsf{\textbf{Operation:}}

\begin{lyxcode}
{\small M~=~1'{[}(ms+mc-1):ms{]}}{\small \par}

{\small T~=~(ra~\&~\textasciitilde{}M)~|~((rb~>\,{}>\,{}>~ds)~\&~M)}{\small \par}

{\small if~(Z)~\{}{\small \par}

~{\small ~\#~Zero~extend}{\small \par}

~{\small ~rd~=~ra~$\leftarrow$~(T~\&~1'{[}(ms+mc-1):0{]})}{\small \par}

{\small else~if~(X)~\{}{\small \par}

~{\small ~\#~Sign~extend}{\small \par}

~{\small ~rd~=~ra~$\leftarrow$~(T{[}ms+mc-1{]})~?~(T~|~1'{[}63:(ms+mc){]})~:~(T~\&~1'{[}(ms+mc-1):0{]})}{\small \par}

{\small \}~else~\{}{\small \par}

~{\small ~rd~=~ra~$\leftarrow$~T}{\small \par}

{\small \}}{\small \par}


\end{lyxcode}
\newpage
\texttt{\textbf{\large bswap}}\textsf{}\\
\textsf{\Large Byte Swap}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small bswap}}&
\texttt{\small rd = ra}&
\textsf{rd = ra $\leftarrow$ ByteSwap(ra)}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small bswap} \textsf{uop reverses the endianness
of the} \textsf{\emph{ra}} \textsf{operand. The uop's effective result
size determines the range of bytes which are reversed.}
\item \textsf{This uop's semantics are identical to the x86} \texttt{\small bswap}
\textsf{instruction.}
\item \textsf{This uop does not generate any condition code flags.}
\end{itemize}
\newpage
\texttt{\textbf{\large collcc}}\textsf{}\\
\textsf{\Large Collect Condition Codes}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small collcc}}&
\texttt{\small rd = ra,rb,rc}&
\parbox[t]{0.50\columnwidth}{\textsf{rd.zaps = ra.zaps }\\
\textsf{rd.cf = rb.cf}\\
\textsf{rd.of = rc.of}\\
\textsf{rd = rd.flags}}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small collcc} \textsf{uop collects the condition
code flags from three potentially distinct source operands into a
single output with the combined condition code flags in both its appended
flags and data.}
\item \textsf{This uop is useful for collecting all flags before passing
them as input to another uop which only supports one source of flags
(for instance, the shift and rotate uops).}
\end{itemize}
\newpage
\texttt{\textbf{\large movccr movrcc}}\textsf{}\\
\textsf{\Large Move Condition Code Flags Between Register Value and
Flag Parts}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small movccr}}&
\texttt{\small rd = ra}&
\parbox[t]{0.50\columnwidth}{\textsf{rd = ra.flags}\\
\textsf{rd.flags = 0}}\tabularnewline
\texttt{\textbf{\small movrcc}}&
\texttt{\small rd = ra}&
\parbox[t]{0.50\columnwidth}{\textsf{rd.flags = ra}\\
\textsf{rd = ra}}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small movccr} \textsf{uop takes the condition
code flag bits attached to} \textsf{\emph{ra}} \textsf{and copies
them into the 64-bit register part of the result.}
\item \textsf{The} \texttt{\small movrcc} \textsf{uop takes the low bits
of the} \textsf{\emph{ra}} \textsf{operand and moves those bits into
the condition code flag bits attached to the result.}
\item \textsf{The bits moved consist of the ZF, PF, SF, CF, OF flags}
\item \textsf{The WAIT and INV flags of the result are always cleared since
the uop would not even issue if these were set in} \textsf{\emph{ra}}\textsf{.}
\end{itemize}
\newpage
\texttt{\textbf{\large andcc orcc ornotcc xorcc}}\textsf{}\\
\textsf{\Large Logical Operations on Condition Codes}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small andcc}}&
\texttt{\small rd = ra,rb}&
\textsf{rd.flags = ra.flags \& rb.flags}\tabularnewline
\texttt{\textbf{\small orcc}}&
\texttt{\small rd = ra,rb}&
\textsf{rd.flags = ra.flags | rb.flags}\tabularnewline
\texttt{\textbf{\small ornotcc}}&
\texttt{\small rd = ra,rb}&
\textsf{rd.flags = ra.flags | (\textasciitilde{}rb.flags)}\tabularnewline
\texttt{\textbf{\small xorcc}}&
\texttt{\small rd = ra,rb}&
\textsf{rd.flags = ra.flags \textasciicircum{} rb.flags}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops are used to perform logical operations on the condition
code flags attached to} \textsf{\emph{ra}} \textsf{and} \textsf{\emph{rb}}\textsf{.}
\item \textsf{If the} \textsf{\emph{rb}} \textsf{operand is an immediate,
the immediate data is used instead of the flags normally attached
to a register operand.}
\item \textsf{The 64-bit value of the output is always set to zero.}
\end{itemize}
\newpage
\texttt{\textbf{\large mull mulh}}\textsf{}\\
\textsf{\Large Integer Multiplication}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small mull}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ lowbits(ra $\times$ rb)}\tabularnewline
\texttt{\textbf{\small mulh}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ highbits(ra $\times$ rb)}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops multiply} \textsf{\emph{ra}} \textsf{and} \textsf{\emph{rb}}\textsf{,
then retain only the low} \textsf{\emph{N}} \textsf{bits or high}
\textsf{\emph{N}} \textsf{bits of the result (where N is the uop's
effective result size in bits). This result is then merged into} \textsf{\emph{ra}}\textsf{.}
\item \textsf{The condition code flags generated by these uops correspond
to the normal x86 semantics for integer multiplication (}\texttt{\small imul}\textsf{);
the flags are calculated relative to the effective result size.}
\item \textsf{The} \textsf{\emph{rb}} \textsf{operand may be an immediate}
\end{itemize}
\newpage
\texttt{\textbf{\large bt bts btr btc}}\textsf{}\\
\textsf{\Large Bit Testing and Manipulation}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small bt}}&
\texttt{\small rd = ra,rb}&
\parbox[t]{0.50\columnwidth}{\textsf{rd.cf = ra{[}rb{]} }\\
\textsf{rd = ra $\leftarrow$ (rd.cf) ? -1 : +1}}\tabularnewline
\texttt{\textbf{\small bts}}&
\texttt{\small rd = ra,rb}&
\parbox[t]{0.50\columnwidth}{\textsf{rd.cf = ra{[}rb{]}}\\
\textsf{rd = ra $\leftarrow$ ra | (1 <\,{}< rb)}}\tabularnewline
\texttt{\textbf{\small btr}}&
\texttt{\small rd = ra,rb}&
\parbox[t]{0.50\columnwidth}{\textsf{rd.cf = ra{[}rb{]}}\\
\textsf{rd = ra $\leftarrow$ ra \& (\textasciitilde{}(1 <\,{}<
rb))}}\tabularnewline
\texttt{\textbf{\small btc}}&
\texttt{\small rd = ra,rb}&
\parbox[t]{0.50\columnwidth}{\textsf{rd.cf = ra{[}rb{]}}\\
\textsf{rd = ra $\leftarrow$ ra \textasciicircum{} (1 <\,{}<
rb)}}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops test a given bit in} \textsf{\emph{ra}} \textsf{and
then atomically modify (set, reset or complement) that bit in the
result.}
\item \textsf{The CF flag of the output is set to the original value in
bit position} \textsf{\emph{rb}} \textsf{of} \textsf{\emph{ra}}\textsf{.
Other condition code flag bits in the output are undefined.}
\item \textsf{The} \texttt{\small bt} \textsf{(bit test) uop is special:
it generates a value of -1 or +1 if the tested bit is 1 or 0, respectively.
This is used in microcode for setting up an increment for the} \texttt{\small rep}
\textsf{x86 instructions.}
\end{itemize}
\newpage
\texttt{\textbf{\large ctz clz}}\textsf{}\\
\textsf{\Large Count Trailing or Leading Zeros}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small ctz}}&
\texttt{\small rd = ra}&
\parbox[t]{0.50\columnwidth}{\textsf{rd.zf = (ra == 0)}\\
\textsf{rd = (ra) ? LSBIndex(ra) : 0}}\tabularnewline
\texttt{\textbf{\small clz}}&
\texttt{\small rd = ra}&
\parbox[t]{0.50\columnwidth}{\textsf{rd.zf = (ra == 0)}\\
\textsf{rd = (ra) ? MSBIndex(ra) : 0}}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops find the bit index of the first '1' bit in} \textsf{\emph{ra}}\textsf{,
starting from the lowest bit 0 (for} \texttt{\small ctz}\textsf{)
or the highest bit 63 (for} \texttt{\small clz}\textsf{).}
\item \textsf{The result is zero (technically, undefined) if ra is zero.}
\item \textsf{The ZF flag of the result is 1 if} \textsf{\emph{ra}} \textsf{was
zero, or 0 if} \textsf{\emph{ra}} \textsf{was nonzero. Other condition
code flags are undefined.}
\end{itemize}
\newpage
\texttt{\textbf{\large ctpop}}\textsf{}\\
\textsf{\Large Count Population of '1' Bits}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small ctpop}}&
\texttt{\small rd = ra}&
\parbox[t]{0.50\columnwidth}{\textsf{rd.zf = (ra == 0)}\\
\textsf{rd = PopulationCount(ra)}}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small ctpop} \textsf{uop counts the number of
'1' bits in the} \textsf{\emph{ra}} \textsf{operand.}
\item \textsf{The ZF flag of the result is 1 if} \textsf{\emph{ra}} \textsf{was
zero, or 0 if} \textsf{\emph{ra}} \textsf{was nonzero. Other condition
code flags are undefined.}
\end{itemize}
\newpage
\texttt{\textbf{\large ~}}\textsf{}\\
\textsf{\Large Floating Point Format and Merging}{\Large \par}
\lyxline{\Large}

\medskip{}
\textsf{All floating point uops use the same encoding to specify the
precision and vector format of the operands. The uop's} \textsf{\emph{size}}
\textsf{field is encoded as follows:}

\begin{itemize}
\item \texttt{\textbf{\small 00:}} \textsf{Single precision scalar floating
point (}\texttt{\emph{\small op}}\texttt{\textbf{\small fp}} \textsf{mnemonic).
The operation is only performed on the low 32 bits (in IEEE single
precision format) of the 64-bit inputs; the high 32 bits of the ra
operand are copied to the high 32 bits of the output.}
\item \texttt{\textbf{\small 01:}} \textsf{Single precision vector floating
point (}\texttt{\emph{\small op}}\texttt{\textbf{\small fv}} \textsf{mnemonic).
The operation is performed on both 32 bit halves (in IEEE single precision
format) of the 64-bit inputs in parallel}
\item \texttt{\textbf{\small 1x:}} \textsf{Double precision scalar floating
point (}\texttt{\emph{\small op}}\texttt{\textbf{\small fd}} \textsf{mnemonic).
The operation is performed on the full 64 bit inputs (in IEEE double
precision format)}
\end{itemize}
\textsf{Most floating point operations merge the result with the}
\textsf{\emph{ra}} \textsf{operand to prepare the destination. Since
a full 64-bit result is generated with the vector and double formats,
the} \textsf{\emph{ra}} \textsf{operand is not needed and may be specified
as zero to reduce dependencies.}

\textsf{Exceptions to this encoding are listed where appropriate.}

\textsf{Unless otherwise noted, all operations update the internal
floating point status register (FPSR, equivalent to the MXCSR register
in x86 code) by ORing in any exceptions that occur. If the uop is
encoded to generate an actual exception on excepting conditions, the}
\texttt{\small FLAG\_INV} \textsf{flag is attached to the output to
cause an exception at commit time.}

\textsf{No condition code flags are generated by floating point uops
unless otherwise noted.}

\newpage
\texttt{\textbf{\large addf subf mulf divf minf maxf}}\textsf{}\\
\textsf{\Large Floating Point Arithmetic}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small addf}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ ra + rb}\tabularnewline
\texttt{\textbf{\small subf}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ ra - rb}\tabularnewline
\texttt{\textbf{\small mulf}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ ra $\times$ rb}\tabularnewline
\texttt{\textbf{\small divf}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ ra / rb}\tabularnewline
\texttt{\textbf{\small minf}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ (ra < rb) ? ra : rb}\tabularnewline
\texttt{\textbf{\small maxf}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ (ra >= rb) ? ra : rb}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops do arithmetic on floating point numbers in various
formats as specified in the} \textsf{\emph{Floating Point Format and
Merging}} \textsf{page.}
\end{itemize}
\newpage
\texttt{\textbf{\large maddf msubf}}\textsf{}\\
\textsf{\Large Fused Multiply Add and Subtract}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small maddf}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra $\times$ rb) + rc}\tabularnewline
\texttt{\textbf{\small msubf}}&
\texttt{\small rd = ra,rb,rc}&
\textsf{rd = ra $\leftarrow$ (ra $\times$ rb) - rc}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{The} \texttt{\small maddf} \textsf{and} \texttt{\small msubf}
\textsf{uops perform fused multiply and accumulate operations on three
operands.}
\item \textsf{The full internal precision is preserved between the multiply
and add operations; rounding only occurs at the end.}
\item \textsf{These uops are primarily used by microcode to calculate floating
point division, square root and reciprocal.}
\end{itemize}
\newpage
\texttt{\textbf{\large sqrtf rcpf rsqrtf}}\textsf{}\\
\textsf{\Large Square Root, Reciprocal and Reciprocal Square Root}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small sqrtf}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ sqrt(rb)}\tabularnewline
\texttt{\textbf{\small rcpf}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ 1 / rb}\tabularnewline
\texttt{\textbf{\small rsqrtf}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ 1 / sqrt(rb)}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops perform the specified unary operation on rb and
merge the result into ra (for a single precision scalar mode only)}
\item \textsf{The} \texttt{\small rcpf} \textsf{and} \texttt{\small rsqrtf}
\textsf{uops are approximates - they do not provide the full precision
results. These approximations are in accordance with the standard
x86 SSE/SSE2 semantics.}
\end{itemize}
\newpage
\texttt{\textbf{\large cmpf}}\textsf{}\\
\textsf{\Large Compare Floating Point}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small cmpf}}\texttt{\textbf{\emph{\small .type}}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ CompareFP(ra, rb, type) ? -1 : 0}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{This uop performs the specified comparison of} \textsf{\emph{ra}}
\textsf{and} \textsf{\emph{rb}}\textsf{. If the comparison is true,
the result is set to all '1' bits; otherwise it is zero. The result
is then merged into ra.}
\item \textsf{The} \textsf{\emph{cond}} \textsf{field in the uop encoding
holds the comparison type. The set of compare types matches the x86
SSE/SSE2 CMPxx instructions.}
\end{itemize}
\newpage
\texttt{\textbf{\large cmpccf}}\textsf{}\\
\textsf{\Large Compare Floating Point and Generate Condition Codes}{\Large \par}
\lyxline{\Large}

\begin{tabular}{lll}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}\tabularnewline
\hline
\texttt{\textbf{\small cmpccf}}\texttt{\textbf{\emph{\small .type}}}&
\texttt{\small rd = ra,rb}&
\textsf{rd.flags = CompareFPFlags(ra, rb)}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{This uop performs all comparisons of} \textsf{\emph{ra}} \textsf{and}
\textsf{\emph{rb}} \textsf{and produces x86 condition code flags (ZF,
PF, CF) to represent the result.}
\item \textsf{The semantics of the generated condition code flags exactly
matches the x86 SSE/SSE2 instructions} \texttt{\small COMISS}\textsf{/}\texttt{\small COMISD}\textsf{/}\texttt{\small UCOMISS}\textsf{/}\texttt{\small UCOMISD}\textsf{.}
\item \textsf{Unlike most encodings, the} \textsf{\emph{size}} \textsf{field
holds the comparison type of the two values as follows:}

\begin{itemize}
\item \texttt{\textbf{\small 00:}} \texttt{\small cmpccfp}\textsf{: single
precision ordered compare (same semantics as x86 SSE} \texttt{\small COMISS}\textsf{)}
\item \texttt{\textbf{\small 01:}} \texttt{\small cmpccfp.u}\textsf{: single
precision unordered compare (same semantics as x86 SSE} \texttt{\small UCOMISS}\textsf{)}
\item \texttt{\textbf{\small 10:}} \texttt{\small cmpccfd}\textsf{: double
precision ordered compare (same semantics as x86 SSE2} \texttt{\small COMISD}\textsf{)}
\item \texttt{\textbf{\small 11:}} \texttt{\small cmpccfd.u}\textsf{: double
precision ordered compare (same semantics as x86 SSE2} \texttt{\small UCOMISD}\textsf{)}
\end{itemize}
\end{itemize}
\newpage
\texttt{\textbf{\large cvtf.i2s.ins cvtf.i2s.p cvtf.i2d.lo cvtf.i2d.hi}}\textsf{}\\
\textsf{\Large Convert 32-bit Integer to Floating Point}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}p{0.10\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}&
\textsf{\textbf{Used By}}\tabularnewline
\hline
\texttt{\textbf{\small cvtf.i2s.ins}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ Int32ToFloat(rb)}&
\texttt{\small CVTSI2SS}\tabularnewline
\texttt{\textbf{\small cvtf.i2s.p}}&
\texttt{\small rd = ra}&
\parbox[t]{0.50\columnwidth}{\textsf{rd{[}31:0{]} = Int32ToFloat(ra{[}31:0{]})}\\
\textsf{rd{[}63:32{]} = Int32ToFloat(ra{[}63:32{]})}}&
\texttt{\small CVTPI2PS}\tabularnewline
\texttt{\textbf{\small cvtf.i2d.lo}}&
\texttt{\small rd = ra}&
\textsf{rd = Int32ToDouble(ra{[}31:0{]})}&
\parbox[t]{0.10\columnwidth}{\texttt{\small CVTSI2SD}~\\
\texttt{\small CVTPI2PD}}\tabularnewline
\texttt{\textbf{\small cvtf.i2d.hi}}&
\texttt{\small rd = ra}&
\textsf{rd = Int32ToDouble(ra{[}63:32{]})}&
\texttt{\small CVTPI2PD}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops convert 32-bit integers to single or double precision
floating point}
\item \textsf{The semantics of these instructions are identical to the semantics
of the x86 SSE/SSE2 instructions shown in the table}
\item \textsf{The uop} \textsf{\emph{size}} \textsf{field is not used by
these uops}
\end{itemize}
\newpage
\texttt{\textbf{\large cvtf.q2s.ins cvtf.q2d}}\textsf{}\\
\textsf{\Large Convert 64-bit Integer to Floating Point}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}p{0.10\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}&
\textsf{\textbf{Used By}}\tabularnewline
\hline
\texttt{\textbf{\small cvtf.q2s.ins}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ Int64ToFloat(rb)}&
\parbox[t]{0.10\columnwidth}{\texttt{\small CVTSI2SS}\textsf{}\\
\textsf{(x86-64)}}\tabularnewline
\texttt{\textbf{\small cvtf.q2d}}&
\texttt{\small rd = ra}&
\textsf{rd = Int64ToDouble(ra)}&
\parbox[t]{0.10\columnwidth}{\texttt{\small CVTPI2PS}~\\
\textsf{(x86-64)}}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops convert 64-bit integers to single or double precision
floating point}
\item \textsf{The semantics of these instructions are identical to the semantics
of the x86 SSE/SSE2 instructions shown in the table}
\item \textsf{The uop} \textsf{\emph{size}} \textsf{field is not used by
these uops}
\end{itemize}
\newpage
\texttt{\textbf{\large cvtf.s2i cvt.s2q cvtf.s2i.p}}\textsf{}\\
\textsf{\Large Convert Single Precision Floating Point to Integer}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}p{0.10\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}&
\textsf{\textbf{Used By}}\tabularnewline
\hline
\texttt{\textbf{\small cvtf.s2i}}&
\texttt{\small rd = ra}&
\textsf{rd = FloatToInt32(ra{[}31:0{]})}&
\texttt{\small CVTSS2SI}\tabularnewline
\texttt{\textbf{\small cvtf.s2i.p}}&
\texttt{\small rd = ra}&
\parbox[t]{0.50\columnwidth}{\textsf{rd{[}31:0{]} = FloatToInt32(ra{[}31:0{]})}\\
\textsf{rd{[}63:32{]} = FloatToInt32(ra{[}63:32{]})}}&
\parbox[t]{0.10\columnwidth}{\texttt{\small CVTPS2PI}~\\
\texttt{\small CVTPS2DQ}}\tabularnewline
\texttt{\textbf{\small cvtf.s2q}}&
\texttt{\small rd = ra}&
\textsf{rd = FloatToInt64(ra)}&
\parbox[t]{0.10\columnwidth}{\texttt{\small CVTSS2SI}\\
\textsf{(x86-64)}}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops convert single precision floating point values
to 32-bit or 64-bit integers}
\item \textsf{The semantics of these instructions are identical to the semantics
of the x86 SSE/SSE2 instructions shown in the table}
\item \textsf{Unlike most encodings, the} \textsf{\emph{size}} \textsf{field
holds the rounding type of the result as follows:}

\begin{itemize}
\item \texttt{\textbf{\small x0:}} \textsf{normal IEEE rounding (as determined
by FPSR)}
\item \texttt{\textbf{\small x1:}} \textsf{truncate to zero}
\end{itemize}
\end{itemize}
\newpage
\texttt{\textbf{\large cvtf.d2i cvtf.d2q cvtf.d2i.p}}\textsf{}\\
\textsf{\Large Convert Double Precision Floating Point to Integer}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}p{0.10\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}&
\textsf{\textbf{Used By}}\tabularnewline
\hline
\texttt{\textbf{\small cvtf.d2i}}&
\texttt{\small rd = ra}&
\textsf{rd = DoubleToInt32(ra)}&
\texttt{\small CVTSD2SI}\tabularnewline
\texttt{\textbf{\small cvtf.d2i.p}}&
\texttt{\small rd = ra,rb}&
\parbox[t]{0.50\columnwidth}{\textsf{rd{[}63:32{]} = DoubleToInt32(ra)}\\
\textsf{rd{[}31:0{]} = DoubleToInt32(rb)}}&
\parbox[t]{0.10\columnwidth}{\texttt{\small CVTPD2PI}~\\
\texttt{\small CVTPD2DQ}}\tabularnewline
\texttt{\textbf{\small cvtf.s2q}}&
\texttt{\small rd = ra}&
\textsf{rd = DoubleToInt64(ra)}&
\parbox[t]{0.10\columnwidth}{\texttt{\small CVTSD2SI}\\
\textsf{(x86-64)}}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops convert double precision floating point values
to 32-bit or 64-bit integers}
\item \textsf{The semantics of these instructions are identical to the semantics
of the x86 SSE/SSE2 instructions shown in the table}
\item \textsf{Unlike most encodings, the} \textsf{\emph{size}} \textsf{field
holds the rounding type of the result as follows:}

\begin{itemize}
\item \texttt{\textbf{\small x0:}} \textsf{normal IEEE rounding (as determined
by FPSR)}
\item \texttt{\textbf{\small x1:}} \textsf{truncate to zero}
\end{itemize}
\end{itemize}
\newpage
\texttt{\textbf{\large cvtf.d2s.ins cvtf.d2s.p cvtf.s2d.lo cvtf.s2d.hi}}\textsf{}\\
\textsf{\Large Convert Between Double Precision and Single Precision
Floating Point}{\Large \par}
\lyxline{\Large}

\begin{tabular}{llp{0.50\columnwidth}p{0.10\columnwidth}}
\textsf{\textbf{Mnemonic}}&
\textsf{\textbf{Syntax}}&
\textsf{\textbf{Operation}}&
\textsf{\textbf{Used By}}\tabularnewline
\hline
\texttt{\textbf{\small cvtf.d2s.ins}}&
\texttt{\small rd = ra,rb}&
\textsf{rd = ra $\leftarrow$ DoubleToFloat(rb)}&
\texttt{\small CVTSD2SS}\tabularnewline
\texttt{\textbf{\small cvtf.d2s.p}}&
\texttt{\small rd = ra}&
\parbox[t]{0.50\columnwidth}{\textsf{rd{[}63:32{]} = DoubleToFloat(ra)}\\
\textsf{rd{[}31:0{]} = DoubleToFloat(rb)}}&
\texttt{\small CVTPD2PS}\tabularnewline
\texttt{\textbf{\small cvtf.s2d.lo}}&
\texttt{\small rd = ra}&
\textsf{rd = FloatToDouble(ra{[}31:0{]})}&
\parbox[t]{0.10\columnwidth}{\texttt{\small CVTSS2SD}~\\
\texttt{\small CVTPS2PD}}\tabularnewline
\texttt{\textbf{\small cvtf.s2d.hi}}&
\texttt{\small rd = ra}&
\textsf{rd = FloatToDouble(ra{[}63:32{]})}&
\texttt{\small CVTPS2PD}\tabularnewline
\end{tabular}

\medskip{}
\textsf{\textbf{Notes:}}

\begin{itemize}
\item \textsf{These uops convert single precision floating point values
to double precision floating point values}
\item \textsf{The semantics of these instructions are identical to the semantics
of the x86 SSE/SSE2 instructions shown in the table}
\item \textsf{The uop} \textsf{\emph{size}} \textsf{field is not used by
these uops}
\end{itemize}

\chapter{\label{sec:PerformanceCounters}Performance Counters}

PTLsim maintains hundreds of performance and statistical counters
and data points as it simulates user code. In Section \ref{sec:StatisticsInfrastructure},
the basic mechanisms and data structures through which PTLsim collects
these data were disclosed, and a guide to extending the existing set
of collection points was presented.

This section is a reference listing of all the current performance
counters present in PTLsim by default. The sections below are arranged
in a hierarchical tree format, just as the data are represented in
PTLsim's data store.


\section{General}

As described in Section \ref{sec:StatisticsInfrastructure}, PTLsim
maintains a hierarchical tree of statistical data. At the root of
the tree are a potentially large number of snapshots, numbered starting
at 0. The final snapshot, taken just before simulation completes,
is labeled as {}``final''. Each snapshot branch contains all of
the data structures described in the next few sections. Snapshots
are enabled with the \texttt{\small -snapshot} configuration option
(Section \ref{sec:ConfigurationOptions}); if they are disabled, only
the {}``0'' and {}``final'' snapshots are provided.

In addition to the snapshots, the root of the data tree contains a
{}``ptlsim'' node with the following miscellaneous information:

\textsf{\textbf{ptlsim:}} metadata about the PTLsim build and host
system it is running on

\begin{itemize}
\item \textsf{\textbf{executable:}} the full path of the program being simulated
\item \textsf{\textbf{args:}} arguments to the program being simulated
\item \textsf{\textbf{hw-ver:}} simulated core hardware version
\item \textsf{\textbf{ptl-ver:}} microcode version
\item \textsf{\textbf{build-hostname:}} the machine on which PTLsim was
compiled
\item \textsf{\textbf{build-timestamp:}} the date on which PTLsim was compiled
(this is set whenever \texttt{\small config.o} is compiled)
\item \textsf{\textbf{build-compiler-version:}} gcc version used to build
PTLsim
\item \textsf{\textbf{hostname:}} the host machine PTLsim is running on
\item \textsf{\textbf{native-mhz:}} the clock speed of the host microprocessor
running PTLsim (this is obtained in accordance with Section \ref{sec:Timing})
\end{itemize}

\section{Out of Order Core}

\textsf{\textbf{summary:}} summarizes the performance of user code
running on the simulator

\begin{itemize}
\item \textsf{\textbf{cycles:}} total number of processor cycles simulated
\item \textsf{\textbf{commits:}} total number of committed uops
\item \textsf{\textbf{usercommits:}} total number of committed x86 instructions
\item \textsf{\textbf{issues:}} total number of uops issued. This includes
uops issued more than once by through replay (Section \ref{sec:Scheduling}).
\item \textsf{\textbf{ipc:}} Instructions Per Cycle (IPC) statistics

\begin{itemize}
\item \textsf{\textbf{commit-in-uops:}} average number of uops committed
per cycle
\item \textsf{\textbf{issue-in-uops:}} average number of uops issued per
cycle
\item \textsf{\textbf{commit-in-user-insns:}} average number of x86 instructions
committed per cycle\\
\textbf{\emph{}}\\
\textbf{\emph{NOTE:}} Because one x86 instruction may be broken up
into numerous uops, it is \textbf{\emph{\underbar{never}}} appropriate
to compare IPC figures for committed x86 instructions per clock with
IPC values from a RISC machine. Furthermore, different x86 implementations
use varying numbers of uops per x86 instruction as a matter of encoding,
so even comparing the uop based IPC between x86 implementations or
RISC-like machines is inaccurate. Users are strongly advised to use
relative performance measures instead (e.g. total cycles taken to
complete a given benchmark).
\end{itemize}
\end{itemize}
\textsf{\textbf{simulator:}} describes the performance of PTLsim itself.
Useful for tuning the simulator.

\begin{itemize}
\item \textsf{\textbf{cycles:}} total time in seconds \emph{(not simulated
cycles!)} spent in various parts of the simulator. Please refer to
the source code (in \emph{ooocore.cpp}) for the range of code each
time value corresponds to.
\item \textsf{\textbf{rate:}} PTLsim simulator performance

\begin{itemize}
\item \textsf{\textbf{total-secs:}} total seconds spent in simulation mode
(native mode does not count towards this total)
\item \textsf{\textbf{cycles-per-sec:}} average number of processor cycles
simulated per second
\item \textsf{\textbf{issues-per-sec:}} average number of uops issued in
the simulator per second
\item \textsf{\textbf{commits-per-sec:}} average number of uops committed
in the simulator per second
\item \textsf{\textbf{user-commits-per-sec:}} average number of x86 instructions
committed in the simulator per second
\end{itemize}
\item \textsf{\textbf{bbcache:}} Decoded basic block cache performance

\begin{itemize}
\item \textsf{\textbf{count:}} total basic blocks encountered in the user
code
\item \textsf{\textbf{inserts:}} insertions into the basic block cache
\item \textsf{\textbf{removes:}} removals from the basic block cache (e.g.
when a block must be re-translated after unaligned loads and stores
are found, etc.)
\end{itemize}
\end{itemize}
\textsf{\textbf{fetch:}} fetch stage statistics

\begin{itemize}
\item \textsf{\textbf{width:}} histogram of the fetch width actually used
on each cycle
\item \textsf{\textbf{stop:}} totals up the reasons why fetching finally
stopped in each cycle

\begin{itemize}
\item \textsf{\textbf{icache-miss:}} an instruction cache miss prevented
further fetches
\item \textsf{\textbf{fetchq-full:}} the uop fetch queue is full
\item \textsf{\textbf{bogus-rip:}} speculative execution redirected the
fetch unit to an inaccessible (or non-executable) page. The fetch
unit remains stalled in this state until the mis-speculation is resolved.
\item \textsf{\textbf{branch-taken:}} taken branches to non-sequential addresses
always stop fetching
\item \textsf{\textbf{full-width:}} the maximum fetch width was utilized
without encountering any of the events above
\end{itemize}
\item \textsf{\textbf{blocks:}} blocks of x86 instructions fetched (typically
the processor can read at most e.g. 16 bytes out of a 64 byte instruction
cache line per cycle)
\item \textsf{\textbf{uops:}} total number of uops fetched
\item \textsf{\textbf{user-insns:}} total number of x86 instructions fetched
\item \textsf{\textbf{opclass:}} histogram of how many uops of various operation
classes passed through the fetch unit. The operation classes are defined
in \texttt{\small ptlhwdef.h} and assigned to various opcodes in \texttt{\small ptlhwdef.cpp}.
\end{itemize}
\textsf{\textbf{frontend:}} frontend pipeline (decode, allocate, rename)
statistics

\begin{itemize}
\item \textsf{\textbf{width:}} histogram of the frontend width actually
used on each cycle
\item \textsf{\textbf{status:}} totals up the reasons why frontend processing
finally stopped in each cycle

\begin{itemize}
\item \textsf{\textbf{complete:}} all uops were successfully allocated and
renamed
\item \textsf{\textbf{fetchq-empty:}} no more uops were available for allocation
\item \textsf{\textbf{rob-full:}} reorder buffer (ROB) was full
\item \textsf{\textbf{physregs-full:}} physical register file was full even
though an ROB slot was free
\item \textsf{\textbf{ldq-full:}} load queue was full (too many loads in
the pipeline) even though physical registers were available
\item \textsf{\textbf{stq-full:}} store queue was full (too many stores
in the pipeline)
\end{itemize}
\item \textsf{\textbf{renamed:}} summarizes the type of renaming that occurred
for each uop (of the destination, not the operands)

\begin{itemize}
\item \textsf{\textbf{none:}} uop did not rename its destination (primarily
for stores and branches)
\item \textsf{\textbf{reg:}} uop renamed destination architectural register
\item \textsf{\textbf{flags:}} uop renamed one or more of the ZAPS, CF,
OF flag sets but had no destination architectural register
\item \textsf{\textbf{reg-and-flags:}} uop renamed one or more of the ZAPS,
CF, OF flag sets as well as a destination architectural register
\end{itemize}
\item \textsf{\textbf{alloc:}} summarizes the type of resource allocation
that occurred for each uop (in addition to its ROB slot):

\begin{itemize}
\item \textsf{\textbf{reg:}} uop was allocated a physical register
\item \textsf{\textbf{ldreg:}} uop was a load and was allocated both a physical
register and a load queue entry
\item \textsf{\textbf{sfr:}} uop was a store and was allocated a store forwarding
register (SFR), a.k.a. store queue entry
\item \textsf{\textbf{br:}} uop was a branch and was allocated branch-related
resources (possibly including a destination physical register)
\end{itemize}
\end{itemize}
\textsf{\textbf{dispatch:}} dispatch unit statistics

\begin{itemize}
\item \textsf{\textbf{source:}} totals up where each operand to each uop
currently resided at the time the uop was dispatched

\begin{itemize}
\item \textsf{\textbf{waiting:}} how many operands were waiting (i.e. not
yet ready)
\item \textsf{\textbf{bypass:}} how many operands would come from the bypass
network if the uop were immediately issued
\item \textsf{\textbf{physreg:}} how many operands were already written
back to physical registers
\item \textsf{\textbf{archreg:}} how many operands would be obtained from
architectural registers
\end{itemize}
\item \textsf{\textbf{cluster:}} tracks the number of uops issued to each
cluster (or issue queue) in the processor. This list will vary depending
on the processor configuration. The value \emph{none} means that no
cluster could accept the uop because all issue queues were full.
\end{itemize}
\textsf{\textbf{issue:}} issue statistics

\begin{itemize}
\item \textsf{\textbf{result:}} histogram of the final disposition of issuing
each uop

\begin{itemize}
\item \textsf{\textbf{no-fu:}} no functional unit was available within the
uop's assigned cluster even though it was already issued
\item \textsf{\textbf{replay:}} uop attempted to execute but could not complete,
so it must remain in the issue queue to be replayed. This event generally
occurs when a load or store detects a previously unknown forwarding
dependency on a prior store, when the data to actually store is not
yet available, or when insufficient resources are available to complete
the memory operation. Details are given in Sections \ref{sec:IssuingLoads}
and \ref{sec:SplitPhaseStores}.
\item \textsf{\textbf{misspeculation:}} uop mis-speculated and now all uops
after and including the issued uop must be annulled. This generally
occurs with loads (Section \ref{sec:IssuingLoads}) and stores (Section
\ref{sub:AliasCheck}) when unaligned accesses or load-store aliasing
occurs. This event is handled in accordance with Section \ref{sec:SpeculationRecovery}.
\item \textsf{\textbf{branch-mispredict:}} uop was a branch and mispredicted,
such that all uops after (but not including) the branch uop must be
annulled. See Section \ref{sec:SpeculationAndRecovery} for details.
\item \textsf{\textbf{exception:}} uop caused an exception (though this
may not be a user visible error due to speculative execution)
\item \textsf{\textbf{complete:}} uop completed successfully. Note that
this does \emph{not} mean the result is immediately ready; for loads
it simply means the request was issued to the cache.
\end{itemize}
\item \textsf{\textbf{source:}} totals up where each operand to each uop
was read from as it was issued

\begin{itemize}
\item \textsf{\textbf{bypass:}} how many operands came directly off the
bypass network
\item \textsf{\textbf{physreg:}} how many operands were read from physical
registers
\item \textsf{\textbf{archreg:}} how many operands were read from committed
architectural registers
\end{itemize}
\item \textsf{\textbf{width:}} histogram of the issue width actually used
on each cycle in each cluster. This object is further broken down
by cluster, since various clusters have different issue width and
policies.
\item \textsf{\textbf{opclass:}} histogram of how many uops of various operation
classes were issued. The operation classes are defined in \texttt{\small ptlhwdef.h}
and assigned to various opcodes in \texttt{\small ptlhwdef.cpp}.
\end{itemize}
\textsf{\textbf{writeback:}} writeback stage statistics

\begin{itemize}
\item \textsf{\textbf{total:}} total number of results written back to the
physical register file
\item \textsf{\textbf{transient:}} transient versus persistent values

\begin{itemize}
\item \textsf{\textbf{transient:}} the result technically does not have
to be written back to the physical register file at all, since all
consumers sourced the value off the bypass network and the result
is no longer available since the destination architectural register
pointing to it has since been renamed.
\item \textsf{\textbf{persistent:}} all values which do not meet the conditions
above and hence must still be written back
\end{itemize}
\item \textsf{\textbf{width:}} histogram of the writeback width actually
used on each cycle in each cluster. This object is further broken
down by cluster, since various clusters have different issue width
and policies.
\end{itemize}
\textsf{\textbf{commit:}} commit unit statistics

\begin{itemize}
\item \textsf{\textbf{uops:}} total number of uops committed
\item \textsf{\textbf{userinsns:}} total number of x86 instructions committed
\item \textsf{\textbf{result:}} histogram of the final disposition of attempting
to commit each uop

\begin{itemize}
\item \textsf{\textbf{none:}} one or more uops comprising the x86 instruction
at the head of the ROB were not yet ready to commit, so commitment
is terminated for that cycle
\item \textsf{\textbf{ok:}} result was successfully committed
\item \textsf{\textbf{exception:}} result caused a genuine user visible
exception. Generally this will terminate the simulation.
\item \textsf{\textbf{skipblock:}} This occurs in extremely rare cases when
the processor must skip over the currently executing instruction (such
as in pathological cases of the \texttt{\small rep} x86 instructions).
\item \textsf{\textbf{barrier:}} the processor encountered a barrier instruction,
such as a system call, assist or pipeline flush. The frontend has
already been stopped and fetching has been redirected to the code
to handle the barrier; this condition simply commits the barrier instruction
itself.
\item \textsf{\textbf{stop:}} special case for when the simulation is to
be stopped after committing a certain number of x86 instructions (e.g.
via the \texttt{\small -stopinsns} option in Section \ref{sec:ConfigurationOptions}).
\end{itemize}
\item \textsf{\textbf{setflags:}} how many uops updated the condition code
flags as they committed

\begin{itemize}
\item \textsf{\textbf{yes:}} how many uops updated at least one of the ZAPS,
CF, OF flag sets (the \texttt{\small REG\_flags} internal architectural
register)
\item \textsf{\textbf{no:}} how many uops did not update any flags
\end{itemize}
\item \textsf{\textbf{freereg:}} how many uops were able to free the old
physical register mapped to their architectural destination register
at commit time

\begin{itemize}
\item \textsf{\textbf{pending:}} old physical register was still referenced
within the pipeline or by one or more rename table entries
\item \textsf{\textbf{free:}} old physical register could be immediately
freed
\end{itemize}
\item \textsf{\textbf{physreg-recycled:}} how many physical registers were
recycled (garbage collected) later than normal because of one of the
conditions above
\item \textsf{\textbf{width:}} histogram of the issue width actually used
on each cycle in each cluster. This object is further broken down
by cluster, since various clusters have different issue width and
policies.
\item \textsf{\textbf{opclass:}} histogram of how many uops of various operation
classes were issued. The operation classes are defined in \texttt{\small ptlhwdef.h}
and assigned to various opcodes in \texttt{\small ptlhwdef.cpp}.
\end{itemize}
\textsf{\textbf{branchpred:}} branch predictor statistics

\begin{itemize}
\item \textsf{\textbf{predictions:}} total number of branch predictions
of any type
\item \textsf{\textbf{updates:}} total number of branch predictor updates
of any type
\item \textsf{\textbf{cond:}} conditional branch (\texttt{\small br.cc}
uop) prediction outcomes, broken down into correct predictions and
mispredictions
\item \textsf{\textbf{indir:}} indirect branch (\texttt{\small jmp} uop)
prediction outcomes, broken down into correct predictions and mispredictions
\item \textsf{\textbf{return:}} return (\texttt{\small jmp} uop with \texttt{\small BRANCH\_HINT\_RET}
flag) prediction outcomes, broken down into correct predictions and
mispredictions
\item \textsf{\textbf{summary:}} summary of all prediction outcomes of the
three types above, broken down into correct predictions and mispredictions
\item \textsf{\textbf{ras:}} return address stack (RAS) operations

\begin{itemize}
\item \textsf{\textbf{push:}} RAS pushes on calls
\item \textsf{\textbf{push-overflows:}} RAS pushes on calls in which the
RAS overflowed
\item \textsf{\textbf{pop:}} RAS pops on returns
\item \textsf{\textbf{pop-underflows:}} RAS pops on returns in which the
RAS was empty
\item \textsf{\textbf{annuls:}} annulment operations in which speculative
updates to the RAS were rolled back
\end{itemize}
\end{itemize}

\section{Cache Subsystem}

\textsf{\textbf{load:}} load unit statistics

\begin{itemize}
\item \textsf{\textbf{result:}} histogram of the final disposition of issuing
each load uop

\begin{itemize}
\item \textsf{\textbf{complete:}} cache hit
\item \textsf{\textbf{miss:}} L1 cache miss, and possibly lower levels as
well (Sections \ref{sec:CacheMissHandling} and \ref{sec:InitiatingCacheMiss})
\item \textsf{\textbf{exception:}} load generated an exception (typically
a page fault), although the exception may still be speculative (Section
\ref{sec:IssuingLoads})
\item \textsf{\textbf{ordering:}} load was misordered with respect to stores
(Section \ref{sub:AliasCheck})
\item \textsf{\textbf{unaligned:}} load was unaligned and will need to be
re-executed as a pair of low and high loads (Sections \ref{sub:UnalignedLoadsAndStores}
and \ref{sec:IssuingLoads})
\item \textsf{\textbf{replay:}} histogram of events in which a load needed
to be replayed (Section \ref{sec:IssuingLoads})

\begin{itemize}
\item \textsf{\textbf{sfr-addr-and-data-not-ready:}} load was predicted
to forward data from a prior store (Section \ref{sub:AliasCheck}),
but neither the address nor the data of that store has resolved yet
\item \textsf{\textbf{sfr-addr-not-ready:}} load was predicted to forward
data from a prior store, but the address of that store has not resolved
yet
\item \textsf{\textbf{sfr-data-not-ready:}} load address matched a prior
store in the store queue, but the data that store should write has
not resolved yet
\item \textsf{\textbf{missbuf-full:}} load missed the cache but the miss
buffer and/or LFRQ (Section \ref{sec:InitiatingCacheMiss}) was full
at the time
\end{itemize}
\end{itemize}
\item \textsf{\textbf{hit:}} histogram of the cache hierarchy level each
load finally hit

\begin{itemize}
\item \textsf{\textbf{L1:}} L1 cache hit
\item \textsf{\textbf{L2:}} L1 cache miss, L2 cache hit
\item \textsf{\textbf{L3:}} L! and L2 cache miss, L3 cache hit
\item \textsf{\textbf{mem:}} all caches missed; value read from main memory
\end{itemize}
\item \textsf{\textbf{forward:}} histogram of which sources were used to
fill each load

\begin{itemize}
\item \textsf{\textbf{cache:}} how many loads obtained all their data from
the cache
\item \textsf{\textbf{sfr:}} how many loads obtained all their data from
a prior store in the pipeline (i.e. load completely overlapped that
store)
\item \textsf{\textbf{sfr-and-cache:}} how many loads obtained their data
from a combination of the cache and a prior store
\end{itemize}
\item \textsf{\textbf{dependency:}} histogram of how loads related to previous
stores

\begin{itemize}
\item \textsf{\textbf{independent:}} load was independent of any store currently
in the pipeline
\item \textsf{\textbf{predicted-alias-unresolved:}} load was stalled because
the load store alias predictor (LSAP) predicted that an earlier store
would overlap the load's address address even though that earlier
store's address was unresolved (Section \ref{sub:AliasCheck})
\item \textsf{\textbf{stq-address-match:}} load depended on an earlier store
still found in the store queue
\end{itemize}
\item \textsf{\textbf{type:}} histogram of the type of each load uop

\begin{itemize}
\item \textsf{\textbf{aligned:}} normal aligned loads
\item \textsf{\textbf{unaligned:}} special unaligned load uops \texttt{\small ld.lo}
or \texttt{\small ld.hi} (Section \ref{sub:UnalignedLoadsAndStores})
\item \textsf{\textbf{internal:}} loads from PTLsim space by microcode
\end{itemize}
\item \textsf{\textbf{size:}} histogram of the size in bytes of each load
uop
\item \textsf{\textbf{transfer-L2-to-L1:}} histogram of the types of L2
to L1 line transfers that occurred (Section \ref{sec:CacheHierarchy})

\begin{itemize}
\item \textsf{\textbf{full-L2-to-L1:}} all bytes in cache line were transferred
from L2 to L1 cache
\item \textsf{\textbf{partial-L2-to-L1:}} some bytes in the L1 line were
already valid (because of stores to those bytes), but the remaining
bytes still need to be fetched
\item \textsf{\textbf{L2-to-L1I:}} all bytes in the L2 line were transferred
into the L1 instruction cache
\end{itemize}
\item \textsf{\textbf{dtlb:}} data cache translation lookaside buffer hit
versus miss rate (Section \ref{sec:TranslationLookasideBuffers})
\end{itemize}
\textsf{\textbf{fetch:}} instruction fetch unit statistics (Section
\ref{sec:FetchStage})

\begin{itemize}
\item \textsf{\textbf{hit:}} histogram of the cache hierarchy level each
fetch finally hit

\begin{itemize}
\item \textsf{\textbf{L1:}} L1 cache hit
\item \textsf{\textbf{L2:}} L1 cache miss, L2 cache hit
\item \textsf{\textbf{L3:}} L! and L2 cache miss, L3 cache hit
\item \textsf{\textbf{mem:}} all caches missed; value read from main memory
\end{itemize}
\item \textsf{\textbf{itlb:}} instruction cache translation lookaside buffer
hit versus miss rate (Section \ref{sec:TranslationLookasideBuffers})
\end{itemize}
\textsf{\textbf{prefetches:}} prefetch engine statistics

\begin{itemize}
\item \textsf{\textbf{in-L1:}} requested data already in L1 cache
\item \textsf{\textbf{in-L2:}} requested data already in L2 cache (and possibly
also in L1 cache)
\item \textsf{\textbf{required:}} prefetch was actually required (data was
not cached or was in L3 or lower levels)
\end{itemize}
\textsf{\textbf{missbuf:}} miss buffer performance (Sections \ref{sec:InitiatingCacheMiss}
and \ref{sec:FillingCacheMiss})

\begin{itemize}
\item \textsf{\textbf{inserts:}} total number of lines inserted into the
miss buffer
\item \textsf{\textbf{delivers:}} total number of lines delivered to various
cache hierarchy levels from the miss buffer

\begin{itemize}
\item \textsf{\textbf{mem-to-L3:}} deliver line from main memory to the
L3 cache
\item \textsf{\textbf{L3-to-L2:}} deliver line to the L3 cache to the L2
cache
\item \textsf{\textbf{L2-to-L1D:}} deliver line from the L2 cache to the
L1 data cache
\item \textsf{\textbf{L2-to-L1I:}} deliver line from the L2 cache to the
L1 instruction cache
\end{itemize}
\end{itemize}
\textsf{\textbf{lfrq:}} load fill request queue (LFRQ) performance
(Sections \ref{sec:InitiatingCacheMiss} and \ref{sec:FillingCacheMiss})

\begin{itemize}
\item \textsf{\textbf{inserts:}} total number of loads inserted into the
LFRQ
\item \textsf{\textbf{wakeups:}} total number of loads awakened from the
LFRQ
\item \textsf{\textbf{annuls:}} total number of loads annulled in the LFRQ
(after they were annulled in the processor core)
\item \textsf{\textbf{resets:}} total number of LFRQ resets (all entries
cleared)
\item \textsf{\textbf{total-latency:}} total latency in cycles of all loads
passing through the LFRQ
\item \textsf{\textbf{average-miss-latency:}} average load latency, weighted
by cache level hit and latency to that level
\item \textsf{\textbf{width:}} histogram of how many loads were awakened
per cycle by the LFRQ
\end{itemize}
\textsf{\textbf{store:}} store unit statistics

\begin{itemize}
\item \textsf{\textbf{issue:}} histogram of the final disposition of issuing
each store uop

\begin{itemize}
\item \textsf{\textbf{complete:}} store completed without problems
\item \textsf{\textbf{exception:}} store generated an exception (typically
a page fault), although the exception may still be speculative (Section
\ref{sec:StoreMerging})
\item \textsf{\textbf{ordering:}} store detected that a later load in program
order aliased the store but was issued earlier than the store (Section
\ref{sub:AliasCheck})
\item \textsf{\textbf{unaligned:}} store was unaligned and will need to
be re-executed as a pair of low and high stores (Sections \ref{sub:UnalignedLoadsAndStores})
\item \textsf{\textbf{replay:}} histogram of events in which a store needed
to be replayed (Sections \ref{sec:SplitPhaseStores} and \ref{sec:StoreMerging})

\begin{itemize}
\item \textsf{\textbf{wait-sfraddr-sfrdata:}} neither the address nor the
data of a prior store this store inherits some of its data from was
ready
\item \textsf{\textbf{wait-sfraddr:}} the data of a prior store was ready
but its address was still unavailable
\item \textsf{\textbf{wait-sfrdata:}} the address of a prior store was ready
but its data was still unavailable
\item \textsf{\textbf{wait-storedata-sfraddr-sfrdata:}} the actual data
value to store was not ready (Section \ref{sec:SplitPhaseStores}),
in addition to having neither the data nor the address of a prior
store (Section \ref{sec:StoreMerging})
\item \textsf{\textbf{wait-storedata-sfraddr:}} the actual data value to
store was not ready (Section \ref{sec:SplitPhaseStores}), in addition
to not having the address of the prior store (Section \ref{sec:StoreMerging})
\item \textsf{\textbf{wait-storedata-sfrdata:}} the actual data value to
store was not ready (Section \ref{sec:SplitPhaseStores}), in addition
to not having the data from the prior store (Section \ref{sec:StoreMerging})
\end{itemize}
\end{itemize}
\item \textsf{\textbf{forward:}} histogram of which sources were used to
construct the merged store buffer:

\begin{itemize}
\item \textsf{\textbf{zero:}} no prior store overlapping the current store
was found in the pipeline
\item \textsf{\textbf{sfr:}} data from a prior store in the pipeline was
merged with the value to be stored to form the final store buffer
\end{itemize}
\item \textsf{\textbf{type:}} histogram of the type of each store uop

\begin{itemize}
\item \textsf{\textbf{aligned:}} normal aligned store
\item \textsf{\textbf{unaligned:}} special unaligned store uops \texttt{\small st.lo}
or \texttt{\small st.hi} (Section \ref{sub:UnalignedLoadsAndStores})
\item \textsf{\textbf{internal:}} stores to PTLsim space by microcode
\end{itemize}
\item \textsf{\textbf{size:}} histogram of the size in bytes of each store
uop
\item \textsf{\textbf{commit:}} histogram of how stores are committed

\begin{itemize}
\item \textsf{\textbf{direct:}} store committed directly to the data cache
in the commit stage (Section \ref{sec:CommitStage})
\end{itemize}
\end{itemize}
\newpage
\noindent \begin{center}\textbf{\emph{The End}}\end{center}
\end{document}
