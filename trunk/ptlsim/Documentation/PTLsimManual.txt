 

PTLsim User's Guide and Reference

The Anatomy of an x86-64 Out of Order Microprocessor

Matt T. Yourst
<yourst@yourst.com>

Revision 20051001

\copyright  1999-2005 Matt T. Yourst <yourst@yourst.com>.

The PTLsim software and manual are free software;
they are licensed under the GNU General Public License 
version 2.

Table of Contents

Introducing PTLsim
    Introducing PTLsim
    History
    Documentation Roadmap
Getting Started with PTLsim
    Building PTLsim
    Running PTLsim
    Configuration Options
PTLsim Internals
    Overview
    Common Libraries and Logic Design APIs
        General Purpose Macros
        Super Standard Template Library (SuperSTL)
        Logic Standard Template Library (LogicSTL)
        Miscellaneous Code
    Low Level Startup and Injection
    Simulator Startup
    Address Space Simulation
    Debugging Hints
    Timing Issues
Statistics Collection and Control
    Using PTLstats to Analyze Statistics
    Statistics support in code
    PTLsim Calls From User Code
    Performance and Statistical Counters
x86 Instructions and Micro-Ops (uops)
    Micro-Ops (uops) and TransOps
    Descriptions of uops
    Simple Fast Path Instructions
    x86-64
    Operation Sizes
    Flags Management and Register Renaming
    Unaligned Loads and Stores
    Repeated String Operations
    Problem Instructions
    SSE Support
    x87 Floating Point
    Assists
Introduction
Fetch Stage
    Instruction Fetching and the Basic Block Cache
    Fetch Queue
Frontend and Key Structures
    Resource Allocation
    Reorder Buffer Entries
        ROB States
    Physical Registers
        Physical Registers
        Physical Register File
        Physical Register States
    Load Store Queue Entries
        Register Renaming
        External State
    Frontend Stages
Scheduling, Dispatch and Issue
    Clustering and Issue Queue Configuration
    Cluster Selection
    Issue Queue Structure and Operation
        Implementation
        Other Designs
    Issue
Speculation and Recovery
    Misspeculation Cases
        Branch Mispredictions
        Unaligned Loads and Stores and Aliased Stores
    Recovery
Load Issue
    Issuing Loads
    Store Queue Check and Store Dependencies
    Data Extraction
    Cache Miss Handling
Stores
    Store to Store Forwarding and Merging
    Split Phase Stores
        Load Queue Search (Alias Check)
        Store Queue Search (Merge Check)
Forwarding, Wakeup and Writeback
    Forwarding and the Clustered Bypass Network
    Writeback
Commitment
    Introduction
    Atomicity of x86 instructions
    Commitment
    Physical Register Recycling Complications
        Problem Scenarios
        Reference Counting
        Hardware Implementation
Cache Hierarchy
    General Configurable Parameters
    Initiating a Cache Miss
    Filling a Cache Miss
    Translation Lookaside Buffers
Branch Prediction
    Introduction
    Conditional Branch Predictor
    Branch Target Buffer
    Return Address Stack
PTLsim uop Reference
Performance Counters
    General
    Out of Order Core
    Cache Subsystem



<part:Introduction>PTLsim User's Guide

Introducing PTLsim

 Introducing PTLsim

PTLsim is a state of the art cycle accurate 
microprocessor simulator and virtual machine for the 
x86 and x86-64 instruction sets. This release of PTLsim 
models a modern speculative out of order x86-64 
compatible processor core, cache hierarchy and 
supporting hardware.

PTLsim is very different from most cycle accurate 
simulators used in research applications. It runs 
directly on the same platform it is simulating (an 
x86-64 machine running Linux) and is able to switch in 
and out of full out of order simulation mode and native 
x86-64 mode at any time completely transparent to the 
running user code. This lets users quickly profile a 
small section of the user code without the overhead of 
emulating the uninteresting parts. It is also capable 
of switching between 32-bit and 64-bit threads on the fly.

Compared to competing simulators, PTLsim provides 
extremely high performance even when running in full 
cycle accurate out of order simulation mode. Through 
extensive tuning, cache profiling and the use of x86 
specific accelerated vector operations and 
instructions, PTLsim significantly cuts simulation time 
compared to traditional research simulators. Even with 
its optimized core, PTLsim still allows a significant 
amount of flexibility for easy experimentation through 
the use of optimized C++ template classes and libraries 
suited to synchronous logic design. We have 
successfully run a wide array of programs under PTLsim, 
from typical benchmarks to graphical applications and 
network servers.

 History

PTLsim was designed and developed by Matt T. Yourst 
<yourst@yourst.com> with its beginnings dating back to 
2001. The main PTLsim code base, including the out of 
order processor model, has been in active development 
since 2003 and has been extensively used by our 
processor design research group at the State University 
of New York at Binghamton. The out of order simulator 
was based on an earlier model, also developed by Matt 
Yourst, designed around the Alpha instruction set.

PTLsim is not related to other widely used simulators. 
It is our hope that PTLsim will help microprocessor 
researchers move to a contemporary and widely used 
instruction set (x86 and x86-64) with readily available 
hardware implementations. This will provide a new 
option for researchers stuck with simulation tools 
supporting only the Alpha or MIPS based instruction 
sets, both of which have since been discontinued on 
real commercially available hardware (making 
co-simulation impossible) with an uncertain future in 
up to date compiler toolchains.

The PTLsim software and this manual are free software, 
licensed under the GNU General Public License version 2.

 Documentation Roadmap

This manual has been divided into several parts:

 Part [part:Introduction] introduces PTLsim and describes its structure 
  and operation

 Part [part:x86andUops] describes the PTLsim internal micro-operation 
  instruction set and its relation to x86 and x86-64

 Part [part:OutOfOrderModel] details the design and implementation of the 
  PTLsim out of order core model

 Part [part:Appendices] is a reference manual for the PTLsim internal 
  uop instruction set, the performance monitoring 
  events the simulator supports and a variety of other 
  technical information.

Getting Started with PTLsim

 Building PTLsim

PTLsim is written in C++ with extensive use of x86 and 
x86-64 inline assembly code for performance and 
virtualization purposes. In its present release, it is 
designed for use on an x86-64 host system running Linux 2.6.

Notes:

 PTLsim is currently intended for x86-64 machines only. 
  Do not attempt to build it on a normal 32-bit x86 
  machine - it will not work. However, we will be 
  modifying PTLsim in the near future to run on regular 
  32-bit x86 systems (albeit with lower performance and 
  the lack of x86-64 support).

 PTLsim is very sensitive to the Linux kernel version 
  it is running on. We have tested this version of 
  PTLsim on 2.6.12 and 2.6.13, but it may break on 
  earlier versions due to changes in certain 
  undocumented system calls and structures we use for 
  virtualization purposes. Section [sec:PTLsimInternals] gives more 
  information on this. 

 gcc 3.4.x should be used to compile the code, although 
  it should also work with gcc 3.3 or 4.0 (we have not 
  tested this though)

 We have not tried PTLsim on an Intel based x86-64 
  (a.k.a. EM64T) capable processor, but in theory it 
  should work correctly. You may need to adjust the 
  Makefile options to specify this. Let us know if you 
  get this working!

To build PTLsim, unpack the sources (or obtain them via 
CVS checkout) and just run make (on a multiprocessor 
machine, you may wish to use make -j2). By default, the 
Makefile specifies maximum optimization for an AMD 
x86-64 machine, so compilation may be slow. For 
debugging purposes only, the Makefile line specifying "
-O0 -g3" (no optimization, all debugging info) may be 
used, but this will make PTLsim very slow.

 <sec:RunningPTLsim>Running PTLsim

PTLsim invocation is very simple: after compiling the 
simulator and making sure the ptlsim executable is in 
your path, simply run:

ptlsim  full-path-to-executable arguments...

PTLsim reads configuration options for running various 
user programs by looking for a configuration file named 

/home/username/.ptlsim/path/to/program/executablename.conf. 
To set options for each program, you'll need to create 
a directory of the form /home/username/.ptlsim and make 
sub-directories under it corresponding to the full path 
to the program. For example, to configure /bin/ls 
you'll need to run "mkdir /home/username/.ptlsim/bin" 
and then edit "/home/username/.ptlsim/bin/ls.conf" with 
the appropriate options. For example, try putting the 
following in ls.conf as described:

-logfile ls.ptlsim -loglevel 9 -stats ls.stats 
-stopinsns 10000

Then run:

ptlsim /bin/ls -la

PTLsim should display its system information banner, 
then the output of simulating the directory listing. 
With the options above, PTLsim will simulate /bin/ls 
starting at the first x86 instruction in the dynamic 
linker's entry point, run until 10000 x86 instructions 
have been committed, and will then switch back to 
native mode (i.e. the user code will run directly on 
the real processor) until the program exits. During 
this time, it will compile an extensive log of the 
state of every micro-operation executed by the 
processor and will save it to "ls.ptlsim" in the current 
directory. It will also create "ls.stats", a binary file 
containing snapshots of PTLsim's internal performance 
counters. The ptlstats program can be used to print and 
analyze these statistics by running "ptlstats -dumpraw ls.stats".

 <sec:ConfigurationOptions>Configuration Options

PTLsim supports.a variety of options in the 
configuration file of each program; you can run 
"ptlsim" without arguments to get a full list of these 
options. The options described below are used to 
control which code PTLsim executes and how it profiles 
it. The default value for each option is shown in 
[brackets] at the end of the description.

Logging:

-quiet Do not print PTLsim system information banner [disabled]

-logfile file Log filename file (use /dev/fd/1 for 
stdout, /dev/fd/2 for stderr) [(none)]

-loglevel N Log level N. Higher levels provide a 
breakdown of every operation in every stage of the 
pipeline; "0" disables all logging except startup 
messages and system call tracing [0]

-startlog N Start logging after cycle N [infinity]

Statistics Collection:

-stats file Statistics filename file [(none)]

-snapshot N Take statistical snapshot and reset counters 
every N cycles [infinite]

Simulation Start Point:

-startrip addr Start at RIP address startrip [(none)]

-startrepeat N Start after passing start RIP at least N 
times [(none)]

-excludeld Exclude dynamic linker execution (start at 
main() instead) [disabled]

-trigger Trigger mode: wait for user process to use 
special function ptlcall_switch_to_sim() before 
entering simulation mode [disabled]. This is described 
in Section [sec:TriggerMode].

Simulation Stop Point:

-stop N Stop after N instructions [infinity]

-stoprip addr Stop before basic block RIP addr is 
translated for the first time [0]

-bbinsns N In final basic block, only translate N user 
instructions [infinity]

-stopinsns N Stop after committing N user instructions [infinity]

-flushevery N Flush pipeline after every N user 
instructions commit [infinity]

Sequential and Native Control:

-profonly Profile user code in native mode only using 
CPU performance counters; don't simulate anything [disabled]

-exitend Kill the thread after full simulation 
completes rather than going native [disabled]

Debugging:

-dumpcode file Save page of user code at final RIP to 
file [(none)]

-perfect-cache Perfect cache hit rate [disabled]

-ooo Use out of order core [always enabled]

NOTE: If you use the "-logfile" option, do not run it 
forever by accident since the log files will go on for 
gigabytes. Use -startlog and -stop to limit the logged 
range. 

NOTE: To actually modify the microarchitecture of the 
simulated processor, you will need to edit the source 
code and recompile.

<sec:PTLsimInternals>PTLsim Internals

 Overview

The following is an overview of the source files for PTLsim:

 ooocore.cpp is the out of order simulator itself. The 
  microarchitectural model implemented by this 
  simulator is the subject of Part [part:OutOfOrderModel].

 ptlhwdef.cpp and ptlhwdef.h define the basic uop 
  encodings, flags and registers. The tables of uops 
  might be interesting to see how a modern x86 
  processor is designed at the microcode level. The 
  basic format is discussed in Section [sec:UopIntro]; all uops are 
  documented in Section [sec:UopReference].

 ooohwdef.h defines the parameters of the out of order 
  processor model not intrinsic to the PTLsim uop 
  instruction set itself.

 translate-x86.cpp is where the x86 and x86-64 to uop 
  translation takes place. It is complicated and 
  generally you shouldn't need to deal with this code 
  to use or modify the simulator.

 dcache.cpp and dcache.h and dcacheint.h contain the 
  data cache model. At present the full L1/L2/L3/mem 
  hierarchy is modeled. Note that the instruction cache 
  is missing at this point, but will be added back in 
  very soon. For SPEC this does not matter very much. 
  The cache hierarchy is very flexible configuration 
  wise; it is described further in Section [sec:CacheHierarchy].

 kernel.cpp and kernel.h is where all the virtual 
  machine "black magic" takes place to let PTLsim 
  transparently switch between simulation and native 
  mode and 32-bit/64-bit mode. In general you should 
  not need to touch this since it is very Linux kernel 
  specific (this version works with 2.6.12 - 2.6.13) 
  and works at a level below the standard C/C++ libraries.

 branchpred.cpp and branchpred.h is the branch 
  predictor. Currently this is set up as a hybrid 
  bimodal and history based predictor with various 
  customizable parameters.

 logic.h is a library of C++ templates for implementing 
  synchronous logic structures like associative arrays, 
  queues, register files, etc. It has some very clever 
  features like FullyAssociativeArray8bit, which uses 
  x86 SSE vector instructions to associatively match 
  and process ~16 byte-sized tags every cycle. These 
  classes are fully parameterized and useful for all 
  kinds of simulations.

 globals.h, superstl.h and superstl.cpp implement 
  various standard library functions and classes as an 
  alternative to C++ STL. These libraries also contain 
  a number of features very useful for bit manipulation.

 uopinterface.cpp contains code for mapping uop opcodes 
  and characteristics to their implementation in uopimpl.S.

 uopimpl.S contains x86-64 assembly language 
  implementations of all uops and their variations. 
  PTLsim implements most ALU and floating point uops in 
  assembler so as to leverage the exact semantics and 
  flags generated by real x86 instructions, since most 
  PTLsim uops are so similar to the equivalent x86 instructions.

 ptlsim.cpp and ptlsim.h are responsible for 
  initializing PTLsim and starting the appropriate 
  simulation core code.

 config.cpp and config.h manage the PTLsim 
  configuration options for each user program.

 datastore.cpp and datastore.h manage the PTLsim 
  statistics data store file structure.

 lowlevel-64bit.S contains 64-bit startup and context 
  switching code. PTLsim execution starts here.

 lowlevel-32bit.S contains 32-bit startup and context 
  switching code.

 injectcode.cpp is compiled into the 32-bit and 64-bit 
  code injected into the target process to map the 
  ptlsim binary and pass control to it.

 loader.h is used to pass information to the injected 
  boot code.

 ptlstats.cpp is a utility for printing and analyzing 
  the statistics data store files in various human 
  readable ways.

 cpuid.cpp is a utility program to show various data 
  returned by the x86 cpuid instruction. Run it under 
  PTLsim for a surprise.

 genoffsets.cpp is a utility program used during the 
  build process to give the assembly language code the 
  offsets of various C++ structure fields.

 ptlcalls.c and ptlcalls.h are optionally compiled into 
  user programs to let them switch into and out of 
  simulation mode on their own. The ptlcalls.o file is 
  typically linked with Fortran programs that can't use 
  regular C header files.

 Common Libraries and Logic Design APIs

PTLsim includes a number of powerful C++ templates, 
macros and functions not found anywhere else. This 
section attempts to provide an overview of these 
structures so that users of PTLsim will use them 
instead of trying to duplicate work we've already done.

 General Purpose Macros

The file globals.h contains a wide range of very useful 
definitions, functions and macros we have accumulated 
over the years, including:

 Basic data types used throughout PTLsim (e.g. W64 for 
  64-bit words and so on), globals.h defines a

 Type safe C++ template based functions, including min, 
  max, abs, mux, etc.

 Iterator macros (foreach) 

 Template based metaprogramming functions including 
  lengthof (finds the length of any static array) and 
  log2 (takes the base-2 log of any constant at compile time)

 Floor, ceiling and masking functions for integers and 
  powers of two (floor, trunc, ceil, mask, floorptr, 
  ceilptr, maskptr, signext, etc)

 Bit manipulation macros (bit, bitmask, bits, lowbits, 
  setbit, clearbit, assignbit). Note that the bitvec 
  template (see below) should be used in place of these 
  macros wherever it is more convenient.

 Comparison functions (aligned, strequal, inrange, clipto)

 Modulo arithmetic (add_index_modulo, modulo_span, et al)

 Definitions of basic x86 SSE vector functions (e.g. 
  x86_cpu_pcmpeqb et al)

 Definitions of basic x86 assembly language functions 
  (e.g. x86_bsf64 et al)

 A full suite of bit scanning functions (lsbindex, 
  msbindex, popcount et al)

 Miscellaneous functions (arraycopy, setzero, etc)

 Super Standard Template Library (SuperSTL)

The Super Standard Template Library (SuperSTL) is an 
internal C++ library we use internally in lieu of the 
normal C++ STL for various technical and preferential 
reasons. While the full documentation is in the 
comments of superstl.h and superstl.cpp, the following 
is a brief list of its features:

 I/O stream classes familiar from Standard C++, 
  including istream and ostream. Unique to SuperSTL is 
  how the comma operator (",") can be used to separate a 
  list of objects to send to or from a stream, in 
  addition to the usual C++ insertion operator ("<<").

 To read and write binary data, the idstream and 
  odstream classes should be used instead.

 String buffer (stringbuf) class for composing strings 
  in memory the same way they would be written to or 
  read from an ostream or istream.

 String formatting classes (intstring, hexstring, 
  padstring, bitstring, bytemaskstring, floatstring) 
  provide a wrapper around objects to exercise greater 
  control of how they are printed.

 Array (array) template class represents a fixed size 
  array of objects. It is essentially a simple but very 
  fast wrapper for a C-style array.

 Bit vector (bitvec) is a heavily optimized and 
  rewritten version of the Standard C++ bitset class. 
  It supports many additional operations well suited to 
  logic design purposes and emphasizes extremely fast 
  branch free code.

 Dynamic Array (dynarray) template class provides for 
  dynamically sized arrays, stacks and other such 
  structures, similar to the Standard C++ valarray class.

 Linked list node (listlink) template class forms the 
  basis of double linked list structures in which a 
  single pointer refers to the head of the list.

 Queue list node (queuelink) template class supports 
  more operations than listlink and can serve as both a 
  node in a list and a list head/tail header.

 Index reference (indexref) is a smart pointer which 
  compresses a full pointer into an index into a 
  specific structure (made unique by the template 
  parameters). This class behaves exactly like a 
  pointer when referenced, but takes up much less space 
  and may be faster. The indexrefnull class adds 
  support for storing null pointers, which indexref lacks.

 Hashtable class is a general purpose chaining based 
  hash table with user configurable key hashing and 
  management via add-on template classes.

 ChunkHashtable class is a simplified hash table 
  designed for small data items, for instance where we 
  simply want to detect the presence of a key rather 
  than associate data with it. It tries to pack many 
  keys into cache line sized chunks and does parallel 
  vectorized matching on each chunk for added speed.

 CRC32 calculation class is useful for hashing

 CycleTimer is useful for timing intervals with 
  sub-nanosecond precision using the CPU cycle counter 
  (discussed in Section [sec:Timing]).

 Logic Standard Template Library (LogicSTL)

The Logic Standard Template Library (LogicSTL) is an 
internally developed add-on to SuperSTL which supports 
a variety of structures useful for modeling sequential 
logic. Some of its primitives may look familiar to 
Verilog or VHDL programmers. While the full 
documentation is in the comments of logic.h, the 
following is a brief list of its features:

 latch template class works like any other assignable 
  variable, but the new value only becomes visible 
  after the clock() method is called (potentially from 
  a global clock chain).

 Queue template class implements a general purpose 
  fixed size queue. The queue supports various 
  operations from both the head and the tail, and is 
  ideal for modeling queues in microprocessors.

 Iterators for Queue objects such as foreach_forward, 
  foreach_forward_from, foreach_forward_after, 
  foreach_backward, foreach_backward_from, 
  foreach_backward_before.

 HistoryBuffer maintains a shift register of values, 
  which when combined with a hash function is useful 
  for implementing predictor histories and the like.

 FullyAssociativeTags template class is a general 
  purpose array of associative tags in which each tag 
  must be unique. This class uses highly efficient 
  matching logic and supports pseudo-LRU eviction, 
  associative invalidation and direct indexing. It 
  forms the basis for most associative structures in PTLsim.

 FullyAssociativeArray pairs a FullyAssociativeTags 
  object with actual data values to form the basis of a cache.

 AssociativeArray divides a FullyAssociativeArray into 
  sets. In effect, this class can provide a complete 
  cache implementation for a processor.

 LockableFullyAssociativeTags, 
  LockableFullyAssociativeArray and 
  LockableAssociativeArray provide the same services as 
  the classes above, but support locking lines into the cache.

 CommitRollbackCache leverages the 
  LockableFullyAssociativeArray class to provide a 
  cache structure with the ability to roll back all 
  changes made to memory (not just within this object, 
  but everywhere) after a checkpoint is made.

 FullyAssociativeTags8bit and FullyAssociativeTags16bit 
  work just like FullyAssociativeTags, except that 
  these classes are dramatically faster when using 
  small 8-bit and 16-bit tags. This is possible through 
  the clever use of x86 SSE vector instructions to 
  associatively match and process 16 8-bit tags or 8 
  16-bit tags every cycle. In addition, these classes 
  support features like removing an entry from the 
  middle of the array while compacting entries around 
  it in constant time. These classes should be used in 
  place of FullyAssociativeTags whenever the tags are 
  small enough (i.e. almost all tags except for memory 
  addresses).

 Miscellaneous Code

The out of order simulator, ooocore.cpp, contains 
several reusable classes, including:

 IssueQueue template class can be used to implement all 
  kinds of broadcast based issue queues

 StateList and ListOfStateLists is useful for 
  collecting various lists that objects can be on into 
  one structure.

 <sec:Injection>Low Level Startup and Injection

Note: This section deals with the internal operation of 
the PTLsim virtual machine manager, independent of the 
out of order simulation engine. If you are only 
interested in modifying the simulator itself, you can 
skip this section.

PTLsim is a very unusual Linux program. It does its own 
internal memory management and threading without help 
from the standard libraries, injects itself into other 
processes to take control of them, and switches between 
32-bit and 64-bit mode within a single process image. 
For these reasons, it is very closely tied to the Linux 
kernel and uses a number of undocumented system calls 
and features only available in late 2.6 series kernels. 

PTLsim always starts and runs as a 64-bit process even 
when running 32-bit threads; it context switches 
between modes as needed. The statically linked ptlsim 
executable begins executing at ptlsim_preinit_entry in 
lowlevel-64bit.S. This code calls ptlsim_preinit() in 
kernel.cpp to set up our custom memory manager and 
threading environment before any standard C/C++ 
functions are used. After doing so, the normal main() 
function is invoked.

The ptlsim binary can run in two modes. If executed 
from the command line as a normal program, it starts up 
in inject mode. Specifically, main() in ptlsim.cpp 
checks if the inside_ptlsim variable has been set by 
ptlsim_preinit_entry, and if not, PTLsim enters inject 
mode. In this mode, ptlsim_inject() in kernel.cpp is 
called to effectively inject the ptlsim binary into 
another process and pass control to it before even the 
dynamic linker gets to load the program. In 
ptlsim_inject(), the PTLsim process is forked and the 
child is placed under the parent's control using 
ptrace(). The child process then uses exec() to start 
the user program to simulate (this can be either a 
32-bit or 64-bit program). 

However, the user program starts in the stopped state, 
allowing ptlsim_inject() to use ptrace() and related 
functions to inject either 32-bit or 64-bit boot loader 
code directly into the user program address space, 
overwriting the entry point of the dynamic linker. This 
code, derived from injectcode.cpp (specifically 
compiled as injectcode-32bit.o and injectcode-64bit.o) 
is completely position independent. Its sole function 
is to map the rest of ptlsim into the user process 
address space at virtual address 0x70000000 and set up 
a special LoaderInfo structure to allow the master 
PTLsim process and the user process to communicate. The 
boot code also restores the old code at the dynamic 
linker entry point after relocating itself. Finally, 
ptlsim_inject() adjusts the user process registers to 
start executing the boot code instead of the normal 
program entry point, and resumes the user process.

At this point, the PTLsim image injected into the user 
process exists in a bizarre environment: if the user 
program is 32 bit, the boot code will need to switch to 
64-bit mode before calling the 64-bit PTLsim 
entrypoint. Fortunately x86-64 and the Linux kernel 
make this process easy, despite never being used by 
normal programs: a regular far jump switches the 
current code segment descriptor to 0x33, effectively 
switching the instruction set to x86-64. For the most 
part, the kernel cannot tell the difference between a 
32-bit and 64-bit process: as long as the code uses 
64-bit system calls (i.e. syscall instruction instead 
of int 0x80 as with 32-bit system calls), Linux assumes 
the process is 64-bit. There are some subtle issues 
related to signal handling and memory allocation when 
performing this trick, but PTLsim implements 
workarounds to these issues.

After entering 64-bit mode if needed, the boot code 
passes control to PTLsim at ptlsim_preinit_entry. The 
ptlsim_preinit() function checks for the special 
LoaderInfo structure on the stack and in the ELF header 
of PTLsim as modified by the boot code; if these 
structures are found, PTLsim knows it is running inside 
the user program address space. After setting up memory 
management and threading, it captures any state the 
user process was initialized with. This state is used 
to fill in fields in the global ctx structure of class 
CoreContext: various floating point related fields and 
the user program entry point and original stack pointer 
are saved away at this point. If PTLsim is running 
inside a 32-bit process, the 32-bit arguments, 
environment and kernel auxiliary vector array (auxv) 
need to be converted to their 64-bit format for PTLsim 
to be able to parse them from normal C/C++ code. 
Finally, control is returned to main() to allow the 
simulator to start up normally.

 Simulator Startup

In ptlsim.cpp, the main() function calls init_config() 
to read in the user program specific configuration as 
described in Sections [sec:RunningPTLsim] and [sec:ConfigurationOptions], then starts up the various 
other simulator subsystems. If one of the -excludeld or 
-startrip options were given, a breakpoint is inserted 
at the RIP address where the user process should switch 
from native mode to simulation mode (this may be at the 
dynamic linker entry point by default).

Finally, switch_to_native_restore_context() is called 
to restore the state that existed before PTLsim was 
injected into the process and return to the dynamic 
linker entry point. This may involve switching from 
64-bit back to 32-bit mode to start executing the user 
process natively as discussed in Section [sec:Injection].

After native execution reaches the inserted breakpoint 
thunk code, the code performs a 32-to-64-bit long jump 
back into PTLsim, which promptly restores the code 
underneath the inserted breakpoint thunk. At this 
point, the switch_to_sim() function in ptlsim.cpp is 
invoked to actually begin the simulation. This is done 
by calling out_of_order_core_toplevel_loop() in ooocore.cpp.

At some point during simulation, the user program or 
the configuration file may request a switch back to 
native mode for the remainder of the program. In this 
case, the show_stats_and_switch_to_native() function 
gets called to save the statistics data store, map the 
PTLsim internal state back to the x86 compatible 
external state and return to the 32-bit or 64-bit user 
code, effectively removing PTLsim from the loop.

While the real PTLsim user process is running, the 
original PTLsim injector process simply waits in the 
background for the real user program with PTLsim inside 
it to terminate, then returns its exit code.

 <sec:AddressSpaceSimulation>Address Space Simulation

PTLsim maintains the AddressSpace class as global 
variable asp (see kernel.cpp) to track the attributes 
of each page within the virtual address space. To do 
this, PTLsim uses Shadow Page Access Tables (SPATs), 
which are essentially large two-level bitmaps. Since 
pages are 4096 bytes in size, each 64 kilobyte chunk of 
the bitmap can track 2 GB of virtual address space. In 
each SPAT, each top level array entry points to a chunk 
mapping 2 GB, such that with 131072 top level pointers, 
the full 48 bit virtual address space can typically be 
mapped with under a megabyte of SPAT chunks, assuming 
the address space is sparse.

In the AddressSpace structure, there are separate SPAT 
tables for readable pages (readmap field), writable 
pages (writemap field) and executable pages (execmap 
field). Two additional SPATs, dtlbmap and itlbmap, are 
used to track which pages are currently mapped by the 
simulated translation lookaside buffers (TLBs); this is 
discussed further in Section [sec:TranslationLookasideBuffers].

When running in native mode, PTLsim cannot track 
changes to the process memory map made by native calls 
to mmap(), munmap(), etc. Therefore, at every switch 
from native to simulation mode, the 
resync_with_process_maps() function is called. This 
function parses the /proc/self/maps metafile maintained 
by the kernel to build a list of all regions mapped by 
the current process. Using this list, the SPATs are 
rebuilt to reflect the current memory map. This is 
absolutely critical for correct operation, since during 
simulation, speculative loads and stores will only read 
and write memory if the appropriate SPAT indicates the 
address is accessible to user code. If the SPATs become 
out of sync with the real memory map, PTLsim itself may 
crash rather than simply marking the offending load or 
store as invalid. The resync_with_process_maps() 
function (or more specifically, the mqueryall() helper 
function) is fairly kernel version specific since the 
format of /proc/self/maps has changed between Linux 
2.6.x kernels. New kernels may require updating this function.

PTLsim does not use the normal C library 
implementations of malloc(), free(), mmap(), new, 
delete and so on. Instead, PTLsim code should always 
use the ptl_alloc_private_pages() family of functions 
defined in kernel.cpp to ensure that PTLsim memory 
remains completely invisible to user code (except, of 
course, within PTLsim generated microcode sequences). 
This is done by clearing the bits in the read, write 
and execute SPATs corresponding to the allocated pages. 
The new, delete, malloc() and free() functions can 
still be used since PTLsim overrides these. Note that 
memory allocated in this way will not be accessible to 
user code.

 Debugging Hints

When adding or modifying PTLsim, bugs will invariably 
crop up. Fortunately, PTLsim provides a trivial way to 
find the location of bugs which silently corrupt 
program execution. Since PTLsim can transparently 
switch between simulation and native mode, isolating 
the divergence point between the simulated behavior and 
what a real reference machine would do can be done 
through binary search. The -stopinsns configuration 
option can be set to stop simulation before the problem 
occurs, then incremented until the first x86 
instruction to break the program is determined.

The out of order simulator (ooocore.cpp) includes 
extensive debugging and integrity checking assertions. 
These may be turned off by default for improved 
performance, but they can be easily re-enabled by 
defining the ENABLE_CHECKS symbol at the top of 
ooocore.cpp. Additional check functions are in the code 
but commented out; these may be used as well.

 <sec:Timing>Timing Issues

PTLsim uses the CycleTimer class extensively to gather 
data about its own performance using the CPU's 
timestamp counter. At startup in superstl.cpp, the 
CPU's maximum frequency is queried from the appropriate 
Linux kernel sysfs node (if available) or from 
/proc/cpuinfo if not. Processors which dynamically 
scale their frequency and voltage in response to load 
(like all Athlon 64 and K8 based AMD processors) 
require special handling. It is assumed that the 
processor will be running at its maximum frequency (as 
reported by sysfs) or a fixed frequency (as reported by 
/proc/cpuinfo) throughout the majority of the 
simulation time; otherwise the timing results will be bogus.

<sec:StatisticsInfrastructure>Statistics Collection and Control

 Using PTLstats to Analyze Statistics

PTLsim maintains a huge number of statistical counters 
and data points during the simulation process, and can 
optionally save this data to a statistics data store by 
using the "-stats filename" configuration option 
introduced in Section [sec:ConfigurationOptions]. The data store is a binary file 
format (defined in datastore.cpp) used to efficiently 
capture large quantities of statistical information for 
later analysis. This file format supports storing 
multiple regular snapshots of all counters by 
specifying the "-snapshot N" option to save a snapshot of 
the simulator state every N cycles in addition to the 
final state.

The PTLstats program is used to analyze the statistics 
data store files produced by PTLsim. The syntax of this 
command is "ptlstats -command filename". Presently the 
most useful command is "-dumpraw", which prints out a 
textual representation of the internal tree format in 
which the statistics are maintained. PTLstats will 
automatically sum up all entries in certain branches of 
the tree to provide the user with a breakdown by 
percentages of the total for that subtree in addition 
to the raw values.

The following is an example of the type of output you 
can expect from the PTLstats command -dumpraw:

dcache {

  store {

    issue (total 134243383) {

      [  22% ] replay (total 29278598) {

        [   0% ] wait-sfraddr = 0;

        [  33% ] wait-storedata-sfraddr = 9755097;

        [  33% ] wait-storedata-sfraddr-sfrdata = 9755097;

        [   6% ] wait-storedata-sfrdata = 1891253;

        [   4% ] wait-sfrdata = 1069751;

        [  23% ] wait-sfraddr-sfrdata = 6807400;

      }

      [   0% ] exception = 196094;

      [   0% ] ordering = 55369;

      [  78% ] complete = 104592504;

      [   0% ] unaligned = 120818;

    }

    ...



 Statistics support in code

Internally, PTLsim represents data store nodes in a 
tree format via the DataStoreNode class defined in 
datastore.h. Each node can contain other data store 
nodes as well as a value (64-bit integer, floating 
point, character string) or an array of values. The 
following example illustrates the proper way to save 
statistical counters (typically declared as global 
variables of type W64, a 64-bit integer) into the data 
store. In this example, the root DataStoreNode is 
assumed to be passed in as the parent of all other nodes:

DataStoreNode& issue = root("issue"); {

  DataStoreNode& unit = issue("unit"); {

    unit.summable = 1;

    unit.add("integer", issued_integer_uops);

    unit.add("fp", issued_fp_uops);

    unit.add("load", issued_load_uops);

    unit.add("store", issued_store_uops);

  }

  DataStoreNode& histogram = issue("times"); {

    cluster.summable = 1;

    foreach (i, MAX_CLUSTERS) {

      stringbuf sb; sb << i;

      cluster.addfloat(sb, timer_histogram[i]);

    }

  }

}

In the example above, a statistics tree is created in 
which all subnodes under the "unit" node are integers 
together assumed to total 100% of whatever quantity is 
being measured. Setting summable = 1 tells PTLstats to 
print percentages next to the raw values in this 
subtree for easier viewing. The "times" subnode contains 
floating point values; the stringbuf utility class is 
used to convert each slot index of this example 
histogram into a proper node name. This is only an 
example - PTLsim already contains code to save all 
statistics it generates.

PTLsim will by default call ooo_capture_stats() every 
time a snapshot or final capture is taken, and will 
pass this function the top level DataStoreNode for that 
snapshot. Therefore, if adding your own statistics and 
counters, you should start by saving these counters 
using code inside ooo_capture_stats() like that shown 
above. 

We suggest using the data store mechanism to store all 
statistics generated by your additions to PTLsim, since 
this system has built-in support for snapshots, 
checkpointing and structured easy to parse data (unlike 
simply writing values to a text file). It is further 
suggested that only raw values be saved, rather than 
doing computations in the simulator itself - leave the 
analysis to PTLstats after gathering the raw data. In 
particular, try to avoid using floating point within 
the simulator if at all possible, since some floating 
point calculations may reconfigure the SSE rounding and 
control flags in ways that break the assumptions used 
to execute actual user code.

 <sec:TriggerMode>PTLsim Calls From User Code

PTLsim optionally allows user code to control the 
simulator mode through the ptlcall_xxx() family of 
functions found in ptlcalls.h when trigger mode is 
enabled (-trigger configuration option). This file 
should be included by any PTLsim-aware user programs; 
these programs must be recompiled to take advantage of 
these features. Amongst the functions provided by 
ptlcalls.h are:

 ptlcall_switch_to_sim() is only available while the 
  program is executing in native mode. It forces PTLsim 
  to regain control and begin simulating instructions 
  as soon as this call returns.

 ptlcall_switch_to_native() stops simulation and 
  returns to native execution, effectively removing 
  PTLsim from the loop.

 ptlcall_marker() simply places a user-specified marker 
  number in the PTLsim log file

 ptlcall_capture_stats() adds a new statistics data 
  store snapshot at the time it is called. 

 ptlcall_nop() does nothing but test the call mechanism.

These calls work by forcing execution to code on a "
gateway page" at a specific fixed address (0x1000 
currently); PTLsim will write the appropriate call gate 
code to this page depending on whether the process is 
in native or simulated mode. In native mode, the call 
gate page typically contains a 64-to-64-bit or 
32-to-64-bit far jump into PTLsim, while in simulated 
mode it contains a reserved x86 opcode interpreted by 
the x86 decoder as a special kind of system call.

Generally these calls are used to perform "intelligent 
benchmarking": the ptlcall_switch_to_sim() call is made 
at the top of the main loop of a benchmark after 
initialization, while the ptlcall_switch_to_native() 
call is inserted after some number of iterations to 
stop simulation after a representative subset of the 
code has completed. This intelligent approach is far 
better than the blind "sample for N million cycles after 
S million startup cycles" approach used by most researchers.

Fortran programs will have to actually link in the 
ptlcalls.o object file, since they cannot include C 
header files. The function names that should be used in 
the Fortran code remain the same as those from the 
ptlcalls.h header file.

 Performance and Statistical Counters

The full list of PTLsim performance and statistical 
counters is given in Section [sec:PerformanceCounters].

<part:x86andUops>x86 Instruction Set and PTLsim Micro-Operations

x86 Instructions and Micro-Ops (uops)

 <sec:UopIntro>Micro-Ops (uops) and TransOps

PTLsim presents to user code a full implementation of 
the x86 and x86-64 instruction set (both 32-bit and 
64-bit modes), including most user level instructions 
supported by the Intel Pentium 4 and AMD K8 
microprocessors (i.e. all standard instructions, 
SSE/SSE2, x86-64 and most of x87 FP). At the present 
stage of development, the vast majority of all 
userspace instructions are implemented. 

The x86 instruction set is based on the two-operand 
CISC concept of load-and-compute and 
load-compute-store. However, modern x86 processors 
(including PTLsim) do not directly execute complex x86 
instructions. Instead, these processors translate each 
x86 instruction into a series of micro-operations 
(uops) very similar to classical load-store RISC 
instructions. Uops can be executed very efficiently on 
an out of order core, unlike x86 instructions. In 
PTLsim, uops have three source registers and one 
destination register. They may generate a 64-bit result 
and various x86 status flags, or may be loads, stores 
or branches.

The x86 instruction decoding process initially 
generates translated uops (transops), which have a 
slightly different structure than the true uops used in 
the processor core. Specifically, sources and 
destinations are represented as un-renamed 
architectural registers (or special temporary register 
numbers), and a variety of additional information is 
attached to each uop only needed during the renaming 
and retirement process. TransOps (represented by the 
TransOp structure) consist of the following:

 som: Start of Macro-Op. Since x86 instructions may 
  consist of multiple transops, the first transop in 
  the sequence has its som bit set to indicate this.

 eom: End of Macro-Op. This bit is set for the last 
  transop in a given x86 instruction (which may also be 
  the first uop for single-uop instructions)

 bytes: Number of bytes in the corresponding x86 
  instruction (1-15). This is only valid for a SOM uop.

 opcode: the uop (not x86) opcode

 size: the effective operation size (0-3, for 1/2/4/8 bytes)

 cond: the x86 condition code for branches, selects, 
  sets, etc. For loads and stores, this field is reused 
  to specify unaligned access information as described later.

 setflags: subset of the x86 flags set by this uop (see 
  Section [sub:FlagsManagement])

 internal: set for certain microcode operations. For 
  instance, loads and stores marked internal access 
  on-chip registers or buffers invisible to x86 code 
  (e.g. machine state registers, segmentation caches, 
  floating point constant tables, etc).

 rd, ra, rb, rc: the architectural source and 
  destination registers (see Section [sub:RegisterRenaming])

 extshift: shift amount (0-3 bits) used for shifted 
  adds (x86 memory addressing and LEA). The rc operand 
  is shifted left by this amount.

 cachelevel: used for prefetching and non-temporal 
  loads and stores

 rbimm and rcimm: signed 64-bit immediates for the rb 
  and rc operands. These are selected by specifying the 
  special constant REG_imm in the rb and rc fields, respectively.

 riptaken: for branches only, the 64-bit target RIP of 
  the branch if it were taken.

 ripseq: for branches only, the 64-bit sequential RIP 
  of the branch if it were not taken.

There may be other fields used for debugging or not 
relevant to the out of order version of the simulator; 
these should be ignored.

 Descriptions of uops

Section [sec:UopReference] describes the semantics and encoding of all 
uops supported by the PTLsim processor model.

 Simple Fast Path Instructions

Simple integer and floating point operations are fairly 
straightforward to decode into loads, stores and ALU 
operations; a typical load-op-store ALU operation will 
consist of a load to fetch one operand, the ALU 
operation itself, and a store to write the result. The 
instruction set also implements a number of important 
but complex instructions with bizarre semantics; 
typically the translator will synthesize and inject 
into the uop stream up to 8 uops for more complex 
instructions. 

 x86-64

The 64-bit x86-64 instruction set is a fairly 
straightforward extension of the 32-bit IA-32 (x86) 
instruction set. The x86-64 ISA was introduced by AMD 
in 2000 with its K8 microarchitecture; the same 
instructions were subsequently plagiarized by Intel 
under a different name several years later. In addition 
to extending all integer registers and ALU datapaths to 
64 bits, x86-64 also provides a total of 16 integer 
general purpose registers and 16 SSE (vector floating 
and fixed point) registers. It also introduced several 
64-bit address space simplifications, including 
RIP-relative addressing and corresponding new 
addressing modes, and eliminated a number of legacy 
features from 64-bit mode, including segmentation, BCD 
arithmetic, some byte register manipulation, etc. 
Limited forms of segmentation are still present to 
allow thread local storage and mark code segments as 
64-bit. In general, the encoding of x86-64 and x86 are 
very similar, with 64-bit mode adding a one byte REX 
prefix to specify additional bits for source and 
destination register indexes and effective address 
size. As a result, both variants can be decoded by 
similar decoding logic into a common set of uops.

 <sec:OperationSizes>Operation Sizes

Most x86-64 instructions can operate on 8, 16, 32 or 64 
bits of a given register. For 8-bit and 16-bit 
operations, only the low 8 or 16 bits of the 
destination register are actually updated; 32-bit and 
64-bit operations are zero extended as with RISC 
architectures. As a result, a dependency on the old 
destination register may be introduced so merging can 
be performed. Fortunately, since x86 features 
destructive overwrites of the destination register 
(i.e. the rd and ra operands are the same), the ra 
operand is generally already a dependency. Thus, the 
PT2x uop encoding reserves 2 bits to specify the 
operation size; the low bits of the new result are 
automatically merged with the old destination value (in 
ra) as part of the ALU logic. This applies to the mov 
uop as well, allowing operations like "mov al,bl" in one 
uop. Loads do not support this mode, so loads into 
8-bit and 16-bit registers must be followed by a 
separate mov uop to truncate and merge the loaded value 
into the old destination properly.

The x86 ISA defines some bizarre byte operations as a 
carryover from the ancient 8086 architecture; for 
instance, it is possible to address the second byte of 
many integer registers as a separate register (i.e. as 
ah, bh, ch, dh). The inshb, exthb and movhb uops are 
provided for handling this rare but important operation.

 <sub:FlagsManagement>Flags Management and Register Renaming

Many x86 arithmetic instructions modify some or all of 
the processor's numerous status and condition flag 
bits, but only 5 are relevant to normal execution: 
Zero, Parity, Sign, Overflow, Carry. In accordance with 
the well-known "ZAPS rule", any instruction that updates 
any of the Z/P/S flags updates all three flags, so in 
reality only three flag entities need to be tracked: 
ZPS, O, F ("ZAPS" also includes an Auxiliary flag not 
accessible by most modern user instructions; it is 
irrelevant to the discussion below).

The x86 flag update semantics can hamper out of order 
execution, so we use a simple and well known solution. 
The 5 flag bits are attached to each result and 
physical register (along with the invalid and waiting 
bits described in Section [sec:PhysicalRegisters]); these bits are then 
consumed along with the actual result value by any 
consumers that also need to access the flags. It should 
be noted that not all uops generate all the flags as 
well as a 64-bit result, and some uops only generate 
flags and no result data. 

The register renaming mechanism is aware of these 
semantics, and tracks the latest x86 instruction in 
program order to update each set of flags (ZAPS, C, O); 
this allows branches and other flag consumers to 
directly access the result with the most recent 
program-ordered flag updates yet still allows full out 
of order scheduling. To do this, x86 processors 
maintain three separate rename table entries for the 
ZAPS, CF, OF flags in addition to the register rename 
table entry, any or all of which may be updated when 
uops are renamed. The TransOp structure for each uop 
has a 3-bit setflags field filled out during decoding 
in accordance with x86 semantics; the SETFLAG_ZF, 
SETFLAG_CF, SETFLAG_OF bits in this field are used to 
determine which of the ZPS, O, F flag subsets to rename.

As mentioned above, any consumer of the flags needs to 
consult at most three distinct sources: the last ZAPS 
producer, the Carry producer and the Overflow producer. 
This conveniently fits into PTLsim's three-operand uop 
semantics. Various special uops access the flags 
associated with an operand rather than the 64-bit 
operand data itself. Branches always take two flag 
sources, since in x86 this is enough to evaluate any 
possible condition code combination (the 
cond_code_to_flag_regs array provides this mapping). 
Various ALU instructions consume only the flags part of 
a source physical register; these include addc (add 
with carry), rcl/rcr (rotate carry), sel.cc (select for 
conditional moves) and so on. Finally, the collcc uop 
takes three operands (the latest producer of the ZAPS, 
CF and OF flags) and merges the flag components of each 
operand into a single flag set as its result. These 
uops are all documented in Section [sec:UopReference].

 <sub:UnalignedLoadsAndStores>Unaligned Loads and Stores

Compared to RISC architectures, the x86 architecture is 
infamous for its relatively widespread use of unaligned 
memory operations; any implementation must efficiently 
handle this scenario. Fortunately, analysis shows that 
unaligned accesses are rarely in the performance 
intensive parts of a modern program, so we can 
aggressively eliminate them on contact through 
rescheduling. PTLsim does this by initially causing all 
unaligned loads and stores to raise an UnalignedAccess 
internal exception, forcing a rollback of the current 
trace. To locate the RIP of the offending x86 
instruction, the sequential version of the trace is 
then executed until the unaligned access is encountered 
again. At this point, a special "unaligned" bit is set 
for the problem load or store in its translated basic 
block representation. The sequential basic block is 
then retranslated such that when the x86 instruction 
with the problem RIP is encountered, the offending load 
or store is split into two aligned loads or stores.

PTLsim includes special uops to handle loads and stores 
split into two in this manner. The ld.lo uop rounds 
down its effective address \left\lfloor A\right\rfloor  to the nearest 64-bit 
boundary and performs the load. The ld.hi uop rounds up 
to \left\lceil A+8\right\rceil , performs another load, then takes as its third rc 
operand the first (ld.lo) load's result. The two loads 
are concatenated into a 128-bit word and the final 
unaligned data is extracted. Stores are handled in a 
similar manner, with st.lo and st.hi rounding down and 
up to store parts of the unaligned value in adjacent 
64-bit blocks. Just as with normal loads and stores, 
these unaligned load or store pairs access separate 
store buffers for each half as if they were independent.

 Repeated String Operations

The x86 architecture allows for repeated string 
operations, including block moves, stores, compares and 
scans. The iteration count of these repeated operations 
depends on a combination of the rcx register and the 
flags set by the repeated operation (e.g. compare). To 
translate these instructions, PTL treats the rep xxx 
instruction as a single basic block; any basic block in 
progress before the repeat instruction is capped and 
the repeat is translated as a separate basic block. 
This conveniently lets us unroll and optimize the 
repeated loop just like any other basic block. To 
handle the unusual case where the repeat count is zero, 
a check instruction is inserted at the top of the loop 
to protect against this case; PTL simply bypasses the 
offending block if the check fails.

 <sec:ShiftRotateProblems>Problem Instructions

The shift and rotate instructions have some of the most 
bizarre semantics in the entire x86 instruction set: 
they may or may not modify a subset of the flags 
depending on the rotation count operand, which we may 
not even know until the instruction issues. For fixed 
shifts and rotates, these semantics can be preserved by 
the uops generated, however variable rotations are more 
complex. The collcc uop is put to use here to collect 
all flags; the collected result is then fed into the 
shift or rotate uop as its rc operand; the uop then 
replicates the precise x86 behavior (including rotates 
using the carry flag) according to its input operands.

 SSE Support

PTLsim provides full support for SSE and SSE2 vector 
floating point and fixed point, in both scalar and 
vector mode. As is done in the AMD K8, each SSE 
operation on a 128-bit vector is split into two 64-bit 
halves; each half (possibly consisting of a 64-bit load 
and one or more FPU operations) is scheduled 
independently. Because SSE instructions do not set 
flags like x86 integer instructions, architectural 
state management can be restricted to the 16 128-bit 
SSE registers (represented as 32 paired 64-bit 
registers) and a single mxcsr architectural register 
containing sticky exception bits, which has no effect 
on out of order execution). The processor's floating 
point units can operate in either 64-bit IEEE double 
precision mode or on two parallel 32-bit single 
precision values.

 <sub:x87-Floating-Point>x87 Floating Point

The legacy x87 floating point architecture is the bane 
of all x86 processor vendors' existence, largely 
because its stack based nature makes out of order 
processing so difficult. While there are certainly ways 
of translating stack based instruction sets into flat 
addressing for scheduling purposes, we do not do this. 
Fortunately, following the Pentium III and AMD Athlon's 
introduction, x87 is rapidly headed for planned 
obsolescence; most major applications released within 
the last three years now use SSE instructions for their 
floating point needs either exclusively or in all 
performance critical parts. To this end, even Intel has 
relegated x86 support on the Pentium 4 to a separate 
low performance in-order legacy unit, and AMD has 
severely restricted its use in 64-bit mode. For this 
reason, PTLsim translates legacy x87 instructions into 
a serialized, program ordered and emulated form; the 
hardware does not contain any x87-style 80-bit floating 
point registers (all floating point hardware is 32-bit 
and 64-bit IEEE compliant). We have noticed little to 
no performance problem from this approach when 
examining typical binaries, which rarely if ever still 
use x87 instructions in compute-intensive code.

 Assists

Some operations are too complex to inline directly into 
the uop stream. To perform these instructions, a 
special uop (brp: branch private) is executed to branch 
to an assist function implemented in microcode. In 
PTLsim, some assist functions are implemented as 
regular C/C++ or assembly language code when they 
interact with the rest of the virtual machine. Examples 
of instructions requiring assists include system calls, 
interrupts, some forms of integer division, handling of 
rare floating point conditions, CPUID, MSR 
reads/writes, etc. These are listed in the ASSIST_xxx 
enum found in translate-x86.cpp and ptlhwdef.h.

When the processor issues an assist (brp uop), the 
frontend pipeline is stalled and execution waits until 
the brp retires, at which point an assist function 
within PTLsim is called. In a real processor there are 
more efficient ways of doing this without flushing the 
pipeline, however in PTLsim assists are sufficiently 
rare that the performance impact is negligible and this 
approach significantly reduces complexity.

<part:OutOfOrderModel>Out of Order Processor Model

<sec:OutOfOrderFeatures>Introduction

PTLsim completely models a modern out of order x86-64 
compatible processor and cache hierarchy with cycle 
accurate simulation. The basic microarchitecture of 
this model is most similar to the Intel Pentium 4 
processor, but incorporates some ideas from AMD K8, IBM 
Power4/Power5 and Alpha EV8. The following is a summary 
of the characteristics of this processor model:

 The simulator directly fetches pre-decoded 
  micro-operations (Section [sec:FetchStage]) but can simulate cache 
  accesses as if x86 instructions were being decoded on fetch

 Branch prediction is configurable; PTLsim currently 
  includes various models including a hybrid g-share 
  based predictor, bimodal predictors, saturating 
  counters, etc.

 Register renaming takes into account x86 quirks such 
  as flags renaming (Section [sub:FlagsManagement])

 Front end pipeline has configurable number of cycles 
  to simulate x86 decoding or other tasks; this is used 
  for adjusting the branch mispredict penalty

 Unified physical and architectural register file maps 
  both in-flight uops as well as committed 
  architectural register values. Two rename tables 
  (speculative and committed register rename tables) 
  are used to track which physical registers are 
  currently mapped to architectural registers.

 Unified physical register file for both integer and 
  floating point values.

 Operands are read from the physical register file 
  immediately before issue. Unlike in some 
  microprocessors, PTLsim does not do speculative 
  scheduling: the schedule and register read loop is 
  assumed to take one cycle.

 Issue queues based on a collapsing design use 
  broadcast based matching to wake up instructions.

 Clustered microarchitecture is highly configurable, 
  allowing multi-cycle latencies between clusters and 
  multiple issue queues within the same logical cluster.

 Functional units, mapping of functional units to 
  clusters, issue ports and issue queues and uop 
  latencies are all configurable.

 Speculation recovery from branch mispredictions and 
  load/store aliasing uses the forward walk method to 
  recover the rename tables, then annuls all uops after 
  and optionally including the mis-speculated uop.

 Replay of loads and stores after store to load 
  forwarding and store to store merging dependencies 
  are discovered.

 Stores may issue even before data to store is known; 
  the store uop is replayed when all operands arrive.

 Load and store queues use partial chunk address 
  matching and store merging for high performance and 
  easy circuit implementation.

 Prediction of load/store aliasing to avoid 
  mis-speculation recovery overhead.

 Prediction and splitting of unaligned loads and stores 
  to avoid mis-speculation overhead

 Commit unit supports stalling until all uops in an x86 
  instruction are complete, to make x86 instruction 
  commitment atomic

The PTLsim model is fully configurable in terms of the 
sizes of key structures, pipeline widths, latency and 
bandwidth and numerous other features.

Fetch Stage

 <sec:FetchStage>Instruction Fetching and the Basic Block Cache

As described in Section [sec:UopIntro], x86 instructions are decoded 
into transops prior to actual execution by the out of 
order core. Some processors do this translation as x86 
instructions are fetched from an L1 instruction cache, 
while others use a trace cache to store pre-decoded 
uops. PTLsim takes a middle ground to allow maximum 
simulation flexibility. Specifically, the Fetch stage 
accesses the L1 instruction cache and stalls on cache 
misses as if it were fetching several variable length 
x86 instructions per cycle. However, actually decoding 
x86 instructions into uops over and over again during 
simulation would be extraordinarily slow. 

Therefore, for simulation purposes only, PTLsim 
maintains a basic block cache containing the program 
ordered translated uop (transop) sequence for 
previously decoded basic blocks in the program. Each 
basic block (BasicBlock structure) consists of up to 64 
transops and is terminated by either a control flow 
operation (conditional, unconditional, indirect branch) 
or a barrier operation (e.g. system call, synchronizing 
instruction, etc). During the fetch process 
(implemented in the fetch() function), PTLsim looks up 
the current RIP to fetch from and uses the basic block 
cache to map that RIP to a BasicBlock structure. The 
transop stream is then read from that decoded basic 
block in lieu of decoding the x86 instructions again. 
As execution runs off the end of each decoded basic 
block, the fetch unit checks if the next RIP exists in 
the basic block cache. If so, the next block is 
streamed into the fetch queue. Otherwise, the 
x86-to-transop translator is called via 
translate_basic_block() to translate an entire basic 
block at the current RIP before resuming execution. 
Since the basic block cache is for simulation purposes 
only, this adds no additional cycles to the simulated program.

An additional optimization, called synthesis, is also 
used: each uop in the basic block is mapped to the 
address of a native PTLsim function implementing the 
semantics of that uop. This saves us from having to use 
a large jump table later on, and can map uops to 
pre-compiled templates that avoid nearly all further 
decoding of the uop during execution.

 Fetch Queue

Each transop fetched into the pipeline is immediately 
assigned a monotonically increasing uuid (universally 
unique identifier) to uniquely track it for debugging 
and statistical purposes. The fetch unit attaches 
additional information to each transop (such as the 
uop's uuid and the RIP of the corresponding x86 
instruction) to form a FetchBufferEntry structure. This 
fetch buffer is then placed into the fetch queue 
(fetchq) assuming it isn't full (if it is, the fetch 
stage stalls). As the fetch unit encounters transops 
with their EOM (end of macro-op) bit set, the fetch RIP 
is advanced to the next x86 instruction according to 
the instruction length stored in the SOM transop.

Branch uops trigger the branch prediction mechanism 
used to select the next fetch RIP. Based on various 
information encoded in the branch transop and the next 
RIP after the x86 instruction containing the branch, 
the branchpred.predict() function is used to redirect 
fetching. If the branch is predicted not taken, the 
sense of the branch's condition code is inverted and 
the transop's riptaken and ripseq fields are swapped; 
this ensures all branches are considered correct only 
if taken. Indirect branches (jumps) have their riptaken 
field overwritten by the predicted target address.

Frontend and Key Structures

 Resource Allocation

During the Allocate stage, PTLsim dequeues uops from 
the fetch queue, ensures all resources needed by those 
uops are free, and assigns resources to each uop as 
needed. These resources include Reorder Buffer (ROB) 
slots, physical registers and load store queue (LSQ) 
entries. In the event that the fetch queue is empty or 
any of the ROB, physical register file, load queue or 
store queue is full, the allocation stage stalls until 
some resources become available.

 Reorder Buffer Entries

The Reorder Buffer (ROB) in the PTLsim out of order 
model works exactly like a traditional ROB: as a queue, 
entries are allocated from the tail and committed from 
the head. Each ReorderBufferEntry structure is the 
central tracking structure for uops in the pipeline. 
This structure contains a variety of fields including:

 The decoded uop (uop field). This is the fully decoded 
  TransOp augmented with fetch-related information like 
  the uop's UUID, RIP and branch predictor information 
  as described in the Fetch stage (Section [sec:FetchStage]).

 Current state of the ROB entry and uop 
  (current_state_list; see below)

 Indices of the physical register (physreg), LSQ entry 
  (lsq) and other resources allocated to the uop

 Indices of the three physical register operands to the 
  uop, as well as a possible store dependency used in 
  replay scheduling (described later)

 Various cycle counters and related fields for 
  simulating progress through the pipeline

 ROB States

Each ROB entry and corresponding uop can be in one of a 
number of states describing its progress through the 
simulator state machine. ROBs are linked into linked 
lists according to their current state; these lists are 
named rob_statename_list. The current_state_list field 
specifies the list the ROB is currently on. ROBs can be 
moved between states using the 
ROB::changestate(statelist) method. The specific states 
will be described below as they are encountered.

NOTE: the terms "ROB entry" (singular) and "uop" are used 
interchangeably from now on unless otherwise stated, 
since there is a 1:1 mapping between the two.

 <sec:PhysicalRegisters>Physical Registers

 Physical Registers

Physical registers are represented in PTLsim by the 
PhysicalRegister structure. Physical registers store 
several components:

 The actual 64-bit register data

 x86 flags: Z, P, S, O, C. These are discussed below in 
  Section [sub:FlagsManagement].

 Waiting flag (FLAG_WAIT) for results not yet ready

 Invalid flag (FLAG_INVAL) for ready results which 
  encountered an exception. The exception code is 
  written to the data field in lieu of the real result.

 Current state of the physical register

 ROB currently owning this physical register, or 
  architectural register mapping this physical register

 Reference counter for the physical register. This is 
  required for reasons described in Section [sub:PhysicalRegisterRecyclingComplications].

 Physical Register File

PTLsim models a unified physical register file holding 
both integer and floating point results; this is 
required because presently many PTL uops assume both 
types of registers are accessible as operands to a 
single uop.

 Physical Register States

Each physical register can be in one of several states 
at any given time; PTLsim maintains linked lists (the 
physreg_statename_list lists) to track which registers 
are in each state. The current_state_list field in each 
physical register specifies this state. The valid 
states are:

 free: the register is not allocated to any uop.

 used: the register has been allocated to a uop but 
  that uop has not issued yet.

 ready: the uop associated with the register has issued 
  and produced a value (or encountered an exception), 
  but that value is only on the bypass network - it has 
  not actually been written back yet. For simulation 
  purposes only, uops immediately write their results 
  into the physical register as soon as they issue, 
  even though technically the result is still only on 
  the bypass network. This helps simplify the simulator 
  considerably without compromising accuracy.

 written: the uop associated with the register has 
  passed through the writeback stage and the value of 
  the physical register is now up to date; all future 
  consumers will read the uop's result from this 
  physical register.

 arch: the physical register is currently mapped to one 
  of the architectural registers; it has no associated 
  uop currently in the pipeline

 pendingfree: this is a special state described in 
  Section [sub:PhysicalRegisterRecyclingComplications].

One physical register is allocated to each uop and 
moved into the used state, regardless of which type of 
uop it is. For integer, floating point and load uops, 
the physical register holds the actual numerical value 
generated by the corresponding uop. Branch uops place 
the target RIP of the branch in a physical register. 
Store uops place the physical address in the register. 
Technically branches and stores do not need physical 
registers, but to keep the processor design simple, 
they are allocated registers anyway.

 <sec:LoadStoreQueueEntry>Load Store Queue Entries

Load Store Queue (LSQ) Entries (the LoadStoreQueueEntry 
structure in PTLsim) are used to track additional 
information about loads and stores in the pipeline that 
cannot be represented by a physical register. 
Specifically, LSQ entries track:

 Physical address of the corresponding load or store

 Data field (64 bits) stores the loaded value (for 
  loads) or the value to store (for stores)

 Address valid bit flag indicates if the load or store 
  knows its effective physical address yet. If set, the 
  physical address field is valid.

 Data valid bit flag indicates if the data field is 
  valid. For loads, this is set when the data has 
  arrived from the cache. For stores, this is set when 
  the data to store becomes ready and is merged.

 Invalid bit flag is set if an exception occurs in the 
  corresponding load or store.

The LoadStoreQueueEntry structure is technically a 
superset of a structure known as an SFR (Store 
Forwarding Register), which completely represents any 
load or store and can be passed between PTLsim 
subsystems easily. One LSQ entry is allocated to each 
load or store during the Allocate stage.

In real processors, the load queue (LDQ) and store 
queue (STQ) are physically separate for circuit 
complexity reasons. However, in PTLsim a unified LSQ is 
used to make searching operations easier. One 
additional bit flag (store bit) specifies whether an 
LSQ entry is a load or store.

 <sub:RegisterRenaming>Register Renaming

The basic register renaming process in the PTLsim x86 
model is very similar to classical register renaming, 
with the exception of the flags complications described 
in Section [sub:FlagsManagement]. Two versions of the register rename table 
(RRT) are maintained: a speculative RRT which is 
updated as uops are renamed, and a commit RRT, which is 
only updated when uops successfully commit. Since the 
simulator implements a unified physical and 
architectural register file, the commit process does 
not actually involve any data movement between physical 
and architectural registers: only the commit RRT needs 
to be updated. The commit RRT is used only for 
exception and branch mispredict recovery, since it 
holds the last known good mapping of architectural to 
physical registers.

Each rename table contains 80 entries as shown in Table [table:ArchitecturalRegisters]
. This table maps architectural registers and 
pseudo-registers to the most up to date physical 
registers for the following:

 16 x86-64 integer registers

 16 128-bit SSE registers (represented as separate 
  64-bit high and low halves)

 ZAPS, CF, OF flag sets described in Section [sub:FlagsManagement]. These 
  rename table entries point to the physical register 
  (with attached flags) of the most recent uop in 
  program order to update any or all of the ZAPS, CF, 
  OF flag sets, respectively.

 Various integer and x87 status registers

 Temporary pseudo-registers temp0-temp7 not visible to 
  x86 code but required to hold temporaries (e.g. 
  generated addresses or value to swap in xchg instructions).

 Special fixed values, e.g. zero, imm (value is in 
  immediate field), mem (destination of stores)

<table:ArchitecturalRegisters>Architectural registers and pseudo-registers used for renaming.

+------------------------------------------------------------------------------------+
|                    Architectural Registers and Pseudo-Registers                    |
+------------------------------------------------------------------------------------+
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 0   | rax     | rcx     | rdx     | rbx     | rsp     | rbp     | rsi     | rdi    |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 8   | r8      | r9      | r10     | r11     | r12     | r13     | r14     | r15    |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 16  | xmml0   | xmmh0   | xmml1   | xmmh1   | xmml2   | xmmh2   | xmml3   | xmmh3  |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 24  | xmml4   | xmmh4   | xmml5   | xmmh5   | xmml6   | xmmh6   | xmml7   | xmmh7  |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 32  | xmml8   | xmmh8   | xmml9   | xmmh9   | xmml10  | xmmh10  | xmml11  | xmmh11 |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 40  | xmml12  | xmmh12  | xmml13  | xmmh13  | xmml14  | xmmh14  | xmml15  | xmmh15 |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 48  | fptos   | fpsw    | fpcw    | fptags  | fp4     | fp5     | fp6     | fp7    |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 56  | rip     | flags   | sr3     | mxcsr   | sr0     | sr1     | sr2     | zero   |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 64  | temp0   | temp1   | temp2   | temp3   | temp4   | temp5   | temp6   | temp7  |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+
| 72  | zf      | cf      | of      | imm     | mem     | temp8   | temp9   | temp10 |
+-----+---------+---------+---------+---------+---------+---------+---------+--------+


Once the uop's three architectural register sources are 
mapped to physical registers, these physical registers 
are placed in the operands[0,1,2] fields. The fourth 
operand field, operands[3], is used to hold a store 
buffer dependency for loads and stores; this will be 
discussed later. The speculative RRT entries for both 
the destination physical register and any modified 
flags are then overwritten. Finally, the ROB is moved 
into the frontend state.

 External State

Since the rest of the simulator outside of the out of 
order core does not know about the RRTs and expects 
architectural registers to be in a standardized format, 
the system-wide ctx structure of class CoreState is 
used to house the architectural register file. The 
REG_flags and REG_rip entries of this structure are 
directly updated by the out of order core as 
instructions commit.

 Frontend Stages

To simulate various processor frontend pipeline depths, 
ROBs are placed in the frontend state for a 
user-selectable number of cycles. In the frontend() 
function, the cycles_left field in each ROB is 
decremented until it becomes zero. At this point, the 
uop is moved to the ready_to_dispatch state. This 
feature can be used to simulate various branch 
mispredict penalties by setting the FRONTEND_STAGES constant.

<sec:ClusterDispatchScheduleIssue>Scheduling, Dispatch and Issue

 <sec:Clustering>Clustering and Issue Queue Configuration

The PTLsim out of order model can simulate an 
arbitrarily complex set of functional units grouped 
into clusters. Clusters are specified by the Cluster 
class and are defined by the clusters[] array in 
ooohwdef.h. Each Cluster element defines the name of 
the cluster, which functional units belong to the 
cluster (fu_mask field) and the maximum number of uops 
that can be issued in that cluster each cycle 
(issue_width field)

The intercluster_latency_map matrix defines the 
forwarding latency, in cycles, between a given cluster 
and every other cluster. If 
intercluster_latency_map[A][B] is L cycles, this means 
that functional units in cluster B must wait L cycles 
after a uop U in cluster A completes before cluster B's 
functional units can issue a uop dependent on U's 
result. If the latency is zero between clusters A and 
B, producer and consumer uops in A and B can always be 
issued back to back in subsequent cycles. Hence, the 
diagonal of the forwarding latency matrix is always all zeros.

This clustering mechanism can be used to implement 
several features of modern microprocessors. First, 
traditional clustering is possible, in which it takes 
multiple additional cycles to forward results between 
different clusters (for instance, one or more integer 
clusters and a floating point unit). Second, several 
issue queues and corresponding issue width limits can 
be defined within a given virtual cluster, for instance 
to sort loads, stores and ALU operations into separate 
issue queues with different policies. This is done by 
specifying an inter-cluster latency of zero cycles 
between the relevant pseudo-clusters with separate 
issue queues. Both of these uses are required to 
accurately model most modern processors.

There is also an equivalent intercluster_bandwidth_map 
matrix to specify the maximum number of values that can 
be routed between any two clusters each cycle.

The IssueQueue template class is used to declare issue 
queues; each cluster has its own issue queue. The 
syntax IssueQueue<size> issueq_name; is used to declare 
an issue queue with a specific size. In the current 
implementation, the size can be from 1 to 64 slots. The 
macros foreach_issueq(), 
sched_get_all_issueq_free_slots() and 
issueq_operation_on_cluster_with_result() macros must 
be modified if the cluster and issue queue 
configuration is changed to reflect all available 
clusters; the modifications required should be obvious 
from the example code. These macros with switch 
statements are required instead of a simple array since 
the issue queues can be of different template types and sizes.

 Cluster Selection

The ReorderBufferEntry::select_cluster() function is 
responsible for routing a given uop into a specific 
cluster at the time it is dispatched; uops do not 
switch between clusters after this.

Various heuristics are employed to select which cluster 
a given uop should be routed to. In the reference 
implementation provided in ooocore.cpp, a weighted 
score is generated for each possible cluster by 
scanning through the uop's operands to determine which 
cluster they will be forwarded from. If a given 
operand's corresponding producer uop S is currently 
either dispatched to cluster C but waiting to execute 
or is still on the bypass network of cluster C, then 
cluster C's score is incremented. The final cluster is 
selected as the cluster with the highest score out of 
the set of clusters which the uop can actually issue on 
(e.g. a floating point uop cannot issue on a cluster 
with only integer units). This mechanism is designed to 
route each uop to the cluster in which the majority of 
its operands will become available at the earliest 
time; in practice it works quite well and variants of 
this technique are often used in real processors.

 <sec:Scheduling>Issue Queue Structure and Operation

PTLsim implements issue queues in the IssueQueue 
template class using the collapsing priority queue 
design used in most modern processors. 

As each uop is dispatched, it is placed at the end of 
the issue queue for its cluster and several associative 
arrays are updated to reflect which operands the uop is 
still waiting for. In the IssueQueue class, the 
insert() method takes the ROB index of the uop (its tag 
in issue queue terminology), the tags (ROB indices) of 
its operands, and a map of which of the operands are 
ready versus waiting. The ROB index is inserted into an 
associative array, and the ROB index tags of any 
waiting operands are inserted into corresponding slots 
in parallel arrays, one array per operand (in the 
current implementation, up to 4 operands are tracked). 
If an operand was ready at dispatch time, the slot for 
that operand in the corresponding array is marked as 
invalid since there is no need to wake it up later. 
Notice that the new slot is always at the end of the 
issue queue array; this is made possible by the 
collapsing mechanism described below.

The issue queue maintains two bitmaps to track the 
state of each slot in the queue. The valid bitmap 
indicates which slots are occupied by uops, while the 
issued bitmap indicates which of those uops have been 
issued. Together, these two bitmaps form the state 
machine described in Table [table:IssueQueueStateMachine].

<table:IssueQueueStateMachine>Issue Queue State Machine

+--------+---------+---------------------------------------------------+
| Valid  | Issued  | Meaning                                           |
+--------+---------+---------------------------------------------------+
+--------+---------+---------------------------------------------------+
|   0    |   0     | Unused slot                                       |
+--------+---------+---------------------------------------------------+
|   0    |   1     | (invalid)                                         |
+--------+---------+---------------------------------------------------+
|   1    |   0     | Dispatched but waiting for operands               |
+--------+---------+---------------------------------------------------+
|   1    |   1     | Issued to a functional unit but not yet completed |
+--------+---------+---------------------------------------------------+


After insert() is called, the slot is placed in the 
dispatched state. As each uop completes, its tag (ROB 
index) is broadcast using the broadcast() method to one 
or more issue queues accessible in that cycle. Because 
of clustering, some issue queues will receive the 
broadcast later than others; this is discussed below. 
Each slot in each of the four operand arrays is 
compared against the broadcast value. If the operand 
tag in that slot is valid and matches the broadcast 
tag, the slot (in one of the operand arrays only, not 
the entire issue queue) is invalidated to indicate it 
is ready and no longer waiting for further broadcasts.

Every cycle, the clock() method uses the valid and 
issued bitmaps together with the valid bitmaps of each 
of the operand arrays to compute which issue queue 
slots in the dispatched state are no longer waiting on 
any of their operands. This bitmap of ready slots is 
then latched into the allready bitmap.

The issue() method simply finds the index of the first 
set bit in the allready bitmap (this is the slot of the 
oldest ready uop in program order), marks the 
corresponding slot as issued, and returns the slot. The 
processor then selects a functional unit for the uop in 
that slot and executes it via the 
ReorderBufferEntry::issue() method. After the uop has 
completed execution (i.e. it cannot possibly be 
replayed), the release() method is called to remove the 
slot from the issue queue, freeing it up for incoming 
uops in the dispatch stage. The collapsing design of 
the issue queue means that the slot is not simply 
marked as invalid - all slots after it are physically 
shifted left by one, leaving a free slot at the end of 
the array. This design is relatively simple to 
implement in hardware and makes determining the oldest 
ready to issue uop very trivial.

Because of the collapsing mechanism, it is critical to 
note that the slot index returned by issue() will 
become invalid after the next call to the remove() 
method; hence, it should never be stored anywhere if a 
slot could be removed from the issue queue in the meantime.

If a uop issues but determines that it cannot actually 
complete at that time, it must be replayed. The 
replay() method clears the issued bit for the uop's 
issue queue slot, returning it to the dispatched state. 
The replay mechanism can optionally add additional 
dependencies such that the uop is only re-issued after 
those dependencies are resolved. This is important for 
loads and stores, which may need to add a dependency on 
a prior store queue entry after finding a matching 
address in the load or store queues. In rare cases, a 
replay may also be required when a uop is issued but no 
applicable functional units are left for it to execute 
on. The ReorderBufferEntry::replay() method is a 
wrapper around IssueQueue::replay() used to collect the 
operands the uop is still waiting for.

 Implementation

PTLsim uses a novel method of modeling the issue queue 
and other associative structures with small tags. 
Specifically, the FullyAssociativeArrayTags8bit 
template class declared in logic.h and used to build 
the issue queue makes use of the host processor's 
128-bit vector (SSE) instructions to do massively 
parallel associative matching, masking and bit scanning 
on up to 16 tags every clock cycle. This makes it 
substantially faster than simulators using the naive 
approach of scanning the issue queue entries linearly. 
Similar classes in logic.h support O(1) associative 
searches of both 8-bit and 16-bit tags; tags longer 
than this are generally more efficient if the generic 
FullyAssociativeArrayTags using standard integer 
comparisons is used instead.

As a result of this high performance design, each issue 
queue is limited to 64 entries and the tags to be 
matched must be between 0 and 255 to fit in 8 bits. The 
FullyAssociativeArrayTags16bit class can be used 
instead if longer tags are required, at the cost of 
reduced simulation performance.

 Other Designs

It's important to remember that the issue queue design 
described above is one possible implemention out of the 
many designs currently used in industry and research 
processors. For instance, in lieu of the collapsing 
design (used by the Pentium 4 and Power4/5/970), the 
AMD K8 uses a sequence number tag of the ROB and 
comparator logic to select the earliest ready 
instruction. Similarly, the Pentium 4 uses a set of bit 
vectors (a dependency matrix) instead of tag broadcasts 
to wake up instructions. These other approaches may be 
implemented by modifying the IssueQueue class as appropriate.

 <sec:Issue>Issue

The issue() top-level function issues one or more 
instructions in each cluster from each issue queue 
every cycle. This function consults the 
clusters[clusterid].issue_width field defined in 
ooohwdef.h to determine the maximum number of uops to 
issue from each cluster. The 
issueq_operation_on_cluster_with_result(cluster, 
iqslot, issue()) macro (Section [sec:Clustering]) is used to invoke the 
issue() method of the appropriate cluster to select the 
earliest ready issue queue slot, as described in 
Section [sec:Scheduling]. 

The ReorderBufferEntry::issue() method of the 
corresponding ROB entry is then called to actually 
execute the uop. This method first makes sure a 
functional unit is available within the cluster that's 
capable of executing the uop; if not, the uop is 
replayed and re-issued again on the next cycle. At this 
point, the uop's three operands (ra, rb, rc) are read 
from the physical register file. If any of the operands 
are invalid, the entire uop is marked as invalid with 
an EXCEPTION_Propagate result and is not further 
executed. Otherwise, the uop is executed by calling the 
synthesized execute function for the uop (see Section [sec:FetchStage]).

Loads and stores are handled specially by calling the 
issueload() or issuestore() method. Since loads and 
stores can encounter an mis-speculation (e.g. when a 
load is erroneously issued before an earlier store to 
the same addresses), the issueload() and issuestore() 
functions can return ISSUE_MISSPECULATED to force all 
uops in program order after the mis-speculated uop to 
be annulled and sent through the pipeline again. 
Similarly, if issueload() or issuestore() return 
ISSUE_NEEDS_REPLAY, issuing from that cluster is 
aborted since the uop has been replayed in accordance 
with Section [sec:Scheduling]. It is important to note that loads which 
miss the cache are considered to complete successfully 
and do not require a replay; their physical register is 
simply marked as waiting until the load arrives. In 
both the mis-speculation and replay cases, no further 
uops from the cluster's issue queue are dispatched 
until the next cycle.

Branches are handled similar to integer and floating 
point operations, except that they may cause a 
mis-speculation in the event of a branch misprediction; 
this is discussed below.

If the uop caused an exception, we force it directly to 
the commit stage and not through writeback; this keeps 
dependencies waiting until they can be properly 
annulled by the speculation recovery logic. The commit 
stage will detect the exception and take appropriate 
action. If the exceptional uop was speculatively 
executed beyond a branch, it will never reach commit 
anyway since the bogus branch would have to commit 
before the exception would even become visible.

NOTE: In PTLsim, all issued uops put their result in 
the uop's assigned physical register at the time of 
issue, even though the data technically does not appear 
there until writeback (i.e. the physical register 
enters the written state). This is done to simplify the 
simulator implementation; it is assumed that any data "read"
 from physical registers before writeback is in fact 
being read from the bypass network instead.

<sec:SpeculationAndRecovery>Speculation and Recovery

PTLsim allows speculative execution of two classes of 
uops which may require all uops in program order after 
and optionally including the mis-speculated uop to be 
annulled before they are committed to the architectural state.

 Misspeculation Cases

 Branch Mispredictions

Branch mispredictions form the bulk of all 
mis-speculated operations. Whenever the actual RIP 
returned by a branch uop differs from the riptaken 
field of the uop, the branch has been mispredicted. 
This means all uops after (but not including) the 
branch must be annulled and removed from all processor 
structures. The fetch queue (Section [sec:FetchStage]) is then reset 
and fetching is redirected to the correct branch 
target. However, all uops in program order before the 
branch are still correct and may continue executing.

Note that we do not just reissue the branch: this would 
be pointless, as we already know the correct RIP since 
the branch uop itself has already executed once. 
Instead, we let it writeback and commit as if it were 
predicted correctly.

 Unaligned Loads and Stores and Aliased Stores

Loads and stores may also require the annulment of all 
uops following (and this time including) the load or 
store in program order. The conditions under which this 
process can occur are described in Sections [sec:IssuingLoads] and [sub:AliasCheck].

 <sec:SpeculationRecovery>Recovery

In PTLsim, the ReorderBufferEntry::annul() method 
removes any and all ROBs that entered the pipeline 
after and optionally including the misspeculated uop 
(depending on the keep_misspec_uop argument). Because 
this method moves all affected ROBs to the free state, 
they are instantly taken out of consideration for 
future pipeline stages and will be dropped on the next cycle.

We must be extremely careful to annul all uops in an 
x86 macro-op; otherwise half the x86 instruction could 
be executed twice once refetched. Therefore, if the 
first uop to annul is not also the first uop in the x86 
macro-op, we may have to scan backwards in the ROB 
until we find the first uop of the macro-op. In this 
way, we ensure that we can annul the entire macro-op. 
All uops comprising the macro-op are guaranteed to 
still be in the ROB since none of the uops can commit 
until the entire macro-op can commit. Note that this 
does not apply if the final uop in the macro-op is a 
branch and that branch uop itself is being retained as 
occurs with mispredicted branches.

The first uop to annul is determined in the annul() 
method by scanning backwards in time from the excepting 
uop until a uop with its SOM (start of macro-op) bit is 
set, as described in Section [sec:UopIntro]. This SOM uop represents 
the boundary between x86 instructions, and is where we 
start annulment. The end of the range of uops to annul 
is at the tail of the reorder buffer.

We have to reconstruct the speculative RRT as it 
existed just before the first uop to be annulled was 
renamed. This is done by calling the pseudocommit() 
method of each annulled uop to implement the "fast flush 
with pseudo-commit" algorithm as follows. First, we 
overwrite the speculative RRT with the committed RRT. 
We then simulate the commitment of all non-speculative 
ROBs up to the first uop to be annulled by updating the 
speculative RRT as if it were the commit RRT. This 
brings the speculative RRT to the same state as if all 
in flight nonspeculative operations before the first 
uop to be annulled had actually committed. Fetching is 
then resumed at the correct RIP, where new uops are 
renamed using the recovered speculative RRT.

Other methods of RRT reconstruction (like backwards 
walk with saved checkpoint values) are difficult to 
impossible because of the requirement that flag rename 
tables be restored even if some of the required 
physical registers with attached flags have since been 
freed. Technically RRT checkpointing could be used but 
due to the load/store replay mechanism in use, this 
would require a checkpoint at every load and store as 
well as branches. Hence, the forward walk method seems 
to offer the best performance in practice and is quite 
simple. The Pentium 4 is believed to use a similar 
method of recovering from some types of mis-speculations.

After reconstructing the RRT, for each ROB to annul, we 
broadcast the ROB index to the appropriate cluster's 
issue queue, allowing the issue queue to purge the slot 
of the ROB being annulled. Finally, for each annulled 
uop, we free any resources allocated to it (i.e., the 
ROB itself, the destination physical register, the 
load/store queue entry (if any) and so on. Any updates 
to the branch predictor and return address stack made 
during the speculative execution of branches are also 
rolled back.

Finally, the fetch unit is restarted at the correct RIP 
and uops enter the pipeline and are renamed according 
to the recovered rename tables and allocated resource maps.

Load Issue

 <sec:IssuingLoads>Issuing Loads

The ReorderBufferEntry::issueload() function is 
responsible for issuing all load uops. The issueload() 
method starts by computing the effective physical 
address of the value to load. In the released version 
of PTLsim, physical and virtual addresses are the same. 
If the load is one of the special unaligned fixup forms 
(ld.lo, ld.hi) described in Section [sub:UnalignedLoadsAndStores], the address is 
re-aligned according to the type of instruction. At 
this point, the check_access_and_alignment() function 
is called to resolve any immediately obvious exceptions 
such as page faults or alignment problems (for normal loads).

If a given load or store accesses an unaligned address 
but is not one of the special ld.lo/ld.hi/st.lo/st.hi 
uops described in Section [sub:UnalignedLoadsAndStores], the processor responds by 
annulling all uops after and including the problem 
load; it then refetches instructions starting at the 
RIP address of the load itself. When the load 
instruction is refetched, it is transformed into a pair 
of ld.lo/ld.hi or st.lo/st.hi uops in accordance with 
Section [sub:UnalignedLoadsAndStores]. This refetch approach is required rather than 
a simple replay operation since a replay would require 
allocating two entries in the issue queue and 
potentially two ROBs, which is not possible with the 
PTLsim design once uops have been renamed.

Technically, PTLsim splits loads into into low and high 
fixup uops by simply retranslating the basic block 
starting at the problem load's RIP. In real processors, 
the frontend pipeline generally has logic for 
predicting which loads and stores will be unaligned and 
will dynamically split them into aligned parts. 
Functionally these two approaches are the same, since 
in effect PTLsim predicts which loads need to be split 
by simply retaining the retranslated basic block with 
the split loads.

If a load from the effective address would cause a page 
fault at this point, the load is aborted and execution 
returns to the ReorderBufferEntry::issue() method, 
causing the result to be marked with an exception 
(EXCEPTION_PageFaultOnRead).

One x86-specific complication arises at this point. If 
a load (or store) uop is the high part (ld.hi or st.hi) 
of an unaligned load or store pair, but the actual user 
address did not overlap any of the high 64 bits 
accessed by the ld.hi or st.hi uop, the load should be 
completely ignored, even if the high part overlapped 
onto an invalid page. This is because it is perfectly 
legal to do an unaligned load or store at the very end 
of a page such that the next 64 bit chunk is not mapped 
to a valid page; the x86 architecture mandates that the 
load or store execute correctly as far as the user 
program is concerned.

 Store Queue Check and Store Dependencies

After doing these exception checks, the load/store 
queue (LSQ) is scanned backwards in time from the 
current load's entry to the LSQ's head. If a given LSQ 
entry corresponds to a store, the store's address has 
been resolved and the memory range needed by the load 
overlaps the memory range touched by the store, the 
load effectively has a dependency on the earlier store 
that must be resolved before the load can issue. The 
meaning of "overlapping memory range" is defined more 
specifically in Section [sec:StoreMerging].

In some cases, the addresses of one or more prior 
stores that a load may depend on may not have been 
resolved by the time the load issues. Some processors 
will stall the load uop until all prior store addresses 
are known, but this can decrease performance by 
incorrectly preventing independent loads from starting 
as soon as their address is available. For this reason, 
the PTLsim processor model aggressively issues loads as 
soon as possible unless the load is predicted to 
frequently alias another store currently in the 
pipeline. This load/store aliasing prediction technique 
is described in Section [sub:AliasCheck].

In either of the cases above, in which an overlapping 
store is identified by address but that store's data is 
not yet available for forwarding to the load, or where 
a prior store's address has not been resolved but is 
predicted to overlap the load, the load effectively has 
a data flow dependency on the earlier store. This 
dependency is represented by setting the load's fourth 
rs operand (operands[RS] in the ReorderBufferEntry) to 
the store the load is waiting on. After adding this 
dependency, the replay() method is used to force the 
load back to the dispatched state, where it waits until 
the prior store is resolved. After the load is 
re-issued for a second time, the store queue is scanned 
again to make sure no intervening stores arrived in the 
meantime. If a different match is found this time, the 
load is replayed a third time. In practice, loads are 
rarely replayed more than once.

 Data Extraction

Once the prior store a load depends on (if any) is 
ready and all the exception checks above have passed, 
it is time to actually obtain the load's data. This 
process can be complicated since some bytes in the 
region accessed by the load could come from the data 
cache while other bytes may be forwarded from a prior 
store. If one or more bytes need to be obtained from 
the data cache, the L1 cache is probed (via the 
probe_cache_and_sfr() function) to see if the required 
line is present. If so, and the combination of the 
forwarded store (if any) and the L1 line fills in all 
bytes required by the load, the final data can be extracted.

To extract the data, the load unit creates a 64-bit 
temporary buffer by overlaying the bytes touched by the 
prior store (if any) on top of the bytes obtained from 
the cache. The correct word is then extracted and sign 
extended (if required) from this buffer to form the 
result of the load. Unaligned loads (described in 
Section [sub:UnalignedLoadsAndStores]) are somewhat more complex in that both the 
low and high 64 bit chunks from the ld.lo and ld.hi 
uops, respectively, are placed into a 128-bit buffer 
from which the final result is extracted.

NOTE: For simulation purposes only, the data to load is 
immediately accessed and recorded by issueload() 
regardless of whether or not there is a cache miss. 
This makes the loaded data significantly easier to 
track. In a real processor, the data extraction process 
obviously only happens after the missing line actually 
arrives, however our implementation in no way affects 
performance.

 <sec:CacheMissHandling>Cache Miss Handling

If no combination of the prior store's forwarded bytes 
and data present in the L1 cache can fulfill a load, 
this is miss and lower cache levels must be accessed. 
This process is described in Sections [sec:InitiatingCacheMiss] and [sec:FillingCacheMiss]. As far as 
the core is concerned, the load is completed at this 
point even if the data has not yet arrived. The issue 
queue entry for the load can be released since the load 
is now officially in progress and cannot be replayed. 
Once the loaded data has arrived, the cache subsystem 
calls the ReorderBuffer::loadwakeup(), which marks both 
the physical register and LSQ entry of the load as 
ready, and places the load's ROB into the completed 
state. This allows the processor to wake up dependents 
of the load on the next cycle.

Stores

 <sec:StoreMerging>Store to Store Forwarding and Merging

In the PTLsim out of order model, a given store may 
merge its data with that of a previous store in program 
order. This ensures that loads which may need to 
forward data from a store always reference exactly one 
store queue entry, rather than having to merge data 
from multiple smaller prior stores to cover the entire 
byte range being loaded. In this model, physical memory 
is divided up into 8 byte (64 bit) chunks. As each 
store issues, it scans the store queue backwards in 
program order to find the most recent prior store to 
the same 8 byte aligned physical address. If there is a 
match, the current store depends on the matching prior 
store, and cannot complete and forward its data to 
other consuming loads and stores until the prior store 
in question also completes. This ensures that the 
current store's data can be composited on top of the 
older store's data to form a single up to date 8-byte 
chunk. As described in Section [sec:LoadStoreQueueEntry], each store queue entry 
contains a byte mask to indicate which of the 8 bytes 
in each chunk are currently modified by stores in 
flight versus those bytes which must come from the data cache.

Technically there are more efficient approaches, such 
as allowing stores to issue in any order so long as 
they do not overlap on the basis of individual bytes. 
However, no modern processor allows such arbitrary 
forwarding since the circuit complexity involved with 
scanning the store queue for partial address matches 
would be prohibitive and slow. Instead, most processors 
only support store to load forwarding when a single 
larger prior store covers the entire byte range 
accessed by a smaller or same sized load; all other 
combinations stall the load until the overlapping prior 
stores commit to the data cache. 

The store inheritance scheme used by PTLsim (described 
first) is an improvement to the more common "stall on 
size mismatch" scheme above, but may incur more store 
dependency replays (since stores now depend on other 
stores when they target the same 8-byte chunk) compared 
to a stall on size mismatch scheme. As a case study, 
the Pentium 4 processor (Prescott core) implements a 
combination of these approaches.

 <sec:SplitPhaseStores>Split Phase Stores

The ReorderBufferEntry::issuestore() function is 
responsible for issuing all store uops. Stores are 
unusual in that they can issue even if their rc operand 
(the value to store) is not ready at the same time as 
the ra and rb operands forming the effective address. 
This property is useful since it allows a store to 
establish an entry in the store queue as soon as the 
effective address can be generated, even if the data to 
store is not ready. By establishing addresses in the 
store queue as soon as possible, we can avoid 
performance losses associated with the unnecessary 
replay of loads that may depend on a store whose 
address is unavailable at the time the load issues. In 
effect, this means that each store uop may actually 
issue twice.

In the first phase issue, which occurs as soon as the 
ra and rb operands become ready, the store uop computes 
its effective physical address, checks that address for 
all exceptions (such as alignment problems and page 
faults) and writes the address into the corresponding 
LoadStoreQueueEntry structure before setting its the 
addrvalid bit as described in Section [sec:LoadStoreQueueEntry]. If an exception 
is detected at this point, the invalid bit in the store 
queue entry is set and the destination physical 
register's FLAG_inv flag is set so any attempt to 
commit the store will fail.

 <sub:AliasCheck>Load Queue Search (Alias Check)

The load queue is then searched to find any loads after 
the current store in program order which have already 
issued but have done so without forwarding data from 
the current store. These loads erroneously issued 
before the current store (now known to overlap the 
load's address) was able to forward the correct data to 
the offending load(s). This situation is known as 
aliasing, and is effectively a mis-speculation 
requiring the annulment of all instructions after and 
including the store. The annulment is performed in 
accordance with Section [sec:MisspeculationAnnulment], similar to how uops after a 
mispredicted branch would be annulled. Unlike branches, 
technically we do not need to re-fetch instructions 
starting at the mis-speculated store, but in the 
current implementation of the simulator this is done 
for simplicity reasons.

Since the annulment process required to correct 
aliasing violations is expensive, it is desirable to 
predict in advance which loads and stores are likely to 
alias each other such that loads predicted to alias are 
never issued when prior stores in the store queue still 
have unknown addresses. This works because in most out 
of order processors, statistically speaking, very few 
loads alias stores compared to normal loads from the 
cache. When an aliasing mis-speculation occurs, an 
entry is added to a small fully associative structure 
(typically \le16 entries) called the Load Store Alias 
Predictor (LSAP). This structure is indexed by a 
portion of the address of the load instruction that 
aliased. This allows the load unit to avoid issuing any 
load uop that matches any address in the LSAP if any 
prior store addresses are still unresolved; if this is 
the case, a dependency is created on the first 
unresolved store such that the load is replayed (and 
the load and store queues are again scanned) once that 
store resolves. Similar methods of aliasing prediction 
are used by the Pentium 4 (Prescott core only) and 
Alpha 21264.

 Store Queue Search (Merge Check)

At this point the store queue is searched for prior 
stores to the same 8-byte block as described above in 
Section [sec:StoreMerging]; if the store depends on a prior store, the 
scheduler structures are updated to add an additional 
dependency (in operands[RS]) on this prior store before 
the store is replayed in accordance with Section [sec:Scheduling] to 
wait for the prior store to complete. If no prior store 
is found, or the prior store is ready, the current 
store is marked as a second phase store by setting the 
load_store_second_phase flag in its ROB entry. Finally, 
the store is replayed in accordance with Section [sec:Scheduling].

In the second phase of store uop scheduling, the store 
uop is only re-issued when all four operands (ra + rb 
address, rc data and rs source store queue entry) are 
valid. The second phase repeats the scan of the load 
and store queues described above to catch any loads and 
stores that may have issued between the first and 
second phase issues; the store is replayed a third time 
if necessary. Otherwise, the rc operand data is merged 
with the data from the prior store (if any) store queue 
entry, and the combined data and bytemask is written 
into the current store's store queue entry. Finally, 
the entry's dataready bit is set to make the entry 
available for forwarding to other waiting loads and stores.

The first and second phases may be combined into a 
single issue without replay if both the address and 
data operands of the store are all ready at the same 
time and the prior store (if any) the current store 
inherits from has already successfully issued.

Forwarding, Wakeup and Writeback

 Forwarding and the Clustered Bypass Network

Immediately after each uop is issued and the 
ReorderBufferEntry::issue() method actually generates 
its result, the cycles_left field of the ROB is set to 
the expected latency of the uop (e.g. between 1 and 5 
cycles). The uop is then moved to the issued state and 
placed on the rob_issued_list. Every cycle, the 
complete() method iterates through each ROB in issued 
state and decrements its cycles_left field. If 
cycles_left becomes zero, the corresponding uop has 
completed execution. The ROB is moved to the completed 
state (on rob_completed_list) and its physical register 
or store queue entry is moved to the ready state so 
newly dispatched uops do not try to wait for it.

The transfer() function is also called every cycle. 
This function examines the list of ROBs in the 
completed state and is responsible for broadcasting the 
completed ROB's tag (ROB index) to the issue queues. 
Because of clustering (Section [sec:Clustering]), some issue queues 
will receive the broadcast later than others. 
Specifically, the ROB's forward_cycle field determines 
which issue queues and remote clusters are visible 
forward_cycle cycles after the uop completed. The 
forward() method, called by transfer() for each uop in 
the completed state, indexes into a lookup table 
forward_at_cycle_lut[cluster][forward_cycle] to get a 
bitmap of which remote clusters are accessible 
forward_cycle cycles after he uop completed, relative 
to the original cluster.the uop issued in. The 
IssueQueue::broadcast() method (Section [sec:Scheduling]) is then 
called for each applicable cluster to wake up any 
operands of uops in that cluster waiting on the newly 
completed uop.

The MAX_FORWARDING_LATENCY constant (in ooohwdef.h) 
specifies the maximum number of cycles between any two 
clusters. After the ROB has progressed through 
MAX_FORWARDING_LATENCY cycles in the completed state, 
it is moved to the ready-to-writeback state, 
effectively meaning the result has arrived at the 
physical register file and is eligible for writeback in 
the next cycle.

 Writeback

Every cycle, the writeback() function scans the list of 
ROBs in the ready-to-writeback state and selects at 
most WRITEBACK_WIDTH results to write to the physical 
register file. The forward() method is first called one 
final time to catch the corner case in which a 
dependent uop was dispatched while producer uop was 
waiting in the ready-to-writeback state.

As mentioned in Section [sec:Issue], for simulation purposes only, 
each uop puts its result directly into its assigned 
physical register at the time of issue, even though the 
data technically does not appear there until writeback. 
This is done to simplify the simulator implementation; 
it is assumed that any data "read" from physical 
registers before writeback is in fact being read from 
the bypass network instead. Therefore, no actual data 
movement occurs in the writeback() function; its sole 
purpose is to place the uop's physical register into 
the written state (via the 
PhysicalRegister::writeback() method) and to move the 
ROB into its terminal state, ready-to-commit.

<sec:CommitStage>Commitment

 Introduction

The commit stage examines uops from the head of the 
ROB, blocks until all uops comprising a given x86 
instruction are ready to commit, commits the results of 
those uops to the architectural state and finally frees 
the resources associated with each uop.

 Atomicity of x86 instructions

The x86 architecture specifies atomic execution for all 
distinct x86 instructions. This means that since each 
x86 instruction may be comprised of multiple uops; none 
of these uops may commit until all uops in the 
instruction are ready to commit. In PTLsim, this is 
accomplished by checking if the uop at the head of the 
ROB (next to commit) has its SOM (start of macro-op) 
bit set. If so, the ROB is scanned forwards from the 
SOM uop to the next uop in program order with its EOM 
(end of macro-op) bit set. If all uops in this range 
are ready to commit and exception-free, the SOM uop is 
allowed to commit, effectively unlocking the ROB head 
pointer until the next uop with a SOM bit set is 
encountered. However, any exception in any uop 
comprising the x86 instruction at the head of the ROB 
causes the pipeline to be flushed and an exception to 
be taken. Similarly, external interrupts are only 
acknowledged at the boundary between x86 instructions 
(i.e. after the EOM uop of each instruction).

 Commitment

As each uop commits, it may update several components 
of the architectural state. 

Integer ALU and floating point uops obviously update 
their destination architectural register (rd). In 
PTLsim, this is done by simply updating the committed 
register rename table (commitrrt) rather than actually 
copying register values. However, the old physical 
register mapped to architectural register rd will 
normally become inaccessible after the Commit RRT 
mapping for rd is overwritten with the committing uop's 
physical register index. The old physical register 
previously mapped to rd can then be freed. Technically 
physical registers allocated to intermediate uops (such 
as those used to hold temporary values) can be 
immediately freed without updating any Commit RRT 
entries, but for consistency we do not do this.

In PTLsim, a physical register is freed by moving it to 
the physreg_free state. Unfortunately for various 
reasons related to long pipelines and the renaming of 
x86 flags, register reclamation is not so simple, but 
this will be discussed below in Section [sub:PhysicalRegisterRecyclingComplications].

Some uops may also commit to a subset of the x86 flags, 
as specified in the uop encoding. For these uops, in 
theory no rename tables need updating, since the flags 
can be directly masked into the REG_flags architectural 
pseudo-register. Should the pipeline be flushed, the 
rename table entries for the ZAPS, CF, OF flag sets 
will all be reset to point to the REG_flags 
pseudo-register anyway. However, for the speculation 
recovery scheme described in Section [sec:SpeculationRecovery], the REG_zf, 
REG_cf, and REG_of commit RRT entries are updated as 
well to match the updates done to the speculative RRT.

Branches and jumps update the REG_rip pseudo 
architectural register, while all other uops simply 
increment REG_rip by the number of bytes in the x86 
instruction being committed. The number of bytes (1-15) 
is stored in a 4-bit field of the first uop in each x86 
instruction (i.e. the uop with its SOM bit set).

Stores commit to the architectural state by writing 
directly to the data cache. Remember that a series of 
stores into a given 64-bit chunk of memory are merged 
within the store queue to the store uop's corresponding 
STQ entry as the store uop issues, so the commit unit 
always writes 64 bits to the cache at a time. The byte 
mask associated with the STQ entry of the store uop is 
used to only update the modified bytes in each chunk of 
memory in program order.

 <sub:PhysicalRegisterRecyclingComplications>Physical Register Recycling Complications

 Problem Scenarios

In some processor designs, it is not always possible to 
immediately free the physical register mapped to a 
given architectural register when that old 
architectural register mapping is overwritten during 
commit as described above. Out of order x86 processors 
must maintain three separate rename table entries for 
the ZAPS, CF, OF flags in addition to the register 
rename table entry, any or all of which may be updated 
when uops rename and retire, depending on the uop's 
flag renaming semantics (see Section [sub:FlagsManagement]), For this 
reason, even though a given physical register value may 
become inaccessible and hence dead at commit time, the 
flags associated with that physical register are 
frequently still referenced within the pipeline, so the 
physical register itself must remain allocated.

Consider the following specific example, with uops 
listed in program order:

 sub rax = rax,rbx
  Assign RRT[rax] = phys reg r0
  Assign RRT[flags] = r0 (since SUB all updates flags)

 mov rax = rcx
  Assign RRT[rax] = phys reg r1
  No flags renamed: MOV never updates flags, so 
  RRT[flags] is still r0.

 br.e target
  Depends on flags attached to r0, even though actual 
  architectural register (rax) for r0 has already been 
  overwritten in the commit RRT by the MOV's commit. We 
  cannot free r0 since the BR uop might not have issued yet.

This situation only happens with instruction sets like 
x86 (and SPARC or even PowerPC to some extent) which 
support writing flags (particularly multiple 
independent flags) and data in a single instruction.

 Reference Counting

For these reasons, we need to prevent U2's register 
from being freed if it is still referenced by anything 
still in the pipeline; the normal reorder buffer 
mechanism cannot always handle this situation in a very 
long pipeline.

One solution (the one used by PTLsim) is to give each 
physical register a reference counter. Physical 
registers can be referenced from three structures: as 
operands to ROBs, from the speculative RRT, and from 
the committed RRT. As each uop operand is renamed, the 
counter for the corresponding physical register is 
incremented by calling the PhysicalRegister::addref() 
method. As each uop commits, the counter for each of 
its operands is decremented via the 
PhysicalRegister::unref() method. Similarly, unref() 
and addref() are used whenever an entry in the 
speculative RRT or commit RRT is updated. During 
mis-speculation recovery (see Section [sec:SpeculationRecovery]), unref() is 
also used to unlock the operands of uops slated for 
annulment. Finally, unref() and addref() are used when 
loads and stores need to add a new dependency on a 
waiting store queue entry (see Sections [sec:IssuingLoads] and [sec:SplitPhaseStores]).

As we update the committed RRT during the commit stage, 
the old register R mapped to the destination 
architectural register A of the uop being committed is 
examined. The register R is only moved to the free 
state iff its reference counter is zero. Otherwise, it 
is moved to the pendingfree state. The hardware 
examines the counters of pendingfree physical registers 
every cycle and moves physical registers to the free 
state only when their counters become zero and they are 
in the pendingfree state.

 Hardware Implementation

The hardware implementation of this scheme is 
straightforward and low complexity. The counters can 
have a very small number of bits since it is very 
unlikely a given physical register would be referenced 
by all 100+ uops in the ROB; 3 bits should be enough to 
handle the typical maximum of < 8 uops sharing a given 
operand. Counter overflows can simply stall renaming or 
flush the pipeline since they are so rare.

The counter table can be updated in bulk each cycle by 
adding/subtracting the appropriate sum or just adding 
zero if the corresponding register wasn't used. Since 
there are several stages between renaming and commit, 
the same counter is never both incremented and 
decremented in the same cycle, so race conditions are 
not an issue. 

In real processors, the Pentium 4 uses a scheme similar 
to this one but uses bit vectors instead. For smaller 
physical register files, this may be a better solution. 
Each physical register has a bit vector with one bit 
per ROB entry. If a given physical register P is still 
used by ROB entry E in the pipeline, P's bit vector bit 
R is set. Register P cannot be freed until all bits in 
its vector are zero.

<sec:CacheHierarchy>Cache Hierarchy

The PTLsim cache hierarchy model is highly flexible and 
can be used to model a wide variety of contemporary 
cache structures. The cache subsystem (defined in 
dcacheint.h and implemented by dcache.cpp) by default 
consists of four levels:

 L1 data cache is directly probed by all loads and stores

 L1 instruction cache services all instruction fetches

 L2 cache is shared between data and instructions, with 
  data paths to both L1 caches

 L3 cache is also shared and is optionally present

 Main memory is considered infinite in size but still 
  has configurable characteristics

These cache levels are listed in order from highest 
level (closer to the core) to lowest level (far away). 
The cache hierarchy is assumed to be inclusive, i.e. 
any data in higher levels is assumed to always be 
present in lower levels. Additionally, the cache levels 
are generally write-through, meaning that every store 
updates all cache levels, rather than waiting for a 
dirty line to be evicted. PTLsim supports a 48-bit 
virtual address space and 40-bit physical addresses in 
accordance with the x86-64 minimum requirements.

 General Configurable Parameters

All caches support configuration of:

 Line size in bytes. Any power of two size is 
  acceptable, however the line size of a lower cache 
  level must be the same or larger than any line size 
  of a higher level cache. For example, it is illegal 
  to have 128 byte L1 lines with 64 byte L2 lines.

 Set count may be any power of two number. The total 
  cache size in bytes is of course (line size) \times  (set count)\times 
   (way count)

 Way count (associativity) may be any number from 1 
  (direct mapped) up to the set count (fully 
  associative). Note that simulation performance (and 
  clock speed in a real processor) will suffer if the 
  associativity is too great, particularly for L1 caches.

 Latency in cycles from a load request to the arrival 
  of the data.

In dcacheint.h, the two base classes CacheLine and 
CacheLineWithValidMask are interchangeable, depending 
on the model being used. The CacheLine class is a 
standard cache line with no actual data (since the 
bytes in each line are simply held in memory for 
simulation purposes). 

The CacheLineWithValidMask class adds a bitmask 
specifying which bytes within the cache line contain 
valid data and which are unknown. This is useful for 
implementing "no stall on store" semantics, in which 
stores simply allocate a new way in the appropriate set 
but only set the valid bits for those bytes actually 
modified by the store. The rest of the cache line not 
touched by the store can be brought in later without 
stalling the processor (unless a load tries to access 
them); this is PTLsim's default model. Additionally, 
this technique may be used to implement sectored cache 
lines, in which the line fill bus is smaller than the 
cache line size. This means that groups of bytes within 
the line may be filled over subsequent cycles rather 
than all at once.

The AssociativeArray template class in logic.h forms 
the basis of all caches in PTLsim. To construct a cache 
in which specific lines can be locked into place, the 
LockableAssociativeArray template class may be used 
instead. Finally, the CommitRollbackCache template 
class is useful for creating versions of PTLsim with 
cache level commit/rollback support for out of order 
commit, fault recovery and advanced speculation techniques.

The various caches are defined in dcacheint.h by 
specializations of these template classes. The classes 
are L1Cache, L1ICache, L2Cache and L3Cache.

 <sec:InitiatingCacheMiss>Initiating a Cache Miss

As described in Section [sec:IssuingLoads], in the out of order core 
model, the issueload() function determines if some 
combination of a prior store's forwarded bytes (if any) 
and data present in the L1 cache can fulfill a load. If 
not, this is a miss and lower cache levels must be 
accessed. In this case, a LoadStoreInfo structure 
(defined in dcache.h) is prepared with various metadata 
about the load, including which ROB entry and physical 
register to wake up when the load arrives, its size, 
alignment, sign extension properties, prefetch 
properties and so on. The issueload_slowpath() function 
(defined in dcache.cpp) is then called with this 
information, the physical address to load and any data 
inherited from a prior store still in the pipeline. The 
issueload_slowpath() function moves the load request 
out of the core pipeline and into the cache hierarchy. 

The Load Fill Request Queue (LFRQ) is a structure used 
to hold information about any outstanding loads that 
have missed any cache level. The LFRQ allows a 
configurable number of loads to be outstanding at any 
time and provides a central control point between cache 
lines arriving from the L2 cache or lower levels and 
the movement of the requested load data into the 
processor core to dependent instructions. The 
LoadFillReq structure, prepared by 
issueload_slowpath(), contains all the data needed to 
return a filled load to the core: the physical address 
of the load, the data and bytemask already known so far 
(e.g. forwarded from a prior store) and the 
LoadStoreInfo metadata described above.

The Miss Buffer (MB) tracks all outstanding cache 
lines, rather than individual loads. Each MB slot uses 
a bitmap to track one or more LFRQ entries that need to 
be awakened when the missing cache line arrives. After 
adding the newly created LoadFillReq entry to the LFRQ, 
the MissBuffer::initiate_miss() method uses the missing 
line's physical address to allocate a new slot in the 
miss buffer array (or simply uses an existing slot if a 
miss was already in progress on a given line). In any 
case, the MB's wakeup bitmap is updated to reflect the 
new LFRQ entry referring to that line. Each MB entry 
contains a cycles field, indicating the number of 
cycles remaining for that miss buffer before it can be 
moved up the cache hierarchy until it reaches the core. 
Each entry also contains two bits (icache and dcache) 
indicating which L1 caches to which the line should 
eventually be delivered; this is required because a 
single L2 line (and corresponding miss buffer) may be 
referenced by both the L1 data and instruction caches. 

In initiate_miss(), the L2 and L3 caches are probed to 
see if they contain the required line. If the L2 has 
the line, the miss buffer is placed into the 
STATE_DELIVER_TO_L1 state, indicating that the line is 
now in progress to the L1 cache. Similarly, an L2 miss 
but L3 hit results in the STATE_DELIVER_TO_L2 state, 
and a miss all the way to main memory results in 
STATE_DELIVER_TO_L3.

In the very unlikely event that either the LFRQ slot or 
miss buffer are full, an exception is returned to out 
of order core, which typically replays the affected 
load until space in these structures becomes available. 
For prefetch requests, only a miss buffer is allocated; 
no LFRQ slot is needed.

 <sec:FillingCacheMiss>Filling a Cache Miss

The MissBuffer::clock() method implements all 
synchronous state transitions. For each active miss 
buffer, the cycles counter is decremented, and if it 
becomes zero, the MB's current state is examined. If a 
given miss buffer was in the STATE_DELIVER_TO_L3 state 
(i.e. in progress from main memory) and the cycle 
counter just became zero, a line in the L3 cache can 
now line is validated with the incoming data (this may 
involve evicting another line in the same set to make 
room). The MB is then moved to the next state up the 
cache hierarchy (i.e. STATE_DELIVER_TO_L2 in this 
example) and its cycles field is updated with the 
latency of the cache level it is now leaving (e.g. 
L3_LATENCY in this example). 

This process continues with successive levels until the 
MB is in the STATE_DELIVER_TO_L1 state and its cycles 
field has been decremented to zero. If the MB's dcache 
bit is set, the L1 corresponding line is validated and 
the lfrq.wakeup() method is called to invoke a new 
state machine to wake up any loads waiting on the 
recently filled line (as known from the MB's lfrqmap 
bitmap). If the MB's icache bit was set, the line is 
validated in the L1 instruction cache, and the 
icache_wakeup_func() callback is used to notify the out 
of order core's fetch stage that it may probe the cache 
for the missing line again. In any case, the miss 
buffer is then returned to the unused state.

Each LFRQ slot can be in one of three states: free, 
waiting and ready. LFRQ slots remain in the waiting 
state as long as they are referenced by a miss buffer; 
once the lfrq.wakeup() method is called, all slots 
affiliated with that miss buffer are moved to the ready 
state. The LoadFillRequestQueue::clock() method finds 
up to MAX_WAKEUPS_PER_CYCLE LFRQ slots in the ready 
state and wakes them up by calling the 
load_filled_callback() callback with the saved 
LoadStoreInfo metadata. The out of order core handles 
this callback as described in Section [sec:CacheMissHandling].

For simulation purposes only, the value to be loaded is 
immediately recorded as soon as the load issues, 
independent of the cache hit or miss status. In real 
hardware, the LFRQ entry data would be used to extract 
the correct bytes from the newly arrived line and 
perform sign extension and alignment. If the original 
load required bytes from a mixture of its source store 
buffer and the data cache, the SFR data and mask fields 
in the LFRQ entry would be used to perform this merging 
operation. The data would then be written into the 
physical register specified by the LoadStoreInfo 
metadata and that register would be marked as ready 
before sending a signal to the issue queues to wake up 
dependent operations.

In some cases, the out of order core may need to annul 
speculatively executed loads. The cache subsystem is 
notified of this through the annul_lfrq_slot() function 
called by the core. This function clears the specified 
LFRQ slot in each miss buffer's lfrqmap entry (since 
that slot should no longer be awakened now that it has 
been annulled), and frees the LFRQ entry itself.

 <sec:TranslationLookasideBuffers>Translation Lookaside Buffers

The cache subsystem includes separate translation 
lookaside buffers (TLBs) for data (DTLB) and 
instructions (ITLB) to map virtual to physical 
addresses. Note that virtual addresses are always used 
within the simulated virtual address space; hence the 
TLBs are solely for accurately measuring performance. 
To achieve fast simulation, the TLBs are not actually 
associatively scanned on each access; instead, the 
TranslationLookasideBuffer::check() method simply 
checks one of the simulator's Shadow Page Access Tables 
(SPATs) as described in Section [sec:AddressSpaceSimulation]. For DTLB accesses, 
the dtlbmap SPAT is used, while ITLB accesses use the 
itlbmap SPAT. If a bit in the appropriate SPAT is set, 
that page is considered mapped within the TLB. When 
entries are added to or evicted from the TLBs, the SPAT 
bit for the old entry's virtual page address must be 
cleared and the bit for the new entry's virtual page 
address must be set; this keeps the SPATs up to date.

TLB miss penalties can be modeled in various ways. In 
most x86 processors, a hardware state machine is used 
to walk the 3-level or 4-level page table tree by 
issuing a chain of loads until the lowest level page 
table containing the physical address and attributes is 
reached. This can take from ~10 cycles up to hundreds 
of cycles if the page tables themselves are not already 
in the cache hierarchy. To model this, the 
probe_cache_and_sfr() function queries the DTLB for 
every access, and if a miss is detected, it simulates 
an L2 cache miss to add some latency to the load 
causing the TLB miss. The TLB is then updated with the 
TranslationLookasideBuffer::replace() method.

NOTE: PTLsim does not fully support self modifying 
code. Since no actual data is stored within the data 
and instruction caches (only tags are maintained), self 
modifying code may appear to work correctly. However, 
according to the x86 standard, any stores to any 
instructions currently in the pipeline must flush the 
entire pipeline; PTLsim does not currently do this.

Branch Prediction

 Introduction

PTLsim provides a variety of branch predictors in 
branchpred.cpp. The branch prediction subsystem is 
relatively independent of the core simulator and can be 
treated as a black box, so long as it implements the 
interfaces in branchpred.h.

The branch prediction subsystem always contains at 
least three distinct predictors for the three main 
classes of branches:

 Conditional Branch Predictor returns a boolean (taken 
  or not taken) for each conditional branch (br.cc uop)

 Branch Target Buffer (BTB) predicts indirect branch 
  (jmp uop) targets

 Return Address Stack (RAS) predicts return 
  instructions (i.e. specially marked indirect jmp 
  uops) based on prior calls

 Unconditional branches (bru) are never predicted since 
  their destination is explicitly encoded.

All these predictors are accessed by the core through 
the BranchPredictorInterface object. Based on the 
opcode and other uop information, the core determines 
the type flags of each branch uop:

 BRANCH_HINT_UNCOND for unconditional branches. These 
  are never predicted since the destination is implied.

 BRANCH_HINT_COND for conditional branches.

 BRANCH_HINT_INDIRECT for indirect branches, including returns.

 BRANCH_HINT_CALL for calls (both direct and indirect). 
  This implies that the return address of the call 
  should be a should be pushed on the RAS.

 BRANCH_HINT_RET for returns (indirect branches). This 
  implies that the return address should be taken from 
  the top RAS stack entry, not the BTB.

Multiple flags may be present for each uop (for 
instance, BRANCH_HINT_RET and BRANCH_HINT_INDIRECT are 
both used for the jmp uop terminating an x86 ret instruction).

To make a prediction at fetch time, the core calls the 
BranchPredictorInterface::predict() method, passing it 
a PredictorUpdate structure. This structure is carried 
along with each uop until it retires, and contains all 
the information needed to eventually update the branch 
predictor at the end of the pipeline. The contents will 
vary depending on the predictor chosen, but in general 
this structure contains pointers into internal 
predictor counter tables and various flags. The 
predict() method fills in this structure.

As each uop commits, the 
BranchPredictorInterface::update() method is passed the 
uop's saved PredictorUpdate structure and the branch 
outcome (expected target RIP versus real target RIP) so 
the branch predictor can be updated. In PTLsim, 
predictor updates only occur at retirement to avoid 
corruption caused by speculative instructions.

 Conditional Branch Predictor

The PTLsim conditional branch predictor is the most 
flexible predictor, since it can be easily replaced. 
The default predictor implemented in branchpred.cpp is 
a selection based predictor. In essence, two separate 
predictors are maintained. The history predictor hashes 
a shift register of previously predicted branches into 
a table slot; this slot returns whether or not the 
branch with that history is predicted as taken. PTLsim 
supports various combinations of the history and branch 
address to provide gshare based semantics. The bimodal 
predictor is simpler; it uses 2-bit saturating counters 
to predict if a given branch is likely to be taken. 
Finally, a selection predictor specifies which of the 
two predictors is more accurate and should be used for 
future predictions. This style of predictor, sometimes 
called a McFarling predictor, has been described 
extensively in the literature and variations are used 
by most modern processors.

Through the CombinedPredictor template class, the user 
can specify the sizes of all the tables (history, 
bimodal, selector), the history depth, the method in 
which the global history and branch address are 
combined and so on. Alternatively, the conditional 
branch predictor can be replaced with something 
entirely different if desired.

 Branch Target Buffer

The Branch Target Buffer (BTB) is essentially a small 
cache that maps indirect branch RIP addresses (i.e., 
jmp uops) into predicted target RIP addresses. It is 
set associative, with a user configurable number of 
sets and ways. In PTLsim, the BTB does not take into 
account any indirect branch history information. The 
BTB is a nearly universal structure in branch 
prediction; see the literature for more information.

 Return Address Stack

The Return Address Stack (RAS) predicts the target 
address of indirect jumps marked with the 
BRANCH_HINT_RET flag. Whenever the BRANCH_HINT_RET flag 
is passed to the predict() method, the top RAS stack 
entry is returned as the predicted target, overriding 
anything in the BTB.

Unlike the conditional branch predictor and BTB, the 
RAS updated speculatively in the frontend pipeline, 
before the outcome of calls and returns are known. This 
allows better performance when closely spaced calls and 
returns must be predicted as they are fetched, before 
either the call or corresponding return have actually 
executed. However, when called with the BRANCH_HINT_RET 
flag, the predict() method only returns the RIP at the 
top of the RAS, but does not push or pop the RAS. This 
must be done after the corresponding bru or jmp (for 
direct and or indirect calls, respectively) or jmp (for 
returns) uop is actually allocated in the ROB. 

This approach is required since the RAS is 
speculatively updated: if uops must be annulled 
(because of branch mispredictions or mis-speculations), 
the annulment occurs by walking backwards in the ROB 
until the excepting uop is encountered. However, if the 
RAS were updated during the fetch stage, some uops may 
not be in the ROB yet and hence the rollback logic 
cannot undo speculative changes made to the RAS by 
these uops. This causes the RAS to get out of alignment 
and performance suffers.

To solve this problem, the RAS is only updated in the 
allocate stage immediately after fetch. In the out of 
order core's rename() function, the 
BranchPredictorInterface::updateras() method is called 
to either push or pop an entry from the RAS (calls push 
entries, returns pop entries). Unlike the conditional 
branch predictor and BTB, this is the only place the 
RAS is updated, rather than performing updates at 
commit time.

If uops must be annulled, the 
ReorderBufferEntry::annul() method calls the 
BranchPredictorInterface::annulras() method with the 
PredictorUpdate structure for each uop it encounters in 
reverse program order. This method effectively undoes 
whatever change was made to the RAS when the 
updateras() method was called with the same 
PredictorUpdate information during renaming and 
allocation. This is possible because updateras() saves 
checkpoint information (namely, the old RAS top of 
stack and the value at that stack slot) before updating 
the RAS; this allows the RAS state to be rolled 
backwards in time as uops are annulled in reverse 
program order. At the end of the annulment process when 
fetching is restarted at the correct RIP, the RAS state 
should be identical to the state that existed before 
the last uop to be annulled was originally fetched.

<part:Appendices>Appendices

<sec:UopReference>PTLsim uop Reference

The following sections document the semantics and 
encoding of each micro-operation (uop) supported by the 
PTLsim processor core. The opinfo[] table in 
ptlhwdef.cpp and constants in ptlhwdef.h give actual 
numerical values for the opcodes and other fields 
described below.

 
Merging Rules

  Mnemonic    Syntax        Operation                       
------------------------------------------------------------
  op          rd = ra,rb    rd = ra \leftarrow  (ra op rb)  


Merging Rules:

The x86 compatible ALUs implement operations on 1, 2, 4 
or 8 byte quantities. Unless otherwise indicated, all 
operations take a 2-bit size shift field (sz) used to 
determine the effective size in bytes of the operation 
as follows:

 sz = 0: Low byte of rd is set to the 8-bit result; 
  high 7 bytes of rd are set to corresponding bytes of ra.

 sz = 1: Low two bytes of rd is set to the 16-bit 
  result; high 6 bytes of rd are set to corresponding 
  bytes of ra.

 sz = 2: Low four bytes of rd is set to the 32-bit 
  result; high 4 bytes of rd are cleared to zero in 
  accordance with x86-64 zero extension semantics. The 
  ra operand is unused and should be REG_zero.

 sz = 3: All 8 bytes of rd are set to the 64-bit 
  result. ra is unused and should be REG_zero.

Flags are calculated based on the sz-byte value 
produced by the ALU, not the final 64-bit result in rd.

Other Pseudo-Operators

The descriptions in this reference use various 
pseudo-operators to describe the semantics of each uop. 
These operators are described below.

EvalFlags(ra)

The EvalFlags pseudo-operator evaluates the ZAPS, CF, 
OF flags attached to the source operand ra in 
accordance with the type of condition code evaluation 
specified by the uop. The operator returns 1 if the 
evaluation is true; otherwise 0 is returned.

SignExt(ra, N)

The SignExt operator sign extends the ra operand by the 
number of bits specified by N. Specifically, bit ra[N] 
is copied to all high order bits from bit 63 down to 
bit N. If N is not specified, it is assumed to mean the 
number of bits in the effective size of the uop's 
result (as described under Merging Rules).

MergeWithSFR(mem, sfr)

The MergeWithSFR pseudo-operator is described in the 
reference page for load uops.

MergeAlign(mem, sfr)

The MergeAlign pseudo-operator is described in the 
reference page for load uops.

mov and or xor andnot ornot nand nor eqv
Logical Operations

  Mnemonic    Syntax        Operation                       
------------------------------------------------------------
  mov         rd = ra,rb    rd = ra \leftarrow  rb          
  and         rd = ra,rb    rd = ra \leftarrow  ra & rb     
  or          rd = ra,rb    rd = ra \leftarrow  ra | rb     
  xor         rd = ra,rb    rd = ra \leftarrow  ra ^ rb     
  andnot      rd = ra,rb    rd = ra \leftarrow  (~ra) & rb  
  ornot       rd = ra,rb    rd = ra \leftarrow  (~ra) | rb  
  nand        rd = ra,rb    rd = ra \leftarrow  ~(ra & rb)  
  nor         rd = ra,rb    rd = ra \leftarrow  ~(ra | rb)  
  eqv         rd = ra,rb    rd = ra \leftarrow  ~(ra ^ rb)  


Notes:

 All operations merge the ALU result with ra and 
  generate flags in accordance with the standard x86 
  merging rules described previously.

add sub addadd addsub subadd subsub addm subm addc subc
Add and Subtract

  Mnemonic    Syntax             Operation                                        
----------------------------------------------------------------------------------
  add         rd = ra,rb         rd = ra \leftarrow  ra + rb                      
  sub         rd = ra,rb         rd = ra \leftarrow  ra - rb                      
  adda        rd = ra,rb,rc*S    rd = ra \leftarrow  ra + rb + (rc << S)          
  adds        rd = ra,rb,rc*S    rd = ra \leftarrow  ra + rb + (rc << S)          
  suba        rd = ra,rb,rc*S    rd = ra \leftarrow  ra + rb + (rc << S)          
  subs        rd = ra,rb,rc*S    rd = ra \leftarrow  ra + rb + (rc << S)          
  addm        rd = ra,rb,rc      rd = ra \leftarrow  (ra + rb) & ((1 << rc) - 1)  
  subm        rd = ra,rb,rc      rd = ra \leftarrow  (ra - rb) & ((1 << rc) - 1)  
  addc        rd = ra,rb,rc      rd = ra \leftarrow  (ra + rb) + rc.cf            
  subc        rd = ra,rb,rc      rd = ra \leftarrow  (ra - rb) - rc.cf            


Notes:

 All operations merge the ALU result with ra and 
  generate flags in accordance with the standard x86 
  merging rules described previously.

 The adda, adds, suba, subs uops are useful for small 
  shifts and x86 three-operand LEA-style address generation.

 The addc and subc uops use only the carry flag field 
  of their rc operand; the value is unused.

sel
Conditional Select

  Mnemonic    Syntax             Operation                       
-----------------------------------------------------------------
  sel.cc      rd = (ra),rb,rc    rd = (EvalFlags(ra)) ? rc : rb  


Notes:

 cc is any valid condition code flag evaluation

 The sel uop merges the selected operand with rb in 
  accordance with the standard x86 merging rules 
  described previously (except that sel uses rb as the 
  merge target instead of ra)

 The 64-bit result and all flags are treated as a 
  single value for selection purposes, i.e. the flags 
  attached to the selected input are passed to the output

 If one of the (rb, rc) operands is not valid (has 
  FLAG_INV set) but the selected operand is valid, the 
  result is valid. This is an exception to the invalid 
  bit propagation rule only when the selected input is 
  valid. If the ra operand is invalid, the result is 
  always invalid.

 If any of the inputs are waiting (FLAG_WAIT is set), 
  the uop does not issue, even if the selected input 
  was ready. This is a pipeline simplification.

set
Conditional Set

  Mnemonic    Syntax          Operation                                  
-------------------------------------------------------------------------
  set.cc      rd = (ra),rb    rd = rb \leftarrow  EvalFlags(ra) ? 1 : 0  


Notes:

 cc is any valid condition code flag evaluation

 The value 0 or 1 is zero extended to the operation 
  size and merged with rb in accordance with the 
  standard x86 merging rules described previously 
  (except that set uses rb as the merge target instead 
  of ra)

 Flags attached to ra (condition code) are passed 
  through to the output

set.sub set.and
Conditional Compare and Set 

  Mnemonic      Syntax           Operation                                       
---------------------------------------------------------------------------------
  set.sub.cc    rd = ra,rb,rc    rd = rc \leftarrow  EvalFlags(ra - rb) ? 1 : 0  
  set.and.cc    rd = ra,rb,rc    rd = rc \leftarrow  EvalFlags(ra & rb) ? 1 : 0  


Notes:

 The set.sub and set.and uops take the place of a sub 
  or and uop immediately consumed by a set uop; this is 
  intended to shorten the critical path if uop merging 
  is performed by the processor

 cc is any valid condition code flag evaluation

 The value 0 or 1 is zero extended to the operation 
  size and then merged with rc in accordance with the 
  standard x86 merging rules described previously 
  (except that set.sub and set.and use rc as the merge 
  target instead of ra)

 Flags generated as the result of the comparison are 
  passed through with the result

br
Conditional Branch

  Mnemonic    Syntax                        Operation                                
-------------------------------------------------------------------------------------
  br.cc       rip = (ra),riptaken,ripseq    rip = EvalFlags(ra) ? riptaken : ripseq  


Notes:

 cc is any valid condition code flag evaluation

 The rip (user-visible instruction pointer register) is 
  reset to one of two immediates. If the flags 
  evaluation is true, the riptaken immediate is 
  selected; otherwise the ripseq immediate is selected.

 If the flag evaluation is false (i.e., ripseq is 
  selected), the BranchMispredict internal exception is 
  raised. The processor should annul all uops after the 
  branch and restart fetching at the RIP specified by 
  the result (in this case, ripseq).

 Branches are always assumed to be taken. If the branch 
  is predicted as not taken (i.e. future uops come from 
  the next sequential RIP after the branch), it is the 
  responsibility of the decoder or frontend to swap the 
  riptaken and ripseq immediates and invert the 
  condition of the branch.

 The destination register should always be REG_rip; 
  otherwise this uop is undefined.

 If the target RIP falls within an unmapped page, not 
  present page or a marked as no-execute (NX), the 
  PageFaultOnExec exception is taken.

 No flags are generated by this uop

br.sub br.and
Compare and Conditional Branch

  Mnemonic    Syntax                         Operation                                     
-------------------------------------------------------------------------------------------
  br.cc       rip = ra,rb,riptaken,ripseq    rip = EvalFlags(ra - rb) ? riptaken : ripseq  
  br.cc       rip = ra,rb,riptaken,ripseq    rip = EvalFlags(ra & rb) ? riptaken : ripseq  


Notes:

 The br.sub and br.and uops take the place of a sub or 
  and uop immediately consumed by a br uop; this is 
  intended to shorten the critical path if uop merging 
  is performed by the processor

 cc is any valid condition code flag evaluation

 The rip (user-visible instruction pointer register) is 
  reset to one of two immediates. If the flags 
  evaluation is true, the riptaken immediate is 
  selected; otherwise the ripseq immediate is selected

 If the flag evaluation is false (i.e., ripseq is 
  selected), the BranchMispredict internal exception is 
  raised. The processor should annul all uops after the 
  branch and restart fetching at the RIP specified by 
  the result (in this case, ripseq)

 Branches are always assumed to be taken. If the branch 
  is predicted as not taken (i.e. future uops come from 
  the next sequential RIP after the branch), it is the 
  responsibility of the decoder or frontend to swap the 
  riptaken and ripseq immediates and invert the 
  condition of the branch

 The destination register should always be REG_rip; 
  otherwise this uop is undefined

 If the target RIP falls within an unmapped page, not 
  present page or a marked as no-execute (NX), the 
  PageFaultOnExec exception is taken.

 Flags generated as the result of the comparison are 
  passed through with the result

jmp
Indirect Jump

  Mnemonic    Syntax               Operation  
----------------------------------------------
  jmp         rip = ra,riptaken    rip = ra   


Notes:

 The rip (user-visible instruction pointer register) is 
  reset to the target address specified by ra

 If the ra operand does not match the riptaken 
  immediate, the BranchMispredict internal exception is 
  raised. The processor should annul all uops after the 
  branch and restart fetching at the RIP specified by 
  the result (in this case, ra)

 Indirect jumps are always assumed to match the 
  predicted target in riptaken. If some other target is 
  predicted, it is the responsibility of the decoder or 
  frontend to set the riptaken immediate to that 
  predicted target

 The destination register should always be REG_rip; 
  otherwise this uop is undefined

 If the target RIP falls within an unmapped page, not 
  present page or a marked as no-execute (NX), the 
  PageFaultOnExec exception is taken.

 No flags are generated by this uop

jmpp
Indirect Jump Within Microcode

  Mnemonic    Syntax                Operation         
------------------------------------------------------
  jmpp        null = ra,riptaken    internalrip = ra  


Notes:

 The jmpp uop redirects uop fetching into microcode not 
  accessible as x86 instructions. The target address 
  (inside PTLsim, not x86 space) is specified by ra

 If the ra operand does not match the riptaken 
  immediate, the BranchMispredict internal exception is 
  raised. The processor should annul all uops after the 
  branch and restart fetching at the RIP specified by 
  the result (in this case, ra)

 Indirect jumps are always assumed to match the 
  predicted target in riptaken. If some other target is 
  predicted, it is the responsibility of the decoder or 
  frontend to set the riptaken immediate to that 
  predicted target

 The destination register should always be REG_rip; 
  otherwise this uop is undefined

 The user visible rip register is not updated after 
  this uop issues; otherwise it would point into PTLsim 
  space not accessible to x86 code. Updating is resumed 
  after a normal jmp issues to return to user code. It 
  is the responsibility of the decoder to move the user 
  address to return to into some temporary register 
  (traditionally REG_sr2 but this is not required).

 No flags are generated by this uop

bru
Unconditional Branch

  Mnemonic    Syntax            Operation       
------------------------------------------------
  bru         rip = riptaken    rip = riptaken  


Notes:

 The rip (user-visible instruction pointer register) is 
  reset to the specified immediate. The processor may 
  redirect fetching from the new RIP

 No exceptions are possible with unconditional branches

 If the target RIP falls within an unmapped page, not 
  present page or a marked as no-execute (NX), the 
  PageFaultOnExec exception is taken.

 No flags are generated by this uop

brp
Unconditional Branch Within Microcode

  Mnemonic    Syntax             Operation               
---------------------------------------------------------
  bru         null = riptaken    internalrip = riptaken  


Notes:

 The brp uop redirects uop fetching into microcode not 
  accessible as x86 instructions. The target address 
  (inside PTLsim, not x86 space) is specified by the 
  riptaken immediate

 The rip (user-visible instruction pointer register) is 
  reset to the specified riptaken immediate. The 
  processor may redirect fetching from the new RIP

 No exceptions are possible with unconditional branches

 The user visible rip register is not updated after 
  this uop issues; otherwise it would point into PTLsim 
  space not accessible to x86 code. Updating is resumed 
  after a normal jmp uop issues to return to user code. 
  It is the responsibility of the decoder to move the 
  user address to return to into some temporary 
  register (traditionally REG_sr2 but this is not required).

 No flags are generated by this uop

chk
Check Speculation

  Mnemonic    Syntax                   Operation                        
------------------------------------------------------------------------
  chk.cc      rd = ra,recrip,extype    rd = EvalCheck(ra) ? 0 : recrip  


Notes:

 The chk uop verifies certain properties about ra. If 
  this verification check passes, no action is taken. 
  If the check fails, chk signals an exception of the 
  user specified type in the rc immediate. The result 
  of the chk uop in this case is the user specified RIP 
  to recover at after the check failure is handled in 
  microcode. This recovery RIP is saved in the 
  recoveryrip internal register.

 This mechanism is intended to allow simple inlined uop 
  sequences to branch into microcode if certain 
  conditions fail, since normally inlined uop sequences 
  cannot contain embedded branches. One example use is 
  in the REP series of instructions to ensure that the 
  count is not zero on entry (a special corner case).

 Unlike most conditional uops, the chk uop directly 
  checks the numerical value of ra against zero, and 
  ignores any attached flags. Therefore, the cc 
  condition code flag evaluation type is restricted to 
  the subset (e, ne, be, nbe, l, nl, le, nle).

 No flags are generated by this uop

ld ld.lo ld.hi ldx ldx.lo ldx.hi
Load

  Mnemonic    Syntax                  Operation                                                                
---------------------------------------------------------------------------------------------------------------
  ld          rd = [ra,rb],sfra       rd = MergeWithSFR(mem[ra + rb], sfra)                                    
  ld.lo       rd = [ra+rb],sfra       rd = MergeWithSFR(mem[floor(ra + rb), 8], sfra)                          
  ld.hi       rd = [ra+rb],rc,sfra    rd =   MergeAlign(MergeWithSFR(mem[(floor(ra + rb), 8) + 8], sfra), rc)  


Notes:

 The PTLsim load unit model is described in substantial 
  detail in Section [sec:IssuingLoads]; this section only gives an 
  overview of the load uop semantics.

 The ld family of uops loads values from the virtual 
  address specified by the sum ra + rb. The ld form 
  zero extends the loaded value, while the ldx form 
  sign extends the loaded value to 64 bits.

 All values are zero or sign extended to 64 bits; no 
  subword merging takes place as with ALU uops. The 
  decoder is responsible for following the load with an 
  explicit mov uop to merge 8-bit and 16-bit loads with 
  their old destination register.

 The sfra operand specifies the store forwarding 
  register (a.k.a. store buffer) to merge with data 
  from the cache to form the final result. The 
  inherited SFR may be determined dynamically by 
  querying a store queue or can be predicted statically.

 If the load misses the cache, the FLAG_WAIT flag of 
  the result is set.

 Load uops do not generate any other condition code flags

Unaligned Load Support:

 The processor supports unaligned loads via a pair of 
  ld.lo and ld.hi uops; an overview can be found in 
  Section [sub:UnalignedLoadsAndStores]. The alignment type of the load is stored in 
  the uop's cond field (0 = ld, 1 = ld.lo, 2 = ld.hi).

 The ld.lo uop rounds down its effective address \left\lfloor ra+rb\right\rfloor  to 
  the nearest 64-bit boundary and performs the load. 
  The ld.hi uop rounds \left\lceil ra+rb+8\right\rceil  up to the next 64-bit boundary, 
  performs a load at that address, then takes as its 
  third rc operand the first (ld.lo) load's result. The 
  two loads are concatenated into a 128-bit word and 
  the final unaligned data is extracted (and sign 
  extended if the ldx form was used). 

 Special corner case for when the actual user address 
  (ra + rb) did not actually require any bytes in the 
  8-byte range loaded by the ld.hi uop (i.e. the load 
  was contained entirely within the low 64-bit aligned 
  chunk). Since it is perfectly legal to do an 
  unaligned load to the very end of the page such that 
  the next 64 bit chunk is not mapped to a valid page, 
  the ld.hi uop does not actually access memory; the 
  entire result is extracted from the prior ld.lo 
  result in the rc operand.

Exceptions:

 UnalignedAccess if the address (ra + rb) is not 
  aligned to an integral multiple of the size in bytes 
  of the load. Unaligned loads (ld.lo and ld.hi) do not 
  generate this exception. Since x86 automatically 
  corrects alignment problems, microcode must handle 
  this exception as described in Section [sub:UnalignedLoadsAndStores].

 PageFaultOnRead if the virtual address (ra + rb) falls 
  on a page not accessible to the caller in the current 
  operating mode, or a page marked as not present.

 Various other exceptions and replay conditions may 
  exist depending on the specific processor core model.

st
Store

  Mnemonic    Syntax                    Operation                                             
----------------------------------------------------------------------------------------------
  st          sfrd = [ra,rb],rc,sfra    sfrd = MergeWithSFR((ra + rb), sfra, rc)              
  st.lo       sfrd = [ra+rb],rc,sfra    sfrd = MergeWithSFR(floor(ra + rb, 8), sfra, rc)      
  st.hi       sfrd = [ra+rb],rc,sfra    sfrd = MergeWithSFR(floor(ra + rb, 8) + 8, sfra, rc)  


Notes:

 The PTLsim store unit model is described in 
  substantial detail in Section [sec:StoreMerging]; this section only 
  gives an overview of the store uop semantics.

 The st family of uops prepares values to be stored to 
  the virtual address specified by the sum ra + rb.

 The sfra operand specifies the store forwarding 
  register (a.k.a. store buffer) to merge the data to 
  be stored (the rc operand) into. The inherited SFR 
  may be determined dynamically by querying a store 
  queue or can be predicted statically, as described in [sec:StoreMerging].

 Store uops only generate the SFR for tracking 
  purposes; the cache is only written when the SFR is committed.

 The store uop may issue as soon as the ra and rb 
  operands are ready, even if the rc and sfra operands 
  are not known. The store must be replayed once these 
  operands become known, in accordance with Section [sec:SplitPhaseStores].

 Store uops do not generate any other condition code flags

Unaligned Store Support:

 The processor supports unaligned stores via a pair of 
  st.lo and st.hi uops; an overview can be found in 
  Section [sub:UnalignedLoadsAndStores]. The alignment type of the load is stored in 
  the uop's cond field (0 = st, 1 = st.lo, 2 = st.hi).

 Stores are handled in a similar manner, with st.lo and 
  st.hi rounding down and up to store parts of the 
  unaligned value in adjacent 64-bit blocks. 

 The st.lo uop rounds down its effective address \left\lfloor ra+rb\right\rfloor  to 
  the nearest 64-bit boundary and stores the 
  appropriately aligned portion of the rc operand that 
  actually falls within that range of 8 bytes. The 
  ld.hi uop rounds \left\lceil ra+rb+8\right\rceil  up to the next 64-bit boundary and 
  similarly stores the appropriately aligned portion of 
  the rc operand that actually falls within that high 
  range of 8 bytes.

 Special corner case for when the actual user address 
  (ra + rb) did not actually touch any bytes in the 
  8-byte range normally written by the st.hi uop (i.e. 
  the store was contained entirely within the low 
  64-bit aligned chunk). Since it is perfectly legal to 
  do an unaligned store to the very end of the page 
  such that the next 64 bit chunk is not mapped to a 
  valid page, the st.hi uop does not actually do 
  anything in this case (the bytemask of the generated 
  SFR is set to zero and no exceptions are checked).

Exceptions:

 UnalignedAccess if the address (ra + rb) is not 
  aligned to an integral multiple of the size in bytes 
  of the store. Unaligned stores (st.lo and st.hi) do 
  not generate this exception. Since x86 automatically 
  corrects alignment problems, microcode must handle 
  this exception as described in Section [sub:UnalignedLoadsAndStores].

 PageFaultOnWrite if the virtual address (ra + rb) 
  falls on a write protected page, a page not 
  accessible to the caller in the current operating 
  mode, or a page marked as not present.

 LoadStoreAliasing if a prior load is found to alias 
  the store (see Section [sub:AliasCheck]).

 Various other exceptions and replay conditions may 
  exist depending on the specific processor core model.

ldp ldxp
Load from Internal Space

  Mnemonic    Syntax          Operation                 
--------------------------------------------------------
  ldp         rd = [ra,rb]    rd = MSR[ra+rb]           
  ldxp        rd = [ra+rb]    rd = SignExt(MSR[ra+rb])  


Notes:

 The ldp and ldxp uops load values from the internal 
  PTLsim address space not accessible to x86 code. 
  Typically this address space is mapped to internal 
  machine state registers (MSRs) and microcode scratch 
  space. The internal address to access is specified by 
  the sum ra + rb. The ldp form zero extends the loaded 
  value, while the ldxp form sign extends the loaded 
  value to 64 bits.

 Load uops do not generate any other condition code flags

 Internal loads may not be unaligned, and never stall 
  or generate exceptions.

stp
Store to Internal Space

  Mnemonic    Syntax               Operation        
----------------------------------------------------
  stp         null = [ra,rb],rc    MSR[ra+rb] = rc  


Notes:

 The stp uop stores a value to the internal PTLsim 
  address space not accessible to x86 code. Typically 
  this address space is mapped to internal machine 
  state registers (MSRs) and microcode scratch space. 
  The internal address to store is specified by the sum 
  ra + rb and the value to store is specified by rc.

 Store uops do not generate any other condition code flags

 Internal stores may not be unaligned, and never stall 
  or generate exceptions.

inshb exthb movhb
Byte Operations

  Mnemonic    Syntax        Operation                           
----------------------------------------------------------------
  inshb       rd = ra,rb    rd = ra[15:8] \leftarrow  rb[7:0]   
  exthb       rd = ra,rb    rd = ra[7:0] \leftarrow  rb[15:8]   
  movhb       rd = ra,rb    rd = ra[15:8] \leftarrow  rb[15:8]  


Notes:

 In x86, it is possible to address the second byte of 
  many integer registers as a separate register (as ah, 
  bh, ch, dh). The inshb, exthb and movhb uops are 
  provided for handling this unfortunately still common 
  usage by moving 8 bits between the first and second 
  bytes of the operands.

 These uops do not generate any condition code flags

movhl movl
Merge 32-bit Words

  Mnemonic    Syntax        Operation                           
----------------------------------------------------------------
  movhl       rd = ra,rb    rd = (ra[31:0] << 32) | rb[31:0]    
  movl        rd = ra,rb    rd = ra[31:0] \leftarrow  rb[31:0]  


Notes:

 The movhl uop concatenates two 32-bit words into a 
  64-bit word, while the movl uop simply merges rb into 
  the low 32 bits of ra.

 These uops do not generate any condition code flags

shl shr sar rotl rotr rotcl rotcr
Shifts and Rotates

  Mnemonic    Syntax           Operation                                         
---------------------------------------------------------------------------------
  shl         rd = ra,rb,rc    rd = ra \leftarrow  (ra << rb)                    
  shr         rd = ra,rb,rc    rd = ra \leftarrow  (ra >> rb)                    
  sar         rd = ra,rb,rc    rd = ra \leftarrow  SignExt(ra >> rb)             
  rotl        rd = ra,rb,rc    rd = ra \leftarrow  (ra rotateleft rb)            
  rotr        rd = ra,rb,rc    rd = ra \leftarrow  (ra rotateright rb)           
  rotcl       rd = ra,rb,rc    rd = ra \leftarrow  ({rc.cf, ra} rotateleft rb)   
  rotcr       rd = ra,rb,rc    rd = ra \leftarrow  ({rc.cf, ra} rotateright rb)  


Notes:

 The shift and rotate instructions have some of the 
  most bizarre semantics in the entire x86 instruction 
  set: they may or may not modify flags depending on 
  the rotation count operand, which we may not even 
  know until the instruction issues. This is introduced 
  in Section [sec:ShiftRotateProblems].

 The specific rules are as follows:

   If the count rb=0 is zero, no flags are modified

   If the count rb=1, both OF and CF are modified, but ZAPS 
    is preserved

   If the count rb>1, only the CF is modified. (Technically 
    the value in OF is undefined, but on K8 and P4, it 
    retains the old value, so we try to be compatible).

   Shifts also alter the ZAPS flags while rotates do not.

 For constant counts (immediate rb values), the 
  semantics are easy to determine in advance.

 For variable counts (rb comes from register), things 
  are more complex. Since the shift needs to determine 
  its output flags at runtime based on both the shift 
  count and the input flags (CF, OF, ZAPS), we need to 
  specify the latest versions in program order of all 
  the existing flags. However, this would require three 
  operands to the shift uop not even counting the value 
  and count operands. Therefore, we use a collcc 
  (collect condition code flags, see Section [sub:FlagsManagement]) uop to 
  get all the most up to date flags into one result, 
  using three operands for ZAPS, CF, OF. This forms a 
  zero word with all the correct flags attached, which 
  is then forwarded as the rc operand to the shift. 
  This may add additional scheduling constraints in the 
  case that one of the operands to the shift itself 
  sets the flags, but this is fairly rare. 
  Conveniently, this also lets us directly implement 
  the 65-bit rotcl/rotcr uops in hardware with little 
  additional complexity.

 All operations merge the ALU result with ra and 
  generate flags in accordance with the standard x86 
  merging rules described previously.

 The specific flags attached to the result depend on 
  the input conditions described above. The user should 
  always assume these uops always produce the latest 
  version of each of the ZAPS, CF, OF flag sets.

dupbit
Duplicate Bit

  Mnemonic    Syntax           Operation                                          
----------------------------------------------------------------------------------
  dupbit      rd = ra,rb,rc    rd = ra \leftarrow  (rb[rc], rb[rc], rb[rc], ...)  


Notes:

 The dupbit uop duplicates the specified bit of rb into 
  all 64 bit positions, then merges the result into ra.

 This uop is used to do specialized sign extension

 This uop does not generate any condition code flags

extr extrx
Extract Bit Field

  Mnemonic    Syntax                   Operation                                                                  
------------------------------------------------------------------------------------------------------------------
  extr        rd = ra,shramt,bitcnt    rd = ra \leftarrow  (ra >> shramt) & ((1 << bitcnt) - 1)                   
  extrx       rd = ra,shramt,bitcnt    rd = ra \leftarrow  SignExt((ra >> shramt) & ((1 << bitcnt) - 1), bitcnt)  


Notes:

 The extr and extrx uops are used for generalized bit 
  field extraction when the bounds of the bit field 
  (shramt and bitcnt) are immediates.

 These uops are used extensively within PTLsim 
  microcode, but are also useful if the processor 
  supports dynamically merging a chain of shr and and uops.

 The condition code flags (ZAPS, CF, OF) are the flags 
  logically generated by the final AND operation.

insr
Insert Bit Field

  Mnemonic    Syntax                      Operation                                                                                       
------------------------------------------------------------------------------------------------------------------------------------------
  insr        rd = ra,rb,shlamt,bitcnt    mask = ((1 << bitcnt)-1) << shlamtrd = ra \leftarrow  ((rb << shlamt) & mask) | (ra & (~mask))  


Notes:

 The insr uop is used for generalized bit field 
  insertion when the bounds of the bit field (shlamt 
  and bitcnt) are immediates.

 These uops are used extensively within PTLsim 
  microcode, but are also useful if the processor 
  supports dynamically merging a dependency subgraph of 
  shl , shr, and and or uops.

 The condition code flags (ZAPS, CF, OF) are the flags 
  logically generated by the final OR operation.

sxt zxt
Sign or Zero Extension

  Mnemonic    Syntax     Operation                            
--------------------------------------------------------------
  sxt.sz      rd = ra    rd = ra \leftarrow  SignExt(ra, sz)  
  zxt.sz      rd = ra    rd = ra \leftarrow  ZeroExt(ra, sz)  


Notes:

 The sxt uop is used to sign extend the ra operand from 
  sz bits to the uop's effective result size; this 
  result is then merged into ra.

 The zxt uop performs zero extension with the same semantics

 These uops do not generate any condition code flags.

bswap
Byte Swap

  Mnemonic    Syntax     Operation                         
-----------------------------------------------------------
  bswap       rd = ra    rd = ra \leftarrow  ByteSwap(ra)  


Notes:

 The bswap uop reverses the endianness of the ra 
  operand. The uop's effective result size determines 
  the range of bytes which are reversed.

 This uop's semantics are identical to the x86 bswap instruction.

 This uop does not generate any condition code flags.

collcc
Collect Condition Codes

  Mnemonic    Syntax           Operation                                                  
------------------------------------------------------------------------------------------
  collcc      rd = ra,rb,rc    rd.zaps = ra.zaps rd.cf = rb.cfrd.of = rc.ofrd = rd.flags  


Notes:

 The collcc uop collects the condition code flags from 
  three potentially distinct source operands into a 
  single output with the combined condition code flags 
  in both its appended flags and data.

 This uop is useful for collecting all flags before 
  passing them as input to another uop which only 
  supports one source of flags (for instance, the shift 
  and rotate uops).

movccr movrcc
Move Condition Code Flags Between Register Value and 
Flag Parts

  Mnemonic    Syntax     Operation                  
----------------------------------------------------
  movccr      rd = ra    rd = ra.flagsrd.flags = 0  
  movrcc      rd = ra    rd.flags = rard = ra       


Notes:

 The movccr uop takes the condition code flag bits 
  attached to ra and copies them into the 64-bit 
  register part of the result.

 The movrcc uop takes the low bits of the ra operand 
  and moves those bits into the condition code flag 
  bits attached to the result.

 The bits moved consist of the ZF, PF, SF, CF, OF flags

 The WAIT and INV flags of the result are always 
  cleared since the uop would not even issue if these 
  were set in ra.

andcc orcc ornotcc xorcc
Logical Operations on Condition Codes

  Mnemonic    Syntax        Operation                          
---------------------------------------------------------------
  andcc       rd = ra,rb    rd.flags = ra.flags & rb.flags     
  orcc        rd = ra,rb    rd.flags = ra.flags | rb.flags     
  ornotcc     rd = ra,rb    rd.flags = ra.flags | (~rb.flags)  
  xorcc       rd = ra,rb    rd.flags = ra.flags ^ rb.flags     


Notes:

 These uops are used to perform logical operations on 
  the condition code flags attached to ra and rb.

 If the rb operand is an immediate, the immediate data 
  is used instead of the flags normally attached to a 
  register operand.

 The 64-bit value of the output is always set to zero.

mull mulh
Integer Multiplication

  Mnemonic    Syntax        Operation                                    
-------------------------------------------------------------------------
  mull        rd = ra,rb    rd = ra \leftarrow  lowbits(ra \times  rb)   
  mulh        rd = ra,rb    rd = ra \leftarrow  highbits(ra \times  rb)  


Notes:

 These uops multiply ra and rb, then retain only the 
  low N bits or high N bits of the result (where N is 
  the uop's effective result size in bits). This result 
  is then merged into ra.

 The condition code flags generated by these uops 
  correspond to the normal x86 semantics for integer 
  multiplication (imul); the flags are calculated 
  relative to the effective result size.

 The rb operand may be an immediate

bt bts btr btc
Bit Testing and Manipulation

  Mnemonic    Syntax        Operation                                             
----------------------------------------------------------------------------------
  bt          rd = ra,rb    rd.cf = ra[rb] rd = ra \leftarrow  (rd.cf) ? -1 : +1  
  bts         rd = ra,rb    rd.cf = ra[rb]rd = ra \leftarrow  ra | (1 << rb)      
  btr         rd = ra,rb    rd.cf = ra[rb]rd = ra \leftarrow  ra & (~(1 << rb))   
  btc         rd = ra,rb    rd.cf = ra[rb]rd = ra \leftarrow  ra ^ (1 << rb)      


Notes:

 These uops test a given bit in ra and then atomically 
  modify (set, reset or complement) that bit in the result.

 The CF flag of the output is set to the original value 
  in bit position rb of ra. Other condition code flag 
  bits in the output are undefined.

 The bt (bit test) uop is special: it generates a value 
  of -1 or +1 if the tested bit is 1 or 0, 
  respectively. This is used in microcode for setting 
  up an increment for the rep x86 instructions.

ctz clz
Count Trailing or Leading Zeros

  Mnemonic    Syntax     Operation                                      
------------------------------------------------------------------------
  ctz         rd = ra    rd.zf = (ra == 0)rd = (ra) ? LSBIndex(ra) : 0  
  clz         rd = ra    rd.zf = (ra == 0)rd = (ra) ? MSBIndex(ra) : 0  


Notes:

 These uops find the bit index of the first '1' bit in 
  ra, starting from the lowest bit 0 (for ctz) or the 
  highest bit 63 (for clz).

 The result is zero (technically, undefined) if ra is zero.

 The ZF flag of the result is 1 if ra was zero, or 0 if 
  ra was nonzero. Other condition code flags are undefined.

ctpop
Count Population of '1' Bits

  Mnemonic    Syntax     Operation                                  
--------------------------------------------------------------------
  ctpop       rd = ra    rd.zf = (ra == 0)rd = PopulationCount(ra)  


Notes:

 The ctpop uop counts the number of '1' bits in the ra operand.

 The ZF flag of the result is 1 if ra was zero, or 0 if 
  ra was nonzero. Other condition code flags are undefined.

 
Floating Point Format and Merging

All floating point uops use the same encoding to 
specify the precision and vector format of the 
operands. The uop's size field is encoded as follows:

 00: Single precision scalar floating point (opfp 
  mnemonic). The operation is only performed on the low 
  32 bits (in IEEE single precision format) of the 
  64-bit inputs; the high 32 bits of the ra operand are 
  copied to the high 32 bits of the output.

 01: Single precision vector floating point (opfv 
  mnemonic). The operation is performed on both 32 bit 
  halves (in IEEE single precision format) of the 
  64-bit inputs in parallel

 1x: Double precision scalar floating point (opfd 
  mnemonic). The operation is performed on the full 64 
  bit inputs (in IEEE double precision format)

Most floating point operations merge the result with 
the ra operand to prepare the destination. Since a full 
64-bit result is generated with the vector and double 
formats, the ra operand is not needed and may be 
specified as zero to reduce dependencies.

Exceptions to this encoding are listed where appropriate.

Unless otherwise noted, all operations update the 
internal floating point status register (FPSR, 
equivalent to the MXCSR register in x86 code) by ORing 
in any exceptions that occur. If the uop is encoded to 
generate an actual exception on excepting conditions, 
the FLAG_INV flag is attached to the output to cause an 
exception at commit time.

No condition code flags are generated by floating point 
uops unless otherwise noted.

addf subf mulf divf minf maxf
Floating Point Arithmetic

  Mnemonic    Syntax        Operation                                 
----------------------------------------------------------------------
  addf        rd = ra,rb    rd = ra \leftarrow  ra + rb               
  subf        rd = ra,rb    rd = ra \leftarrow  ra - rb               
  mulf        rd = ra,rb    rd = ra \leftarrow  ra \times  rb         
  divf        rd = ra,rb    rd = ra \leftarrow  ra / rb               
  minf        rd = ra,rb    rd = ra \leftarrow  (ra < rb) ? ra : rb   
  maxf        rd = ra,rb    rd = ra \leftarrow  (ra >= rb) ? ra : rb  


Notes:

 These uops do arithmetic on floating point numbers in 
  various formats as specified in the Floating Point 
  Format and Merging page.

maddf msubf
Fused Multiply Add and Subtract

  Mnemonic    Syntax           Operation                                 
-------------------------------------------------------------------------
  maddf       rd = ra,rb,rc    rd = ra \leftarrow  (ra \times  rb) + rc  
  msubf       rd = ra,rb,rc    rd = ra \leftarrow  (ra \times  rb) - rc  


Notes:

 The maddf and msubf uops perform fused multiply and 
  accumulate operations on three operands.

 The full internal precision is preserved between the 
  multiply and add operations; rounding only occurs at 
  the end.

 These uops are primarily used by microcode to 
  calculate floating point division, square root and reciprocal.

sqrtf rcpf rsqrtf
Square Root, Reciprocal and Reciprocal Square Root

  Mnemonic    Syntax        Operation                         
--------------------------------------------------------------
  sqrtf       rd = ra,rb    rd = ra \leftarrow  sqrt(rb)      
  rcpf        rd = ra,rb    rd = ra \leftarrow  1 / rb        
  rsqrtf      rd = ra,rb    rd = ra \leftarrow  1 / sqrt(rb)  


Notes:

 These uops perform the specified unary operation on rb 
  and merge the result into ra (for a single precision 
  scalar mode only)

 The rcpf and rsqrtf uops are approximates - they do 
  not provide the full precision results. These 
  approximations are in accordance with the standard 
  x86 SSE/SSE2 semantics.

cmpf
Compare Floating Point

  Mnemonic     Syntax        Operation                                             
-----------------------------------------------------------------------------------
  cmpf.type    rd = ra,rb    rd = ra \leftarrow  CompareFP(ra, rb, type) ? -1 : 0  


Notes:

 This uop performs the specified comparison of ra and 
  rb. If the comparison is true, the result is set to 
  all '1' bits; otherwise it is zero. The result is 
  then merged into ra.

 The cond field in the uop encoding holds the 
  comparison type. The set of compare types matches the 
  x86 SSE/SSE2 CMPxx instructions.

cmpccf
Compare Floating Point and Generate Condition Codes

  Mnemonic       Syntax        Operation                          
------------------------------------------------------------------
  cmpccf.type    rd = ra,rb    rd.flags = CompareFPFlags(ra, rb)  


Notes:

 This uop performs all comparisons of ra and rb and 
  produces x86 condition code flags (ZF, PF, CF) to 
  represent the result.

 The semantics of the generated condition code flags 
  exactly matches the x86 SSE/SSE2 instructions 
  COMISS/COMISD/UCOMISS/UCOMISD.

 Unlike most encodings, the size field holds the 
  comparison type of the two values as follows:

   00: cmpccfp: single precision ordered compare (same 
    semantics as x86 SSE COMISS)

   01: cmpccfp.u: single precision unordered compare 
    (same semantics as x86 SSE UCOMISS)

   10: cmpccfd: double precision ordered compare (same 
    semantics as x86 SSE2 COMISD)

   11: cmpccfd.u: double precision ordered compare 
    (same semantics as x86 SSE2 UCOMISD)

cvtf.i2s.ins cvtf.i2s.p cvtf.i2d.lo cvtf.i2d.hi
Convert 32-bit Integer to Floating Point

  Mnemonic        Syntax        Operation                                                               Used By           
--------------------------------------------------------------------------------------------------------------------------
  cvtf.i2s.ins    rd = ra,rb    rd = ra \leftarrow  Int32ToFloat(rb)                                    CVTSI2SS          
  cvtf.i2s.p      rd = ra       rd[31:0] = Int32ToFloat(ra[31:0])rd[63:32] = Int32ToFloat(ra[63:32])    CVTPI2PS          
  cvtf.i2d.lo     rd = ra       rd = Int32ToDouble(ra[31:0])                                            CVTSI2SDCVTPI2PD  
  cvtf.i2d.hi     rd = ra       rd = Int32ToDouble(ra[63:32])                                           CVTPI2PD          


Notes:

 These uops convert 32-bit integers to single or double 
  precision floating point

 The semantics of these instructions are identical to 
  the semantics of the x86 SSE/SSE2 instructions shown 
  in the table

 The uop size field is not used by these uops

cvtf.q2s.ins cvtf.q2d
Convert 64-bit Integer to Floating Point

  Mnemonic        Syntax        Operation                               Used By           
------------------------------------------------------------------------------------------
  cvtf.q2s.ins    rd = ra,rb    rd = ra \leftarrow  Int64ToFloat(rb)    CVTSI2SS(x86-64)  
  cvtf.q2d        rd = ra       rd = Int64ToDouble(ra)                  CVTPI2PS(x86-64)  


Notes:

 These uops convert 64-bit integers to single or double 
  precision floating point

 The semantics of these instructions are identical to 
  the semantics of the x86 SSE/SSE2 instructions shown 
  in the table

 The uop size field is not used by these uops

cvtf.s2i cvt.s2q cvtf.s2i.p
Convert Single Precision Floating Point to Integer

  Mnemonic      Syntax     Operation                                                               Used By           
---------------------------------------------------------------------------------------------------------------------
  cvtf.s2i      rd = ra    rd = FloatToInt32(ra[31:0])                                             CVTSS2SI          
  cvtf.s2i.p    rd = ra    rd[31:0] = FloatToInt32(ra[31:0])rd[63:32] = FloatToInt32(ra[63:32])    CVTPS2PICVTPS2DQ  
  cvtf.s2q      rd = ra    rd = FloatToInt64(ra)                                                   CVTSS2SI(x86-64)  


Notes:

 These uops convert single precision floating point 
  values to 32-bit or 64-bit integers

 The semantics of these instructions are identical to 
  the semantics of the x86 SSE/SSE2 instructions shown 
  in the table

 Unlike most encodings, the size field holds the 
  rounding type of the result as follows:

   x0: normal IEEE rounding (as determined by FPSR)

   x1: truncate to zero

cvtf.d2i cvtf.d2q cvtf.d2i.p
Convert Double Precision Floating Point to Integer

  Mnemonic      Syntax        Operation                                                    Used By           
-------------------------------------------------------------------------------------------------------------
  cvtf.d2i      rd = ra       rd = DoubleToInt32(ra)                                       CVTSD2SI          
  cvtf.d2i.p    rd = ra,rb    rd[63:32] = DoubleToInt32(ra)rd[31:0] = DoubleToInt32(rb)    CVTPD2PICVTPD2DQ  
  cvtf.s2q      rd = ra       rd = DoubleToInt64(ra)                                       CVTSD2SI(x86-64)  


Notes:

 These uops convert double precision floating point 
  values to 32-bit or 64-bit integers

 The semantics of these instructions are identical to 
  the semantics of the x86 SSE/SSE2 instructions shown 
  in the table

 Unlike most encodings, the size field holds the 
  rounding type of the result as follows:

   x0: normal IEEE rounding (as determined by FPSR)

   x1: truncate to zero

cvtf.d2s.ins cvtf.d2s.p cvtf.s2d.lo cvtf.s2d.hi
Convert Between Double Precision and Single Precision 
Floating Point

  Mnemonic        Syntax        Operation                                                    Used By           
---------------------------------------------------------------------------------------------------------------
  cvtf.d2s.ins    rd = ra,rb    rd = ra \leftarrow  DoubleToFloat(rb)                        CVTSD2SS          
  cvtf.d2s.p      rd = ra       rd[63:32] = DoubleToFloat(ra)rd[31:0] = DoubleToFloat(rb)    CVTPD2PS          
  cvtf.s2d.lo     rd = ra       rd = FloatToDouble(ra[31:0])                                 CVTSS2SDCVTPS2PD  
  cvtf.s2d.hi     rd = ra       rd = FloatToDouble(ra[63:32])                                CVTPS2PD          


Notes:

 These uops convert single precision floating point 
  values to double precision floating point values

 The semantics of these instructions are identical to 
  the semantics of the x86 SSE/SSE2 instructions shown 
  in the table

 The uop size field is not used by these uops

<sec:PerformanceCounters>Performance Counters

PTLsim maintains hundreds of performance and 
statistical counters and data points as it simulates 
user code. In Section [sec:StatisticsInfrastructure], the basic mechanisms and data 
structures through which PTLsim collects these data 
were disclosed, and a guide to extending the existing 
set of collection points was presented.

This section is a reference listing of all the current 
performance counters present in PTLsim by default. The 
sections below are arranged in a hierarchical tree 
format, just as the data are represented in PTLsim's 
data store.

 General

As described in Section [sec:StatisticsInfrastructure], PTLsim maintains a 
hierarchical tree of statistical data. At the root of 
the tree are a potentially large number of snapshots, 
numbered starting at 0. The final snapshot, taken just 
before simulation completes, is labeled as "final". Each 
snapshot branch contains all of the data structures 
described in the next few sections. Snapshots are 
enabled with the -snapshot configuration option 
(Section [sec:ConfigurationOptions]); if they are disabled, only the "0" and "final" 
snapshots are provided.

In addition to the snapshots, the root of the data tree 
contains a "ptlsim" node with the following miscellaneous 
information:

ptlsim: metadata about the PTLsim build and host system 
it is running on

 executable: the full path of the program being simulated

 args: arguments to the program being simulated

 hw-ver: simulated core hardware version

 ptl-ver: microcode version

 build-hostname: the machine on which PTLsim was compiled

 build-timestamp: the date on which PTLsim was compiled 
  (this is set whenever config.o is compiled)

 build-compiler-version: gcc version used to build PTLsim

 hostname: the host machine PTLsim is running on

 native-mhz: the clock speed of the host microprocessor 
  running PTLsim (this is obtained in accordance with 
  Section [sec:Timing])

 Out of Order Core

summary: summarizes the performance of user code 
running on the simulator

 cycles: total number of processor cycles simulated

 commits: total number of committed uops

 usercommits: total number of committed x86 instructions

 issues: total number of uops issued. This includes 
  uops issued more than once by through replay (Section [sec:Scheduling]).

 ipc: Instructions Per Cycle (IPC) statistics

   commit-in-uops: average number of uops committed per cycle

   issue-in-uops: average number of uops issued per cycle

   commit-in-user-insns: average number of x86 
    instructions committed per cycle
    
    NOTE: Because one x86 instruction may be broken up 
    into numerous uops, it is never appropriate to 
    compare IPC figures for committed x86 instructions 
    per clock with IPC values from a RISC machine. 
    Furthermore, different x86 implementations use 
    varying numbers of uops per x86 instruction as a 
    matter of encoding, so even comparing the uop based 
    IPC between x86 implementations or RISC-like 
    machines is inaccurate. Users are strongly advised 
    to use relative performance measures instead (e.g. 
    total cycles taken to complete a given benchmark).

simulator: describes the performance of PTLsim itself. 
Useful for tuning the simulator.

 cycles: total time in seconds (not simulated cycles!) 
  spent in various parts of the simulator. Please refer 
  to the source code (in ooocore.cpp) for the range of 
  code each time value corresponds to.

 rate: PTLsim simulator performance

   total-secs: total seconds spent in simulation mode 
    (native mode does not count towards this total)

   cycles-per-sec: average number of processor cycles 
    simulated per second

   issues-per-sec: average number of uops issued in the 
    simulator per second

   commits-per-sec: average number of uops committed in 
    the simulator per second

   user-commits-per-sec: average number of x86 
    instructions committed in the simulator per second

 bbcache: Decoded basic block cache performance

   count: total basic blocks encountered in the user code

   inserts: insertions into the basic block cache

   removes: removals from the basic block cache (e.g. 
    when a block must be re-translated after unaligned 
    loads and stores are found, etc.)

fetch: fetch stage statistics

 width: histogram of the fetch width actually used on 
  each cycle

 stop: totals up the reasons why fetching finally 
  stopped in each cycle

   icache-miss: an instruction cache miss prevented 
    further fetches

   fetchq-full: the uop fetch queue is full

   bogus-rip: speculative execution redirected the 
    fetch unit to an inaccessible (or non-executable) 
    page. The fetch unit remains stalled in this state 
    until the mis-speculation is resolved.

   branch-taken: taken branches to non-sequential 
    addresses always stop fetching

   full-width: the maximum fetch width was utilized 
    without encountering any of the events above

 blocks: blocks of x86 instructions fetched (typically 
  the processor can read at most e.g. 16 bytes out of a 
  64 byte instruction cache line per cycle)

 uops: total number of uops fetched

 user-insns: total number of x86 instructions fetched

 opclass: histogram of how many uops of various 
  operation classes passed through the fetch unit. The 
  operation classes are defined in ptlhwdef.h and 
  assigned to various opcodes in ptlhwdef.cpp.

frontend: frontend pipeline (decode, allocate, rename) statistics

 width: histogram of the frontend width actually used 
  on each cycle

 status: totals up the reasons why frontend processing 
  finally stopped in each cycle

   complete: all uops were successfully allocated and renamed

   fetchq-empty: no more uops were available for allocation

   rob-full: reorder buffer (ROB) was full

   physregs-full: physical register file was full even 
    though an ROB slot was free

   ldq-full: load queue was full (too many loads in the 
    pipeline) even though physical registers were available

   stq-full: store queue was full (too many stores in 
    the pipeline)

 renamed: summarizes the type of renaming that occurred 
  for each uop (of the destination, not the operands)

   none: uop did not rename its destination (primarily 
    for stores and branches)

   reg: uop renamed destination architectural register

   flags: uop renamed one or more of the ZAPS, CF, OF 
    flag sets but had no destination architectural register

   reg-and-flags: uop renamed one or more of the ZAPS, 
    CF, OF flag sets as well as a destination 
    architectural register

 alloc: summarizes the type of resource allocation that 
  occurred for each uop (in addition to its ROB slot):

   reg: uop was allocated a physical register

   ldreg: uop was a load and was allocated both a 
    physical register and a load queue entry

   sfr: uop was a store and was allocated a store 
    forwarding register (SFR), a.k.a. store queue entry

   br: uop was a branch and was allocated 
    branch-related resources (possibly including a 
    destination physical register)

dispatch: dispatch unit statistics

 source: totals up where each operand to each uop 
  currently resided at the time the uop was dispatched

   waiting: how many operands were waiting (i.e. not 
    yet ready)

   bypass: how many operands would come from the bypass 
    network if the uop were immediately issued

   physreg: how many operands were already written back 
    to physical registers

   archreg: how many operands would be obtained from 
    architectural registers

 cluster: tracks the number of uops issued to each 
  cluster (or issue queue) in the processor. This list 
  will vary depending on the processor configuration. 
  The value none means that no cluster could accept the 
  uop because all issue queues were full.

issue: issue statistics

 result: histogram of the final disposition of issuing 
  each uop

   no-fu: no functional unit was available within the 
    uop's assigned cluster even though it was already issued

   replay: uop attempted to execute but could not 
    complete, so it must remain in the issue queue to 
    be replayed. This event generally occurs when a 
    load or store detects a previously unknown 
    forwarding dependency on a prior store, when the 
    data to actually store is not yet available, or 
    when insufficient resources are available to 
    complete the memory operation. Details are given in 
    Sections [sec:IssuingLoads] and [sec:SplitPhaseStores].

   misspeculation: uop mis-speculated and now all uops 
    after and including the issued uop must be 
    annulled. This generally occurs with loads (Section [sec:IssuingLoads]
    ) and stores (Section [sub:AliasCheck]) when unaligned accesses or 
    load-store aliasing occurs. This event is handled 
    in accordance with Section [sec:SpeculationRecovery].

   branch-mispredict: uop was a branch and 
    mispredicted, such that all uops after (but not 
    including) the branch uop must be annulled. See 
    Section [sec:SpeculationAndRecovery] for details.

   exception: uop caused an exception (though this may 
    not be a user visible error due to speculative execution)

   complete: uop completed successfully. Note that this 
    does not mean the result is immediately ready; for 
    loads it simply means the request was issued to the cache.

 source: totals up where each operand to each uop was 
  read from as it was issued

   bypass: how many operands came directly off the 
    bypass network

   physreg: how many operands were read from physical registers

   archreg: how many operands were read from committed 
    architectural registers

 width: histogram of the issue width actually used on 
  each cycle in each cluster. This object is further 
  broken down by cluster, since various clusters have 
  different issue width and policies.

 opclass: histogram of how many uops of various 
  operation classes were issued. The operation classes 
  are defined in ptlhwdef.h and assigned to various 
  opcodes in ptlhwdef.cpp.

writeback: writeback stage statistics

 total: total number of results written back to the 
  physical register file

 transient: transient versus persistent values

   transient: the result technically does not have to 
    be written back to the physical register file at 
    all, since all consumers sourced the value off the 
    bypass network and the result is no longer 
    available since the destination architectural 
    register pointing to it has since been renamed.

   persistent: all values which do not meet the 
    conditions above and hence must still be written back

 width: histogram of the writeback width actually used 
  on each cycle in each cluster. This object is further 
  broken down by cluster, since various clusters have 
  different issue width and policies.

commit: commit unit statistics

 uops: total number of uops committed

 userinsns: total number of x86 instructions committed

 result: histogram of the final disposition of 
  attempting to commit each uop

   none: one or more uops comprising the x86 
    instruction at the head of the ROB were not yet 
    ready to commit, so commitment is terminated for 
    that cycle

   ok: result was successfully committed

   exception: result caused a genuine user visible 
    exception. Generally this will terminate the simulation.

   skipblock: This occurs in extremely rare cases when 
    the processor must skip over the currently 
    executing instruction (such as in pathological 
    cases of the rep x86 instructions).

   barrier: the processor encountered a barrier 
    instruction, such as a system call, assist or 
    pipeline flush. The frontend has already been 
    stopped and fetching has been redirected to the 
    code to handle the barrier; this condition simply 
    commits the barrier instruction itself.

   stop: special case for when the simulation is to be 
    stopped after committing a certain number of x86 
    instructions (e.g. via the -stopinsns option in 
    Section [sec:ConfigurationOptions]).

 setflags: how many uops updated the condition code 
  flags as they committed

   yes: how many uops updated at least one of the ZAPS, 
    CF, OF flag sets (the REG_flags internal 
    architectural register)

   no: how many uops did not update any flags

 freereg: how many uops were able to free the old 
  physical register mapped to their architectural 
  destination register at commit time

   pending: old physical register was still referenced 
    within the pipeline or by one or more rename table entries

   free: old physical register could be immediately freed

 physreg-recycled: how many physical registers were 
  recycled (garbage collected) later than normal 
  because of one of the conditions above

 width: histogram of the issue width actually used on 
  each cycle in each cluster. This object is further 
  broken down by cluster, since various clusters have 
  different issue width and policies.

 opclass: histogram of how many uops of various 
  operation classes were issued. The operation classes 
  are defined in ptlhwdef.h and assigned to various 
  opcodes in ptlhwdef.cpp.

branchpred: branch predictor statistics

 predictions: total number of branch predictions of any type

 updates: total number of branch predictor updates of 
  any type

 cond: conditional branch (br.cc uop) prediction 
  outcomes, broken down into correct predictions and 
  mispredictions

 indir: indirect branch (jmp uop) prediction outcomes, 
  broken down into correct predictions and mispredictions

 return: return (jmp uop with BRANCH_HINT_RET flag) 
  prediction outcomes, broken down into correct 
  predictions and mispredictions

 summary: summary of all prediction outcomes of the 
  three types above, broken down into correct 
  predictions and mispredictions

 ras: return address stack (RAS) operations

   push: RAS pushes on calls

   push-overflows: RAS pushes on calls in which the RAS 
    overflowed

   pop: RAS pops on returns

   pop-underflows: RAS pops on returns in which the RAS 
    was empty

   annuls: annulment operations in which speculative 
    updates to the RAS were rolled back

 Cache Subsystem

load: load unit statistics

 result: histogram of the final disposition of issuing 
  each load uop

   complete: cache hit

   miss: L1 cache miss, and possibly lower levels as 
    well (Sections [sec:CacheMissHandling] and [sec:InitiatingCacheMiss])

   exception: load generated an exception (typically a 
    page fault), although the exception may still be 
    speculative (Section [sec:IssuingLoads])

   ordering: load was misordered with respect to stores 
    (Section [sub:AliasCheck])

   unaligned: load was unaligned and will need to be 
    re-executed as a pair of low and high loads 
    (Sections [sub:UnalignedLoadsAndStores] and [sec:IssuingLoads])

   replay: histogram of events in which a load needed 
    to be replayed (Section [sec:IssuingLoads])

     sfr-addr-and-data-not-ready: load was predicted to 
      forward data from a prior store (Section [sub:AliasCheck]), but 
      neither the address nor the data of that store 
      has resolved yet

     sfr-addr-not-ready: load was predicted to forward 
      data from a prior store, but the address of that 
      store has not resolved yet

     sfr-data-not-ready: load address matched a prior 
      store in the store queue, but the data that store 
      should write has not resolved yet

     missbuf-full: load missed the cache but the miss 
      buffer and/or LFRQ (Section [sec:InitiatingCacheMiss]) was full at the time

 hit: histogram of the cache hierarchy level each load 
  finally hit

   L1: L1 cache hit

   L2: L1 cache miss, L2 cache hit

   L3: L! and L2 cache miss, L3 cache hit

   mem: all caches missed; value read from main memory

 forward: histogram of which sources were used to fill 
  each load

   cache: how many loads obtained all their data from 
    the cache

   sfr: how many loads obtained all their data from a 
    prior store in the pipeline (i.e. load completely 
    overlapped that store)

   sfr-and-cache: how many loads obtained their data 
    from a combination of the cache and a prior store

 dependency: histogram of how loads related to previous stores

   independent: load was independent of any store 
    currently in the pipeline

   predicted-alias-unresolved: load was stalled because 
    the load store alias predictor (LSAP) predicted 
    that an earlier store would overlap the load's 
    address address even though that earlier store's 
    address was unresolved (Section [sub:AliasCheck])

   stq-address-match: load depended on an earlier store 
    still found in the store queue

 type: histogram of the type of each load uop

   aligned: normal aligned loads

   unaligned: special unaligned load uops ld.lo or 
    ld.hi (Section [sub:UnalignedLoadsAndStores])

   internal: loads from PTLsim space by microcode

 size: histogram of the size in bytes of each load uop

 transfer-L2-to-L1: histogram of the types of L2 to L1 
  line transfers that occurred (Section [sec:CacheHierarchy])

   full-L2-to-L1: all bytes in cache line were 
    transferred from L2 to L1 cache

   partial-L2-to-L1: some bytes in the L1 line were 
    already valid (because of stores to those bytes), 
    but the remaining bytes still need to be fetched

   L2-to-L1I: all bytes in the L2 line were transferred 
    into the L1 instruction cache

 dtlb: data cache translation lookaside buffer hit 
  versus miss rate (Section [sec:TranslationLookasideBuffers])

fetch: instruction fetch unit statistics (Section [sec:FetchStage])

 hit: histogram of the cache hierarchy level each fetch 
  finally hit

   L1: L1 cache hit

   L2: L1 cache miss, L2 cache hit

   L3: L! and L2 cache miss, L3 cache hit

   mem: all caches missed; value read from main memory

 itlb: instruction cache translation lookaside buffer 
  hit versus miss rate (Section [sec:TranslationLookasideBuffers])

prefetches: prefetch engine statistics

 in-L1: requested data already in L1 cache

 in-L2: requested data already in L2 cache (and 
  possibly also in L1 cache)

 required: prefetch was actually required (data was not 
  cached or was in L3 or lower levels)

missbuf: miss buffer performance (Sections [sec:InitiatingCacheMiss] and [sec:FillingCacheMiss])

 inserts: total number of lines inserted into the miss buffer

 delivers: total number of lines delivered to various 
  cache hierarchy levels from the miss buffer

   mem-to-L3: deliver line from main memory to the L3 cache

   L3-to-L2: deliver line to the L3 cache to the L2 cache

   L2-to-L1D: deliver line from the L2 cache to the L1 
    data cache

   L2-to-L1I: deliver line from the L2 cache to the L1 
    instruction cache

lfrq: load fill request queue (LFRQ) performance 
(Sections [sec:InitiatingCacheMiss] and [sec:FillingCacheMiss])

 inserts: total number of loads inserted into the LFRQ

 wakeups: total number of loads awakened from the LFRQ

 annuls: total number of loads annulled in the LFRQ 
  (after they were annulled in the processor core)

 resets: total number of LFRQ resets (all entries cleared)

 total-latency: total latency in cycles of all loads 
  passing through the LFRQ

 average-miss-latency: average load latency, weighted 
  by cache level hit and latency to that level

 width: histogram of how many loads were awakened per 
  cycle by the LFRQ

store: store unit statistics

 issue: histogram of the final disposition of issuing 
  each store uop

   complete: store completed without problems

   exception: store generated an exception (typically a 
    page fault), although the exception may still be 
    speculative (Section [sec:StoreMerging])

   ordering: store detected that a later load in 
    program order aliased the store but was issued 
    earlier than the store (Section [sub:AliasCheck])

   unaligned: store was unaligned and will need to be 
    re-executed as a pair of low and high stores 
    (Sections [sub:UnalignedLoadsAndStores])

   replay: histogram of events in which a store needed 
    to be replayed (Sections [sec:SplitPhaseStores] and [sec:StoreMerging])

     wait-sfraddr-sfrdata: neither the address nor the 
      data of a prior store this store inherits some of 
      its data from was ready

     wait-sfraddr: the data of a prior store was ready 
      but its address was still unavailable

     wait-sfrdata: the address of a prior store was 
      ready but its data was still unavailable

     wait-storedata-sfraddr-sfrdata: the actual data 
      value to store was not ready (Section [sec:SplitPhaseStores]), in 
      addition to having neither the data nor the 
      address of a prior store (Section [sec:StoreMerging])

     wait-storedata-sfraddr: the actual data value to 
      store was not ready (Section [sec:SplitPhaseStores]), in addition to 
      not having the address of the prior store 
      (Section [sec:StoreMerging])

     wait-storedata-sfrdata: the actual data value to 
      store was not ready (Section [sec:SplitPhaseStores]), in addition to 
      not having the data from the prior store (Section [sec:StoreMerging])

 forward: histogram of which sources were used to 
  construct the merged store buffer:

   zero: no prior store overlapping the current store 
    was found in the pipeline

   sfr: data from a prior store in the pipeline was 
    merged with the value to be stored to form the 
    final store buffer

 type: histogram of the type of each store uop

   aligned: normal aligned store

   unaligned: special unaligned store uops st.lo or 
    st.hi (Section [sub:UnalignedLoadsAndStores])

   internal: stores to PTLsim space by microcode

 size: histogram of the size in bytes of each store uop

 commit: histogram of how stores are committed

   direct: store committed directly to the data cache 
    in the commit stage (Section [sec:CommitStage])

The End
